{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PRS Japan Workshop is Almost Here!: Aug 31, 2024 Attendees should complete the preworkshop checklist! Download pre-workshop materials for Linux ( Link ) or macOS ( Link ). This short course will equip scientists based in Japan with tools and approaches for polygenic risk scores (PRS) analysis. The course will cover both applied and theoretical topics in PRS research, delivered across a variety of lectures, tutorials, computational practicals and special guest seminars from experts in the field. By the end of the workshop, attendees should have an in-depth theoretical understanding and practical skills in PRS analysis of global populations. The course will begin with an overview of genome-wide association studies, an introduction to PRS analysis, advanced topics in PRS (e.g., pathway-based PRS, PRS Environment interactions, PRS to identify rare variants). This will be followed by the key topic of the \u2018PRS Portability Problem\u2019 and how to address it using PRS methods developed for application to diverse and admixed ancestry samples. Finally, attendees will devise, perform and present their own research project as part of a group on a topic relevant to the content of the course, with feedback from the workshop team.","title":"Home"},{"location":"#prs-japan-workshop-is-almost-here-aug-31-2024","text":"Attendees should complete the preworkshop checklist! Download pre-workshop materials for Linux ( Link ) or macOS ( Link ). This short course will equip scientists based in Japan with tools and approaches for polygenic risk scores (PRS) analysis. The course will cover both applied and theoretical topics in PRS research, delivered across a variety of lectures, tutorials, computational practicals and special guest seminars from experts in the field. By the end of the workshop, attendees should have an in-depth theoretical understanding and practical skills in PRS analysis of global populations. The course will begin with an overview of genome-wide association studies, an introduction to PRS analysis, advanced topics in PRS (e.g., pathway-based PRS, PRS Environment interactions, PRS to identify rare variants). This will be followed by the key topic of the \u2018PRS Portability Problem\u2019 and how to address it using PRS methods developed for application to diverse and admixed ancestry samples. Finally, attendees will devise, perform and present their own research project as part of a group on a topic relevant to the content of the course, with feedback from the workshop team.","title":"PRS Japan Workshop is Almost Here!: Aug 31, 2024"},{"location":"misc_linux/","text":"Linux for Windows Users For windows users, a single WSL command can be used to install linux. Detailed information on WSL can be found on the microsoft website and step-by-step video obstructions can be found here . These steps involve: Installation Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install Ubuntu This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. After it has been installed you will need to create a new username and password, to learn more about this please see microsoft best practices Setup Once Linux has been installed and you have created a new account you can run Ubuntu using windows terminal or by going to the start menu and typing \"Ubuntu\". Additionally, you should see that Ubuntu has been added to your applications and that there is a Ubuntu Folder on your computer (alongside Desktop, Downloads, etc). Double clicking the Ubuntu app will open up the Ubuntu terminal in the Ubuntu folder . You should see many sub-folders within the Ubuntu folder (such as bin, home, etc, etc.) . You can view them using terminal commands or by double-clicking on them. If you go into the home/ folder you should see another folder with your username. This folder ( home/(your username)/ ) is where you should store your workshop folders and run programs during the workshop. Once you have successfully installed Ubuntu, you can follow the rest of the preworkshop tutorial using the Linux instructions.","title":"Linux For Windows"},{"location":"misc_linux/#linux-for-windows-users","text":"For windows users, a single WSL command can be used to install linux. Detailed information on WSL can be found on the microsoft website and step-by-step video obstructions can be found here . These steps involve:","title":"Linux for Windows Users"},{"location":"misc_linux/#installation","text":"Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install Ubuntu This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. After it has been installed you will need to create a new username and password, to learn more about this please see microsoft best practices","title":"Installation"},{"location":"misc_linux/#setup","text":"Once Linux has been installed and you have created a new account you can run Ubuntu using windows terminal or by going to the start menu and typing \"Ubuntu\". Additionally, you should see that Ubuntu has been added to your applications and that there is a Ubuntu Folder on your computer (alongside Desktop, Downloads, etc). Double clicking the Ubuntu app will open up the Ubuntu terminal in the Ubuntu folder . You should see many sub-folders within the Ubuntu folder (such as bin, home, etc, etc.) . You can view them using terminal commands or by double-clicking on them. If you go into the home/ folder you should see another folder with your username. This folder ( home/(your username)/ ) is where you should store your workshop folders and run programs during the workshop. Once you have successfully installed Ubuntu, you can follow the rest of the preworkshop tutorial using the Linux instructions.","title":"Setup"},{"location":"misc_plink_problem/","text":"Plink: Developer Cannot Be Verified If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running whatever program you are using and follow these instructions to give your system permission to run downloaded software. 1- Default Settings By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below): 2- Allowing Exceptions Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future. 3- Downstream BridgePRS Errors If you have moved plink to the trash you will have to recover it, additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS easyrun go -o out1 --pop_configs data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"Mac Security"},{"location":"misc_plink_problem/#plink-developer-cannot-be-verified","text":"If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running whatever program you are using and follow these instructions to give your system permission to run downloaded software.","title":"Plink: Developer Cannot Be Verified"},{"location":"misc_plink_problem/#1-default-settings","text":"By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below):","title":"1- Default Settings"},{"location":"misc_plink_problem/#2-allowing-exceptions","text":"Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future.","title":"2- Allowing Exceptions"},{"location":"misc_plink_problem/#3-downstream-bridgeprs-errors","text":"If you have moved plink to the trash you will have to recover it, additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS easyrun go -o out1 --pop_configs data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"3- Downstream BridgePRS Errors"},{"location":"prep_list/","text":"Introduction Here we present a preworkshop preparation guide to get students sufficiently prepared before the workshop begins. In this guide we help students set up the operating system (Linux or macOS), provide tutorials for Bash, R, and Python for students who are unfamiliar or need a refresher, and help students verify that their system is capable of running the software necessary for the workshop. Reminder: All students must complete preworkship testing and verification At the end of the students are lead through a series of software tests The tests are required to verify that the laptops they will use during the workshop are properly set up. Students unable to properly configure their laptops should email course instructor Souaiaia here . Checklist This checklist will help guide you through the pre-workshop requirements: Operating System : Confirm or install a compatible operating system. Terminal Setup : Setting up the terminal, creating workshop directory. Software Installations : Confirm/Install R, Python3, and Plink as well their as required libraries. Software Tutorials : If unfamiliar with bash, R, or python, please complete the included tutorials. Verification/Testing : Download materials, test that everything is in working order ( Required ).","title":"Checklist"},{"location":"prep_list/#introduction","text":"Here we present a preworkshop preparation guide to get students sufficiently prepared before the workshop begins. In this guide we help students set up the operating system (Linux or macOS), provide tutorials for Bash, R, and Python for students who are unfamiliar or need a refresher, and help students verify that their system is capable of running the software necessary for the workshop. Reminder: All students must complete preworkship testing and verification At the end of the students are lead through a series of software tests The tests are required to verify that the laptops they will use during the workshop are properly set up. Students unable to properly configure their laptops should email course instructor Souaiaia here .","title":"Introduction"},{"location":"prep_list/#checklist","text":"This checklist will help guide you through the pre-workshop requirements: Operating System : Confirm or install a compatible operating system. Terminal Setup : Setting up the terminal, creating workshop directory. Software Installations : Confirm/Install R, Python3, and Plink as well their as required libraries. Software Tutorials : If unfamiliar with bash, R, or python, please complete the included tutorials. Verification/Testing : Download materials, test that everything is in working order ( Required ).","title":"Checklist"},{"location":"prep_os/","text":"Operating System This workshop is designed to help students run statistical genetic analysis. In practice large-scale data analysis is almost always done using a server or cloud-based system running a Linux operating system. For this reason a Linux based operating system is recommended and users running MacOs ( see here ) or Windows ( see here ) are encouraged to install Linux if possible. However, due to the similarity between Unix based operating systems, this workshop and tutorial can be completed in Linux and macOs environments and is described for users with both operating systems. Microsoft windows users are required to install Linux and as such, we provide detailed instructions to do so. This workshop is designed for either of the following operating systems: Linux: A Debian compatible Linux based operating system (Ubuntu, Mint, etc). macOS: A recent version (11+) of the Desktop OS. Microsoft Windows is not supported, but instructions to install Linux can be found here.","title":"Operating System"},{"location":"prep_os/#operating-system","text":"This workshop is designed to help students run statistical genetic analysis. In practice large-scale data analysis is almost always done using a server or cloud-based system running a Linux operating system. For this reason a Linux based operating system is recommended and users running MacOs ( see here ) or Windows ( see here ) are encouraged to install Linux if possible. However, due to the similarity between Unix based operating systems, this workshop and tutorial can be completed in Linux and macOs environments and is described for users with both operating systems. Microsoft windows users are required to install Linux and as such, we provide detailed instructions to do so. This workshop is designed for either of the following operating systems: Linux: A Debian compatible Linux based operating system (Ubuntu, Mint, etc). macOS: A recent version (11+) of the Desktop OS. Microsoft Windows is not supported, but instructions to install Linux can be found here.","title":"Operating System"},{"location":"prep_software/","text":"Required System Software This workshop requires that the following software be installed and tested before the workshop: Name/Link Description Additional Requirements 1. R Most popular analysis program in statistical genetics Yes (Specific Libraries). 2. Python 3 . Multi purpose Programming Language Yes (The matplotlib library). Both popular computer languages, R and python3 will be used during the workshop. Here we will provide a guide for installation of both languages and their associated libraries. 1. R (and associated packages) R is the most popular analysis program in statistical genetics, and required for most genetic analysis. We recommend that MacOS users download an up-to-date (4.4.1) version directly from the R website and using the R installer. If you are having trouble we recommend this video tutorial . While Linux users can also find a suitable version on the website, we recommend using the package installer that comes with Debian based distributions (Debian, Ubuntu, Mint, etc) and downloading R directly using the following terminal commands: sudo apt install r-base # to install R sudo apt install build-essential # to install the essential packages Important: Additional R Packages Are Required This workshop requires the following non-standard R packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils These packages can be installed by opening up an R session from within the terminal, by typing: R And typing the following command: install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\")) After you have finished installation and testing and downloaded our preworkshop materials, please consider completing our R-tutorial to better familiarize yourself with some basic analysis commands. 2. Python3 (and matplotlib) Python3 is a popular multiuse programming language. This workshop requires Python3 and the matplotlib library. Any version of Python3 is acceptable but a newer version Python3.10.10+ is recommended. Many systems come installed with Python3 by default, but it can be downloaded from the python website . The matplotlib package can be found here . Users of macOs can find detailed instructions on how to install python3 by visiting the following video link that we find very helpful. After installing python3, macOs users can install matplotlib using pip: python3 -m ensurepip sudo python3 -m pip install -U matplotlib For Linux we again recommend that you use the package installer to install python3 and matplotlib sudo apt install python3 # to install python3 python3 --version # to verify that it worked sudo apt-get install python3-matplotlib # to install matplotlib After you have finished with this section and downloaded our preworkshop materials, don't forget to complete our Python tutorial to familiarize yourself with some basic commands.","title":"Software Installs"},{"location":"prep_software/#required-system-software","text":"This workshop requires that the following software be installed and tested before the workshop: Name/Link Description Additional Requirements 1. R Most popular analysis program in statistical genetics Yes (Specific Libraries). 2. Python 3 . Multi purpose Programming Language Yes (The matplotlib library). Both popular computer languages, R and python3 will be used during the workshop. Here we will provide a guide for installation of both languages and their associated libraries.","title":"Required System Software"},{"location":"prep_software/#1-r-and-associated-packages","text":"R is the most popular analysis program in statistical genetics, and required for most genetic analysis. We recommend that MacOS users download an up-to-date (4.4.1) version directly from the R website and using the R installer. If you are having trouble we recommend this video tutorial . While Linux users can also find a suitable version on the website, we recommend using the package installer that comes with Debian based distributions (Debian, Ubuntu, Mint, etc) and downloading R directly using the following terminal commands: sudo apt install r-base # to install R sudo apt install build-essential # to install the essential packages Important: Additional R Packages Are Required This workshop requires the following non-standard R packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils These packages can be installed by opening up an R session from within the terminal, by typing: R And typing the following command: install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\")) After you have finished installation and testing and downloaded our preworkshop materials, please consider completing our R-tutorial to better familiarize yourself with some basic analysis commands.","title":"1. R (and associated packages)"},{"location":"prep_software/#2-python3-and-matplotlib","text":"Python3 is a popular multiuse programming language. This workshop requires Python3 and the matplotlib library. Any version of Python3 is acceptable but a newer version Python3.10.10+ is recommended. Many systems come installed with Python3 by default, but it can be downloaded from the python website . The matplotlib package can be found here . Users of macOs can find detailed instructions on how to install python3 by visiting the following video link that we find very helpful. After installing python3, macOs users can install matplotlib using pip: python3 -m ensurepip sudo python3 -m pip install -U matplotlib For Linux we again recommend that you use the package installer to install python3 and matplotlib sudo apt install python3 # to install python3 python3 --version # to verify that it worked sudo apt-get install python3-matplotlib # to install matplotlib After you have finished with this section and downloaded our preworkshop materials, don't forget to complete our Python tutorial to familiarize yourself with some basic commands.","title":"2. Python3 (and matplotlib)"},{"location":"prep_terminal/","text":"Setting up the Terminal Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this: Bash Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir prsworkshop The mkdir command makes a new directory called prsworkshop that is located in your home directory: Next we can move to this newly created directory by typing: cd prsworkshop Now we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Terminal Setup"},{"location":"prep_terminal/#setting-up-the-terminal","text":"Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this:","title":"Setting up the Terminal"},{"location":"prep_terminal/#bash","text":"Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir prsworkshop The mkdir command makes a new directory called prsworkshop that is located in your home directory: Next we can move to this newly created directory by typing: cd prsworkshop Now we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Bash"},{"location":"prep_testing/","text":"Preworkshop Testing Downloading materials For Linux: Link. For macOS: Link. By clicking the link, then the download ( down-arrow ) icon at the top of the screen. If a message comes up that the file can't be scanned for viruses, please click \"download anyway\". You should already have a folder in your home directory named prsworkshop that you created during the terminal part of this guide by typing \"mkdir ~/prsworkshop\". If you haven't created this folder, please do so now. Next, move to this directory by typing: cd ~/prsworkshop Next unzip the downloaded workshop materials and move them to this directory. This can be down by right clicking on the folder to unzip and then dragging it into the prsworkshop folder, or it can be done in the terminal. In Linux this is accomplished by typing: tar -xvzf ~/Downloads/preworkshop_materials_linux.tar.gz -C ~/prsworkshop cd ~/prsworkshop/preworkshop_materials_linux MacOs sometimes unzips downloaded files for you, so macOs users should first type the following: ls ~/Downloads/preworkshop_materials_mac.* If the command returns a file with a \".tar\" ending, then type: tar -xvf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/prsworkshop Otherwise, if the command returns a file with a \".tar.gz\" ending, type: tar -xvzf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/prsworkshop To unzip the folder and move it to the appropriate directory. Finally, move to the appropriate directory to begin testing: cd ~/prsworkshop/preworkshop_materials_mac Testing Software Warning For macOs Users: MacOs often block executables if they are not approved from the app store. If you see an error similar to this, when trying to run plink, PRSice, or bridgePRS, DO NOT DELETE THE FILE . You must change your system settings to allow downloaded software. To learn how to do so, please click here . Plink From the preworkshop directory, navigate into the Plink directory. cd Plink and type the following command: ./code/plink -h If no error is observed and a list of options are displayed then your computer is ready to run plink. If you are not already familiar with plink, now is a great time to use the data found in Plink/tutorials/sample_data/ to run the PLINK tutorial . Testing PRSice Please navigate to the correct directory: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/PRSice and type the following command: ./code/PRSice -h # macOs users If no error is observed and a list of options are displayed then your computer is ready to run PRSice. Testing bridgePRS To verify that bridgePRS is able to run navigate to the folder: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/BridgePRS cd bridgePRS and type the command: ./bridgePRS You should see bridge art. If this command works, then type the following command: ./bridgePRS check requirements To confirm that your system is up to date, all libraries are installed and you are ready to run bridgePRS.","title":"Testing"},{"location":"prep_testing/#preworkshop-testing","text":"","title":"Preworkshop Testing"},{"location":"prep_testing/#downloading-materials","text":"For Linux: Link. For macOS: Link. By clicking the link, then the download ( down-arrow ) icon at the top of the screen. If a message comes up that the file can't be scanned for viruses, please click \"download anyway\". You should already have a folder in your home directory named prsworkshop that you created during the terminal part of this guide by typing \"mkdir ~/prsworkshop\". If you haven't created this folder, please do so now. Next, move to this directory by typing: cd ~/prsworkshop Next unzip the downloaded workshop materials and move them to this directory. This can be down by right clicking on the folder to unzip and then dragging it into the prsworkshop folder, or it can be done in the terminal. In Linux this is accomplished by typing: tar -xvzf ~/Downloads/preworkshop_materials_linux.tar.gz -C ~/prsworkshop cd ~/prsworkshop/preworkshop_materials_linux MacOs sometimes unzips downloaded files for you, so macOs users should first type the following: ls ~/Downloads/preworkshop_materials_mac.* If the command returns a file with a \".tar\" ending, then type: tar -xvf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/prsworkshop Otherwise, if the command returns a file with a \".tar.gz\" ending, type: tar -xvzf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/prsworkshop To unzip the folder and move it to the appropriate directory. Finally, move to the appropriate directory to begin testing: cd ~/prsworkshop/preworkshop_materials_mac","title":"Downloading materials"},{"location":"prep_testing/#testing-software","text":"Warning For macOs Users: MacOs often block executables if they are not approved from the app store. If you see an error similar to this, when trying to run plink, PRSice, or bridgePRS, DO NOT DELETE THE FILE . You must change your system settings to allow downloaded software. To learn how to do so, please click here .","title":"Testing Software"},{"location":"prep_testing/#plink","text":"From the preworkshop directory, navigate into the Plink directory. cd Plink and type the following command: ./code/plink -h If no error is observed and a list of options are displayed then your computer is ready to run plink. If you are not already familiar with plink, now is a great time to use the data found in Plink/tutorials/sample_data/ to run the PLINK tutorial .","title":"Plink"},{"location":"prep_testing/#testing-prsice","text":"Please navigate to the correct directory: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/PRSice and type the following command: ./code/PRSice -h # macOs users If no error is observed and a list of options are displayed then your computer is ready to run PRSice.","title":"Testing PRSice"},{"location":"prep_testing/#testing-bridgeprs","text":"To verify that bridgePRS is able to run navigate to the folder: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/BridgePRS cd bridgePRS and type the command: ./bridgePRS You should see bridge art. If this command works, then type the following command: ./bridgePRS check requirements To confirm that your system is up to date, all libraries are installed and you are ready to run bridgePRS.","title":"Testing bridgePRS"},{"location":"prog_comingsoon/","text":"Start Date: Aug 31, 2024 Schedule Coming Soon. Please see the university website for more information. Contact For questions about this website, or the preliminary materials please contact Dr Tade Souaiaia , or Dr Paul O'Reilly .","title":"Prog comingsoon"},{"location":"prog_comingsoon/#start-date-aug-31-2024","text":"","title":"Start Date: Aug 31, 2024"},{"location":"prog_comingsoon/#schedule","text":"Coming Soon. Please see the university website for more information.","title":"Schedule"},{"location":"prog_comingsoon/#contact","text":"For questions about this website, or the preliminary materials please contact Dr Tade Souaiaia , or Dr Paul O'Reilly .","title":"Contact"},{"location":"tut_R/","text":"Introduction to R R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop. Basics To being type R in the terminal: R Libraries Most functionality of R is organised in packages or libraries. To access these functions, we will have to install and load these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Variables in R You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\") Functions You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2) Plotting While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point() Regression Models In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial))","title":"R Tutorial"},{"location":"tut_R/#introduction-to-r","text":"R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop.","title":"Introduction to R"},{"location":"tut_R/#basics","text":"To being type R in the terminal: R","title":"Basics"},{"location":"tut_R/#libraries","text":"Most functionality of R is organised in packages or libraries. To access these functions, we will have to install and load these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2)","title":"Libraries"},{"location":"tut_R/#variables-in-r","text":"You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\")","title":"Variables in R"},{"location":"tut_R/#functions","text":"You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2)","title":"Functions"},{"location":"tut_R/#plotting","text":"While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point()","title":"Plotting"},{"location":"tut_R/#regression-models","text":"In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial))","title":"Regression Models"},{"location":"tut_bash/","text":"Bash/Shell Tutorial The ability to navigate the filesystem using bash is a very important skill in statistical genetics. Bash is a programming language commonly used to navigate the terminal and manipulate files and folders. Some terminals run \"bash-like\" scripting languages like \"zsh\". For the purposes of this tutorial, they are indistinguishable from bash. There are approximately 50 bash commands that used 95% of the time . Here we will review some common commands and do some simple file analysis. Common Commands cd This command is used to change our directory, in the following manner: cd $PATH where $PATH represents the path to the target directory. Common usage of cd includes: cd ~/ # will bring you to your home directory cd $HOME # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd Downloads # will bring you to the Downloads directory, provided that you are in the home directory cd ~/Downloads # will bring you to Downloads, no matter where you are in the filesystem ls This command allows you to look at the contents of a directory: ls Some common usage of ls includes: ls # list the contents of the current directory ls ~ # list the contents of the home directory ls ~/Desktop # Will list the contents of the Desktop For ls , there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size mkdir This command to create a new directory in your home folder: mkdir ~/test_directory The following commands should be carried out within this directory Use cd to enter the directory: cd ~/test_directory wsl echo Can be used to print to screen: echo \"Hello\" touch Can be used to create a new (empty) file: touch foo rm Can be used to create delete a file: rm foo > The carrot sign can be used to send the output to a file: echo \"Hello\" > output.txt cat Can be used to print the contents of a file to the screen: cat output.txt cp Can be used to copy a a file: cat output.txt output2.txt nano Nano can be used to edit a file, typing: nano data1.txt Will bring up an editor window: Starting from the top line type: bob 1 fred 2 mary 3 noah 4 sally 5 And then save the file using Ctrl-O and press enter, and quit using Ctrl-X . wc The word count command can be used to count the number of lines or words in a file: wc -l data1.txt grep Can be used to search a file for content: grep \"noah\" data1.text While return all the lines in data1.txt containing the search term \"noah\". File Analysis A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new file that squares the data in our previous file: awk '{print $1,$2*$2}' data1.txt > data2.txt We can also use awk to add up all the squared data values: awk '{X+=$2} END{print $2}' data2.txt","title":"Bash Tutorial"},{"location":"tut_bash/#bashshell-tutorial","text":"The ability to navigate the filesystem using bash is a very important skill in statistical genetics. Bash is a programming language commonly used to navigate the terminal and manipulate files and folders. Some terminals run \"bash-like\" scripting languages like \"zsh\". For the purposes of this tutorial, they are indistinguishable from bash. There are approximately 50 bash commands that used 95% of the time . Here we will review some common commands and do some simple file analysis.","title":"Bash/Shell Tutorial"},{"location":"tut_bash/#common-commands","text":"","title":"Common Commands"},{"location":"tut_bash/#cd","text":"This command is used to change our directory, in the following manner: cd $PATH where $PATH represents the path to the target directory. Common usage of cd includes: cd ~/ # will bring you to your home directory cd $HOME # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd Downloads # will bring you to the Downloads directory, provided that you are in the home directory cd ~/Downloads # will bring you to Downloads, no matter where you are in the filesystem","title":"cd"},{"location":"tut_bash/#ls","text":"This command allows you to look at the contents of a directory: ls Some common usage of ls includes: ls # list the contents of the current directory ls ~ # list the contents of the home directory ls ~/Desktop # Will list the contents of the Desktop For ls , there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size","title":"ls"},{"location":"tut_bash/#mkdir","text":"This command to create a new directory in your home folder: mkdir ~/test_directory The following commands should be carried out within this directory Use cd to enter the directory: cd ~/test_directory wsl","title":"mkdir"},{"location":"tut_bash/#echo","text":"Can be used to print to screen: echo \"Hello\"","title":"echo"},{"location":"tut_bash/#touch","text":"Can be used to create a new (empty) file: touch foo","title":"touch"},{"location":"tut_bash/#rm","text":"Can be used to create delete a file: rm foo","title":"rm"},{"location":"tut_bash/#_1","text":"The carrot sign can be used to send the output to a file: echo \"Hello\" > output.txt","title":"&gt;"},{"location":"tut_bash/#cat","text":"Can be used to print the contents of a file to the screen: cat output.txt","title":"cat"},{"location":"tut_bash/#cp","text":"Can be used to copy a a file: cat output.txt output2.txt","title":"cp"},{"location":"tut_bash/#nano","text":"Nano can be used to edit a file, typing: nano data1.txt Will bring up an editor window: Starting from the top line type: bob 1 fred 2 mary 3 noah 4 sally 5 And then save the file using Ctrl-O and press enter, and quit using Ctrl-X .","title":"nano"},{"location":"tut_bash/#wc","text":"The word count command can be used to count the number of lines or words in a file: wc -l data1.txt","title":"wc"},{"location":"tut_bash/#grep","text":"Can be used to search a file for content: grep \"noah\" data1.text While return all the lines in data1.txt containing the search term \"noah\".","title":"grep"},{"location":"tut_bash/#file-analysis","text":"A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new file that squares the data in our previous file: awk '{print $1,$2*$2}' data1.txt > data2.txt We can also use awk to add up all the squared data values: awk '{X+=$2} END{print $2}' data2.txt","title":"File Analysis"},{"location":"tut_intro/","text":"Introduction Here we present preworkshop tutorials for bash/shell , R , and Python3 , and Plink . Students who are not already very familiar with these programs are required to complete these tutorials. The first three tutorials do not require any data and can be completed at anytime while the last tutorial (Plink) requires sample data downloaded in Testing . When the workshop begins students will be expected to understand the following: Bash : Navigate filesystem, copy/move/unzip files. Create directories and edit files (using nano). R : Install packages, read from file, manipulate variables, understand functions, create plots. Python : Import libraries, create variables, understand list comprehension, basic plotting. Plink : Explore and genderate GWAS data, recode and reorder allelic data, use PLINK website.","title":"Introduction"},{"location":"tut_intro/#introduction","text":"Here we present preworkshop tutorials for bash/shell , R , and Python3 , and Plink . Students who are not already very familiar with these programs are required to complete these tutorials. The first three tutorials do not require any data and can be completed at anytime while the last tutorial (Plink) requires sample data downloaded in Testing . When the workshop begins students will be expected to understand the following: Bash : Navigate filesystem, copy/move/unzip files. Create directories and edit files (using nano). R : Install packages, read from file, manipulate variables, understand functions, create plots. Python : Import libraries, create variables, understand list comprehension, basic plotting. Plink : Explore and genderate GWAS data, recode and reorder allelic data, use PLINK website.","title":"Introduction"},{"location":"tut_plink/","text":"Introduction to PLINK (Part I) PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials. Sample Data This tutorial runs on the data in the pre-workshop materials downloaded here . If you have followed the previous directions, this data should be in: ~/prsworkshop/preworkshop_materials_(\"your OS\")/Plink/tutorial/sample_data . \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website Select and exclude lists of samples and SNPs In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click ) Exploring Data To begin the tutorial please navigate to: cd ~/prsworkshop/preworkshop_materials_(\"yourOS\")/Plink/tutorial/sample_data First let's make sure we can call plink from this directory: ../../code/plink Next lets observe the files in the sample data directory: ls We should see the following four files: D1D.map D1D.pcs1234 D1D.ped D1D.pheno1 Let's look each each file by typing the following commands and pressing q to quit after each one: less D1D.map # A map file that associates snps with chromosomal locations less D1D.pcs1234 # A PCA file that lists individuals first four prinicipal components less D1D.ped # A pedigree file that lists individuals genotypes less D1D.pheno1 # A phenotype file that lists individuals phenotypes Next to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side). What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file? Manipulating Data Create 'binary' format PLINK files using the recode command: ../../codeplink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Examine the files ending .bim and .fam. Do not open the .bed file. Examine the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary) Recoding alleles as counts Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: ../../code/plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be? PLINK website Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists. Write SNP list and extract SNPs You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!! Performing QC & GWAS (Part II) Here we will work on the following skills: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS Generate summaries to perform QC There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC. Individual missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"? SNP Missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful? Hardy-Weinberg Equilibrium Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why? Allele frequencies Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way Apply QC filters There are di\ufb00erent strategies for performing QC on your data: (a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice. Apply individual missingness thresholds Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening? Apply SNP missingness and MAF thresholds Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness? Apply Hardy-Weinberg thresholds Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP. Perform GWAS Case/Control GWAS - no covariates Run the following code, which performs a genetic association study using logistic regression on some case/control data: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations? Case/Control GWAS - with covariates Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Plink Tutorial"},{"location":"tut_plink/#introduction-to-plink-part-i","text":"PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials.","title":"Introduction to PLINK  (Part I)"},{"location":"tut_plink/#sample-data","text":"This tutorial runs on the data in the pre-workshop materials downloaded here . If you have followed the previous directions, this data should be in: ~/prsworkshop/preworkshop_materials_(\"your OS\")/Plink/tutorial/sample_data . \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website Select and exclude lists of samples and SNPs In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click )","title":"Sample Data"},{"location":"tut_plink/#exploring-data","text":"To begin the tutorial please navigate to: cd ~/prsworkshop/preworkshop_materials_(\"yourOS\")/Plink/tutorial/sample_data First let's make sure we can call plink from this directory: ../../code/plink Next lets observe the files in the sample data directory: ls We should see the following four files: D1D.map D1D.pcs1234 D1D.ped D1D.pheno1 Let's look each each file by typing the following commands and pressing q to quit after each one: less D1D.map # A map file that associates snps with chromosomal locations less D1D.pcs1234 # A PCA file that lists individuals first four prinicipal components less D1D.ped # A pedigree file that lists individuals genotypes less D1D.pheno1 # A phenotype file that lists individuals phenotypes Next to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side). What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file?","title":"Exploring Data"},{"location":"tut_plink/#manipulating-data","text":"Create 'binary' format PLINK files using the recode command: ../../codeplink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Examine the files ending .bim and .fam. Do not open the .bed file. Examine the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary)","title":"Manipulating Data"},{"location":"tut_plink/#recoding-alleles-as-counts","text":"Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: ../../code/plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be?","title":"Recoding alleles as counts"},{"location":"tut_plink/#plink-website","text":"Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists.","title":"PLINK website"},{"location":"tut_plink/#write-snp-list-and-extract-snps","text":"You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!!","title":"Write SNP list and extract SNPs"},{"location":"tut_plink/#performing-qc-gwas-part-ii","text":"Here we will work on the following skills: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS","title":"Performing QC &amp; GWAS (Part II)"},{"location":"tut_plink/#generate-summaries-to-perform-qc","text":"There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC.","title":"Generate summaries to perform QC"},{"location":"tut_plink/#individual-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"?","title":"Individual missingness"},{"location":"tut_plink/#snp-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful?","title":"SNP Missingness"},{"location":"tut_plink/#hardy-weinberg-equilibrium","text":"Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why?","title":"Hardy-Weinberg Equilibrium"},{"location":"tut_plink/#allele-frequencies","text":"Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way","title":"Allele frequencies"},{"location":"tut_plink/#apply-qc-filters","text":"","title":"Apply QC filters"},{"location":"tut_plink/#there-are-different-strategies-for-performing-qc-on-your-data","text":"(a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice.","title":"There are di\ufb00erent strategies for performing QC on your data:"},{"location":"tut_plink/#apply-individual-missingness-thresholds","text":"Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening?","title":"Apply individual missingness thresholds"},{"location":"tut_plink/#apply-snp-missingness-and-maf-thresholds","text":"Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness?","title":"Apply SNP missingness and MAF thresholds"},{"location":"tut_plink/#apply-hardy-weinberg-thresholds","text":"Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP.","title":"Apply Hardy-Weinberg thresholds"},{"location":"tut_plink/#perform-gwas","text":"","title":"Perform GWAS"},{"location":"tut_plink/#casecontrol-gwas-no-covariates","text":"Run the following code, which performs a genetic association study using logistic regression on some case/control data: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations?","title":"Case/Control GWAS - no covariates"},{"location":"tut_plink/#casecontrol-gwas-with-covariates","text":"Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Case/Control GWAS - with covariates"},{"location":"tut_python/","text":"Introduction to Python Python is a useful general purpose programming language. Here we will go over some basic python commands from an interactive shell. To being open python3 by typing: python3 Libraries Downloaded python libraries can be imported interactively: import matplotlib.pyplot as plt Variables in Python You can assign a value or values to any variable you want using the equals sign: X = 5 # X is an integer (5) X = [1,2,3,4,5] # X is a list containing the first five integers Functions can also be used: X = range(-10,11,1) # X includes all integers between -10 and 10 counting by 1 New variables can be declared with lists: Y = [x*x for x in X] # Y is equal to X squared A figure can be generated: plt.plot(X,Y) And viewed: plt.show()","title":"Python Tutorial"},{"location":"tut_python/#introduction-to-python","text":"Python is a useful general purpose programming language. Here we will go over some basic python commands from an interactive shell. To being open python3 by typing: python3","title":"Introduction to Python"},{"location":"tut_python/#libraries","text":"Downloaded python libraries can be imported interactively: import matplotlib.pyplot as plt","title":"Libraries"},{"location":"tut_python/#variables-in-python","text":"You can assign a value or values to any variable you want using the equals sign: X = 5 # X is an integer (5) X = [1,2,3,4,5] # X is a list containing the first five integers Functions can also be used: X = range(-10,11,1) # X includes all integers between -10 and 10 counting by 1 New variables can be declared with lists: Y = [x*x for x in X] # Y is equal to X squared A figure can be generated: plt.plot(X,Y) And viewed: plt.show()","title":"Variables in Python"},{"location":"images/base/","text":"A place to put the images for the course","title":"Base"},{"location":"images/Day2.docxfolder/Day2.docx/","text":"Polygenic Risk Score Analyses Workshop 2022 {width=\"2.939998906386702in\" height=\"2.8874989063867016in\"} Day 2: Introduction to Polygenic Risk Scores Day 2 Timetable +------------+--------+-------------------------+---------------------+ | > Time | > T | | > **Presenter | | | itle** | | | +============+========+=========================+=====================+ | > 9:00 - | A | | > - | | > 9:15 | rrival | | | +------------+--------+-------------------------+---------------------+ | > 9:15 - | [Lec | > Introduction to PRS I | > Dr Paul O'Reilly | | > 10:30 | ture]{ | | | | | .under | | | | | line}: | | | +------------+--------+-------------------------+---------------------+ | 10:30 - | > | | > - | | 11:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 11:00 - | > [Lec | | > Dr Paul O'Reilly | | 12:00 | ture]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 12:00 - | > | | > - | | 13:30 | Lunch | | | +------------+--------+-------------------------+---------------------+ | 13:30 - | > | | > Dr Conrad Iyegbe | | 14:30 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > I | | | +------------+--------+-------------------------+---------------------+ | 14:30 - | > | | > - | | 15:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 15:00 - | > | | > Dr Conrad Iyegbe | | 16:00 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 16:00 - | > [S | | > Dr Nicki Tiffin | | 17:00 | pecial | | | | | > Sem | | | | | inar]{ | | | | | .under | | | | | line}: | | | | | > Pol | | | | | ygenic | | | | | > Risk | | | | | > | | | | | Scores | | | | | > | | | | | > + | | | | | > and | | | | | > | | | | | Ethics | | | +------------+--------+-------------------------+---------------------+ Contents Day 2 Timetable 1 Day 2 Timetable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Introduction to Polygenic Score > Analyses 3 Key Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . 3 Resources you will be using . . . . . . . . . . . . . . . . . . . . . . 3 Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Understanding GWAS Summary Statistics . . . . . . . . . . . . . . 5 Matching the Base and Target Data sets . . . . . . . . . . . . . . . 6 Linkage Disequilibrium in PRS Analyses . . . . . . . . . . . . . . . 7 Performing Clumping . . . . . . . . . . . . . . . . . . . . . . 7 P-Value Thresholding . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Height PRS using GW-significant SNPs only . . . . . . . . . 8 Height PRS across multiple P-value thresholds 10 High Resolution Scoring 12 Stratifying Samples by PRS 15 Case Control Studies 17 Cross-Trait Analysis 18 Introduction to Polygenic Score Analyses Key Learning Outcomes After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice (Euesden, Lewis & O'Reilly 2015; Choi & O'Reilly 2019) Interpret results generated from PRS analyses Customise visualisation of results Resources you will be using To perform PRS analyses, summary statistics from Genome Wide Association Stud- ies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals ( wood_defining_2014 ) Download Link Coronary artery disease (CAD) CARDIoGRAM plus C4D consortium GWAS on 60,801 CAD cases and 123,504 controls Download Link (consortium_comprehensive_2015) Data Structure You will find all practical materials in the PRS_Workshop/Day_2 directory. Relevant materials that you should see there at the start of the practical are as follows: Practical Base_Data GIANT_Height.txt cad.add.txt cad.add.readme Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.cad TAR.covariate Software plink_mac plink_linux plink.exe PRSice.R PRSice_mac PRSice_linux PRSice_win64.exe {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"} Introduction A PRS is a (usually weak) estimate of an individual's genetic propensity to a pheno- type, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS. Understanding GWAS Summary Statistics When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymor- phism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Matching the Base and Target Data sets The first step in PRS calculation is to ensure consistency between the GWAS sum- mary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs - and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed, where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"} Linkage Disequilibrium in PRS Analyses GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes iden- tifying the independent genetic effects (or their best proxies if these are not geno- typed/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of meth- ods using the different options to date ( mak_polygenic_2017 ). In this work- shop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LD- pred ( vilhjalmsson_modeling_2015 ) and lassosum ( mak_polygenic_2017 ) papers. Performing Clumping Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 > 0.1, with \ud835\udc5f 2 typically calculated from \ud835\udc5d\u210e\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \u210e\ud835\udc4e\ud835\udc5d\ud835\udc59\ud835\udc5c\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52 data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( chang_second_2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: 1 cd \\~/Desktop/PRS\\_Workshop/ Next type the following command (NB. See warning below): 1 ./Software/plink_linux 2 --bfile Target_Data/TAR 3 --clump Base_Data/GIANT_Height.txt 4 --clump-p1 1 5 --clump-snp-field MarkerName 6 --clump-field p 7 --clump-kb 250 8 --clump-r2 0.1 9 --out Results/Height {width=\"0.3229155730533683in\" height=\"0.3229166666666667in\"} The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 > 0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} P-Value Thresholding Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43 -value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43 -value thresholds. Height PRS using GW-significant SNPs only Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the PRS_Workshop/Day_2 directory, run the following command in the terminal: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --bar-levels 5e-8 14 --no-full 15 --fastscore 16 --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID (--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43 -value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addi- tion, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43 -value \\< 5*\u00d7*10 *\u2212*8 . {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the em- pirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"} Height PRS across multiple P-value thresholds A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43 -value threshold provides the \\\"best\\\" predic- tion for our particular data, then we can calculate the PRS under several \ud835\udc43 -value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. []{#_bookmark13 .anchor}Figure 1.1: BARPLOT generated by PRSice {width=\"2.7440616797900264in\" height=\"2.761874453193351in\"} See dudbridge_power_2013 for theory on factors affecting the best-fit PRS). This process is implemented in PRSice and can be performed automatically as fol- lows: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --fastscore 15 --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT (fig. 1.1) generated by PRSice {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} High Resolution Scoring If we limit ourselves to a small number of \ud835\udc43 -value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a large number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot (fig. 1.2) presenting the model fit of PRS calculated at all P-value thresholds. []{#_bookmark15 .anchor}Figure 1.2: High Resolution Plot generated by PRSice {width=\"2.463332239720035in\" height=\"2.455in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user in- puts, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --out Results/Height.sex When covariates were include in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43 \ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by minusing the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"}\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43 \ud835\udc45\ud835\udc46). Usually, with categorical variables, dummy variables have to be generated to rep- resent the different categories. Alternatively, residualized phenotype, generated by regresing the covariates against the phenotype, can be used for downstream anal- yses. A useful feature of PRSice is to automatically generate the dummy variable for users. This can be achieved with the following command: 1 Rscript.exe ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --cov-factor Sex 16 --out Results/Height.sex []{#_bookmark16 .anchor}Figure 1.3: Example of a quantile plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"} Stratifying Samples by PRS An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot (fig. 1.3). To generate quantile plots in PRSice, simply add --quantile 10 option. {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 10 17 --out Results/Height.sex {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} Figure 1.4: Example of a strata plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"} A disadvantage of the quantile plot is that it only seperate samples into quantiles of equal size. However, it is somtimes interesting to investigate whether a specific strata (e.g. top 5% of samples), contain a higher PRS than the reference strata. For example, mavaddat_prediction_2015 found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break, which represents the upper bound of each strata, and --quant-ref, which represents the upper bound of the reference quantile: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 100 17 --quant-break 1,5,10,20,40,60,80,90,95,99,100 18 --quant-ref 60 19 --out Results/Height.sex {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} Case Control Studies In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Tar- get_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/cad.add.txt 4 --target Target_Data/TAR 5 --snp markername 6 --A1 effect_allele 7 --A2 noneffect_allele 8 --chr chr 9 --bp bp_hg19 10 --stat beta 11 --beta 12 --pvalue p_dgc 13 --pheno Target_Data/CAD.pheno 14 --binary-target T 15 --out Results/CAD.highres {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"3.5316666666666667in\" height=\"2.066457786526684in\"} []{#_bookmark19 .anchor}Figure 1.5: Plot taken from Ruderfer et al. 2014 {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Cross-Trait Analysis A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ruderfer_polygenic_2014 (fig. 1.5), which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/CAD.pheno 12 --binary-target T 13 --out Results/Cross.highres {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Day2.docx"},{"location":"images/Day2.docxfolder/Day2.docx/#day-2-timetable","text":"+------------+--------+-------------------------+---------------------+ | > Time | > T | | > **Presenter | | | itle** | | | +============+========+=========================+=====================+ | > 9:00 - | A | | > - | | > 9:15 | rrival | | | +------------+--------+-------------------------+---------------------+ | > 9:15 - | [Lec | > Introduction to PRS I | > Dr Paul O'Reilly | | > 10:30 | ture]{ | | | | | .under | | | | | line}: | | | +------------+--------+-------------------------+---------------------+ | 10:30 - | > | | > - | | 11:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 11:00 - | > [Lec | | > Dr Paul O'Reilly | | 12:00 | ture]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 12:00 - | > | | > - | | 13:30 | Lunch | | | +------------+--------+-------------------------+---------------------+ | 13:30 - | > | | > Dr Conrad Iyegbe | | 14:30 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > I | | | +------------+--------+-------------------------+---------------------+ | 14:30 - | > | | > - | | 15:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 15:00 - | > | | > Dr Conrad Iyegbe | | 16:00 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 16:00 - | > [S | | > Dr Nicki Tiffin | | 17:00 | pecial | | | | | > Sem | | | | | inar]{ | | | | | .under | | | | | line}: | | | | | > Pol | | | | | ygenic | | | | | > Risk | | | | | > | | | | | Scores | | | | | > | | | | | > + | | | | | > and | | | | | > | | | | | Ethics | | | +------------+--------+-------------------------+---------------------+ Contents Day 2 Timetable 1 Day 2 Timetable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Introduction to Polygenic Score > Analyses 3 Key Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . 3 Resources you will be using . . . . . . . . . . . . . . . . . . . . . . 3 Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Understanding GWAS Summary Statistics . . . . . . . . . . . . . . 5 Matching the Base and Target Data sets . . . . . . . . . . . . . . . 6 Linkage Disequilibrium in PRS Analyses . . . . . . . . . . . . . . . 7 Performing Clumping . . . . . . . . . . . . . . . . . . . . . . 7 P-Value Thresholding . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Height PRS using GW-significant SNPs only . . . . . . . . . 8 Height PRS across multiple P-value thresholds 10 High Resolution Scoring 12 Stratifying Samples by PRS 15 Case Control Studies 17 Cross-Trait Analysis 18","title":"Day 2 Timetable"},{"location":"images/Day2.docxfolder/Day2.docx/#introduction-to-polygenic-score-analyses","text":"","title":"Introduction to Polygenic Score Analyses"},{"location":"images/Day2.docxfolder/Day2.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice (Euesden, Lewis & O'Reilly 2015; Choi & O'Reilly 2019) Interpret results generated from PRS analyses Customise visualisation of results","title":"Key Learning Outcomes"},{"location":"images/Day2.docxfolder/Day2.docx/#resources-you-will-be-using","text":"To perform PRS analyses, summary statistics from Genome Wide Association Stud- ies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals ( wood_defining_2014 ) Download Link Coronary artery disease (CAD) CARDIoGRAM plus C4D consortium GWAS on 60,801 CAD cases and 123,504 controls Download Link","title":"Resources you will be using"},{"location":"images/Day2.docxfolder/Day2.docx/#consortium_comprehensive_2015","text":"","title":"(consortium_comprehensive_2015)"},{"location":"images/Day2.docxfolder/Day2.docx/#data-structure","text":"You will find all practical materials in the PRS_Workshop/Day_2 directory. Relevant materials that you should see there at the start of the practical are as follows: Practical Base_Data GIANT_Height.txt cad.add.txt cad.add.readme Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.cad TAR.covariate Software plink_mac plink_linux plink.exe PRSice.R PRSice_mac PRSice_linux PRSice_win64.exe {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"}","title":"Data Structure"},{"location":"images/Day2.docxfolder/Day2.docx/#introduction","text":"A PRS is a (usually weak) estimate of an individual's genetic propensity to a pheno- type, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS.","title":"Introduction"},{"location":"images/Day2.docxfolder/Day2.docx/#understanding-gwas-summary-statistics","text":"When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymor- phism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Understanding GWAS Summary Statistics"},{"location":"images/Day2.docxfolder/Day2.docx/#matching-the-base-and-target-data-sets","text":"The first step in PRS calculation is to ensure consistency between the GWAS sum- mary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs - and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed, where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"}","title":"Matching the Base and Target Data sets"},{"location":"images/Day2.docxfolder/Day2.docx/#linkage-disequilibrium-in-prs-analyses","text":"GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes iden- tifying the independent genetic effects (or their best proxies if these are not geno- typed/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of meth- ods using the different options to date ( mak_polygenic_2017 ). In this work- shop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LD- pred ( vilhjalmsson_modeling_2015 ) and lassosum ( mak_polygenic_2017 ) papers.","title":"Linkage Disequilibrium in PRS Analyses"},{"location":"images/Day2.docxfolder/Day2.docx/#performing-clumping","text":"Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 > 0.1, with \ud835\udc5f 2 typically calculated from \ud835\udc5d\u210e\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \u210e\ud835\udc4e\ud835\udc5d\ud835\udc59\ud835\udc5c\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52 data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( chang_second_2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: 1 cd \\~/Desktop/PRS\\_Workshop/ Next type the following command (NB. See warning below): 1 ./Software/plink_linux 2 --bfile Target_Data/TAR 3 --clump Base_Data/GIANT_Height.txt 4 --clump-p1 1 5 --clump-snp-field MarkerName 6 --clump-field p 7 --clump-kb 250 8 --clump-r2 0.1 9 --out Results/Height {width=\"0.3229155730533683in\" height=\"0.3229166666666667in\"} The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 > 0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Performing Clumping"},{"location":"images/Day2.docxfolder/Day2.docx/#p-value-thresholding","text":"Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43 -value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43 -value thresholds.","title":"P-Value Thresholding"},{"location":"images/Day2.docxfolder/Day2.docx/#height-prs-using-gw-significant-snps-only","text":"Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the PRS_Workshop/Day_2 directory, run the following command in the terminal: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --bar-levels 5e-8 14 --no-full 15 --fastscore 16 --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID (--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43 -value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addi- tion, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43 -value \\< 5*\u00d7*10 *\u2212*8 . {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the em- pirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"}","title":"Height PRS using GW-significant SNPs only"},{"location":"images/Day2.docxfolder/Day2.docx/#height-prs-across-multiple-p-value-thresholds","text":"A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43 -value threshold provides the \\\"best\\\" predic- tion for our particular data, then we can calculate the PRS under several \ud835\udc43 -value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. []{#_bookmark13 .anchor}Figure 1.1: BARPLOT generated by PRSice {width=\"2.7440616797900264in\" height=\"2.761874453193351in\"} See dudbridge_power_2013 for theory on factors affecting the best-fit PRS). This process is implemented in PRSice and can be performed automatically as fol- lows: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --fastscore 15 --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT (fig. 1.1) generated by PRSice {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Height PRS across multiple P-value thresholds"},{"location":"images/Day2.docxfolder/Day2.docx/#high-resolution-scoring","text":"If we limit ourselves to a small number of \ud835\udc43 -value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a large number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot (fig. 1.2) presenting the model fit of PRS calculated at all P-value thresholds. []{#_bookmark15 .anchor}Figure 1.2: High Resolution Plot generated by PRSice {width=\"2.463332239720035in\" height=\"2.455in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user in- puts, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --out Results/Height.sex When covariates were include in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43 \ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by minusing the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"}\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43 \ud835\udc45\ud835\udc46). Usually, with categorical variables, dummy variables have to be generated to rep- resent the different categories. Alternatively, residualized phenotype, generated by regresing the covariates against the phenotype, can be used for downstream anal- yses. A useful feature of PRSice is to automatically generate the dummy variable for users. This can be achieved with the following command: 1 Rscript.exe ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --cov-factor Sex 16 --out Results/Height.sex []{#_bookmark16 .anchor}Figure 1.3: Example of a quantile plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"}","title":"High Resolution Scoring"},{"location":"images/Day2.docxfolder/Day2.docx/#stratifying-samples-by-prs","text":"An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot (fig. 1.3). To generate quantile plots in PRSice, simply add --quantile 10 option. {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 10 17 --out Results/Height.sex {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} Figure 1.4: Example of a strata plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"} A disadvantage of the quantile plot is that it only seperate samples into quantiles of equal size. However, it is somtimes interesting to investigate whether a specific strata (e.g. top 5% of samples), contain a higher PRS than the reference strata. For example, mavaddat_prediction_2015 found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break, which represents the upper bound of each strata, and --quant-ref, which represents the upper bound of the reference quantile: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 100 17 --quant-break 1,5,10,20,40,60,80,90,95,99,100 18 --quant-ref 60 19 --out Results/Height.sex {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"}","title":"Stratifying Samples by PRS"},{"location":"images/Day2.docxfolder/Day2.docx/#case-control-studies","text":"In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Tar- get_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/cad.add.txt 4 --target Target_Data/TAR 5 --snp markername 6 --A1 effect_allele 7 --A2 noneffect_allele 8 --chr chr 9 --bp bp_hg19 10 --stat beta 11 --beta 12 --pvalue p_dgc 13 --pheno Target_Data/CAD.pheno 14 --binary-target T 15 --out Results/CAD.highres {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"3.5316666666666667in\" height=\"2.066457786526684in\"} []{#_bookmark19 .anchor}Figure 1.5: Plot taken from Ruderfer et al. 2014 {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Case Control Studies"},{"location":"images/Day2.docxfolder/Day2.docx/#cross-trait-analysis","text":"A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ruderfer_polygenic_2014 (fig. 1.5), which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/CAD.pheno 12 --binary-target T 13 --out Results/Cross.highres {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Cross-Trait Analysis"},{"location":"images/day3/base/","text":"base","title":"Base"},{"location":"old/Day1a.docx/","text":"Polygenic Risk Score Analyses Workshop 2024 Day 1a: GWAS & relevant Statistics Introduction to Bash Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment. Moving around the File System To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/ Looking at the Current Directory Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size Counting Number of Lines in File We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt Search File Content Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l Filtering and Reshu\ufb04ing Files A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if( \\(7<5e-8)\" and \"\\) 7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met. Introduction to R R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop. Basics If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R Working Directory When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\") Libraries Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet). Variables in R You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\") Functions You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2) Plotting While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point() Regression Models In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Polygenic Risk Score Analyses Workshop 2024"},{"location":"old/Day1a.docx/#polygenic-risk-score-analyses-workshop-2024","text":"","title":"Polygenic Risk Score Analyses Workshop 2024"},{"location":"old/Day1a.docx/#day-1a-gwas-relevant-statistics","text":"","title":"Day 1a: GWAS &amp; relevant Statistics"},{"location":"old/Day1a.docx/#introduction-to-bash","text":"Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment.","title":"Introduction to Bash"},{"location":"old/Day1a.docx/#moving-around-the-file-system","text":"To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/","title":"Moving around the File System"},{"location":"old/Day1a.docx/#looking-at-the-current-directory","text":"Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size","title":"Looking at the Current Directory"},{"location":"old/Day1a.docx/#counting-number-of-lines-in-file","text":"We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt","title":"Counting Number of Lines in File"},{"location":"old/Day1a.docx/#search-file-content","text":"Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l","title":"Search File Content"},{"location":"old/Day1a.docx/#filtering-and-reshuffling-files","text":"A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if( \\(7<5e-8)\" and \"\\) 7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met.","title":"Filtering and Reshu\ufb04ing Files"},{"location":"old/Day1a.docx/#introduction-to-r","text":"R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop.","title":"Introduction to R"},{"location":"old/Day1a.docx/#basics","text":"If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R","title":"Basics"},{"location":"old/Day1a.docx/#working-directory","text":"When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\")","title":"Working Directory"},{"location":"old/Day1a.docx/#libraries","text":"Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet).","title":"Libraries"},{"location":"old/Day1a.docx/#variables-in-r","text":"You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\")","title":"Variables in R"},{"location":"old/Day1a.docx/#functions","text":"You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2)","title":"Functions"},{"location":"old/Day1a.docx/#plotting","text":"While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point()","title":"Plotting"},{"location":"old/Day1a.docx/#regression-models","text":"In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Regression Models"},{"location":"old/misc_glossary/","text":"Linux for Windows Users WSL For windows users, a single WSL can be used to install linux. Detailed information on WSL can be found on the microsoft website . Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. Once it has been installed you will need to create a new user account, to learn more about this please see microsoft best practices Once finished you can run linux using windows terminal or by going to the start menu and typing \"Ubuntu\". This will open Linux in its own console window.","title":"Misc glossary"},{"location":"old/misc_glossary/#linux-for-windows-users","text":"","title":"Linux for Windows Users"},{"location":"old/misc_glossary/#wsl","text":"For windows users, a single WSL can be used to install linux. Detailed information on WSL can be found on the microsoft website . Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. Once it has been installed you will need to create a new user account, to learn more about this please see microsoft best practices Once finished you can run linux using windows terminal or by going to the start menu and typing \"Ubuntu\". This will open Linux in its own console window.","title":"WSL"},{"location":"old/prep_terminal_backup/","text":"Setting up the Terminal Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this: Bash Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir prsworkshop The mkdir command makes a new directory called prsworkshop that is located in your home directory: Next we can move to this newly created directory by typing: cd prsworkshop To verify that we have successfully navigated to the prsworkshop directory we can use two commands the we have previously discussed. Fist, can you recall the command to list the current path? Can you predict what it will return? goal identify current path command pwd [Result] $ HOME / prsworkshop Next, can you recall the command to list the contents of the directory? Can you predict what it will return? goal list directory contents command ls [Result] # nothing is returned because the new prsworkshop directory is empty Hopefully, you were able to answer correctly above and understand that you are now in the empty prsworkshop directory. Next we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Setting up the Terminal "},{"location":"old/prep_terminal_backup/#setting-up-the-terminal","text":"Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this:","title":"Setting up the Terminal"},{"location":"old/prep_terminal_backup/#bash","text":"Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir prsworkshop The mkdir command makes a new directory called prsworkshop that is located in your home directory: Next we can move to this newly created directory by typing: cd prsworkshop To verify that we have successfully navigated to the prsworkshop directory we can use two commands the we have previously discussed. Fist, can you recall the command to list the current path? Can you predict what it will return? goal identify current path command pwd [Result] $ HOME / prsworkshop Next, can you recall the command to list the contents of the directory? Can you predict what it will return? goal list directory contents command ls [Result] # nothing is returned because the new prsworkshop directory is empty Hopefully, you were able to answer correctly above and understand that you are now in the empty prsworkshop directory. Next we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Bash"},{"location":"old/quik_demo/","text":"Running SibArc: Running SibArc on a trait with de-novo tail architecture From the test directory type: ./sibArc.py ../data/trait1-test.csv --out denovoExample This will produce the following output files: denovoExample.result.out, denovoExample.fig.png, denovoExample.fig.pdf Running SibArc on a trait with Mendelian tail architecture From the test directory type: ./sibArc.py ../data/trait2-test.csv --out mendelianExample This will produce the following output files: mendelianExample.result.out, mendelianExample.fig.png, mendelianExample.fig.pdf","title":"Quik demo"},{"location":"old/quik_demo/#running-sibarc","text":"Running SibArc on a trait with de-novo tail architecture From the test directory type: ./sibArc.py ../data/trait1-test.csv --out denovoExample This will produce the following output files: denovoExample.result.out, denovoExample.fig.png, denovoExample.fig.pdf Running SibArc on a trait with Mendelian tail architecture From the test directory type: ./sibArc.py ../data/trait2-test.csv --out mendelianExample This will produce the following output files: mendelianExample.result.out, mendelianExample.fig.png, mendelianExample.fig.pdf","title":"Running SibArc:"},{"location":"old/quik_install/","text":"Preparation After downloading and unzipping sibArc into a suitable directory on your machine you will observe that a folder with the following contents: sibArc <--- program executable data/ <--- input data LICENSE README.me tests/ <--- test directory For Mac/Linux, using the terminal, type the following command from within the directory: chmod +x sibArc.py to make sibArc executable Input Data SibArc requires two column sibling phenotype data. The columns can be separated by a comma or whitespace, a header is optional: Phenotype1,Phenotype2 -2.23511,-2.33331 -1.23041,-1.03325 -1.55322,-0.03213 0.32353,0.991132 0.53233,2.124533 1.23345,0.936323 2.35326,1.323531 Sample Sibling Data Sample Sibling Data can be found in the data folder data/trait1-test.csv data/trait2-test.csv","title":"Quik install"},{"location":"old/quik_install/#preparation","text":"After downloading and unzipping sibArc into a suitable directory on your machine you will observe that a folder with the following contents: sibArc <--- program executable data/ <--- input data LICENSE README.me tests/ <--- test directory For Mac/Linux, using the terminal, type the following command from within the directory: chmod +x sibArc.py to make sibArc executable","title":"Preparation"},{"location":"old/quik_install/#input-data","text":"SibArc requires two column sibling phenotype data. The columns can be separated by a comma or whitespace, a header is optional: Phenotype1,Phenotype2 -2.23511,-2.33331 -1.23041,-1.03325 -1.55322,-0.03213 0.32353,0.991132 0.53233,2.124533 1.23345,0.936323 2.35326,1.323531 Sample Sibling Data Sample Sibling Data can be found in the data folder data/trait1-test.csv data/trait2-test.csv","title":"Input Data"},{"location":"old/quik_result/","text":"Interpreting SibArc Results A trait with denovo tail architecture in both tails (denovoExample.fig.png) Notice: 1. Evidence of De Novo architecture in both trait tails. 2. Lower heritability in both trait tails. A trait with Mendelian tail architecture in both tails (mendelianExample.fig.png) Notice: 1. Polygenic architecture in the lower tail, Mendelian heritability in the upper tail. 2. Increased heritability in the upper tail.","title":"Interpreting SibArc Results"},{"location":"old/quik_result/#interpreting-sibarc-results","text":"A trait with denovo tail architecture in both tails (denovoExample.fig.png) Notice: 1. Evidence of De Novo architecture in both trait tails. 2. Lower heritability in both trait tails. A trait with Mendelian tail architecture in both tails (mendelianExample.fig.png) Notice: 1. Polygenic architecture in the lower tail, Mendelian heritability in the upper tail. 2. Increased heritability in the upper tail.","title":"Interpreting SibArc Results"},{"location":"old/course_data/readME/","text":"This is a directory for course data Files can be uploaded via command line or web portal You can upload large files via git-lfs https://git-lfs.github.com","title":"This is a directory for course data"},{"location":"old/course_data/readME/#this-is-a-directory-for-course-data","text":"Files can be uploaded via command line or web portal You can upload large files via git-lfs https://git-lfs.github.com","title":"This is a directory for course data"},{"location":"old/modules/Day1a.docx/","text":"Polygenic Risk Score Analyses Workshop 2024 Day 1a: GWAS & relevant Statistics Introduction to Bash Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment. Moving around the File System To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/ Looking at the Current Directory Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size Counting Number of Lines in File We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt Search File Content Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l Filtering and Reshu\ufb04ing Files A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if( \\(7<5e-8)\" and \"\\) 7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met. Introduction to R R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop. Basics If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R Working Directory When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\") Libraries Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet). Variables in R You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\") Functions You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2) Plotting While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point() Regression Models In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Polygenic Risk Score Analyses Workshop 2024"},{"location":"old/modules/Day1a.docx/#polygenic-risk-score-analyses-workshop-2024","text":"","title":"Polygenic Risk Score Analyses Workshop 2024"},{"location":"old/modules/Day1a.docx/#day-1a-gwas-relevant-statistics","text":"","title":"Day 1a: GWAS &amp; relevant Statistics"},{"location":"old/modules/Day1a.docx/#introduction-to-bash","text":"Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment.","title":"Introduction to Bash"},{"location":"old/modules/Day1a.docx/#moving-around-the-file-system","text":"To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/","title":"Moving around the File System"},{"location":"old/modules/Day1a.docx/#looking-at-the-current-directory","text":"Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size","title":"Looking at the Current Directory"},{"location":"old/modules/Day1a.docx/#counting-number-of-lines-in-file","text":"We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt","title":"Counting Number of Lines in File"},{"location":"old/modules/Day1a.docx/#search-file-content","text":"Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l","title":"Search File Content"},{"location":"old/modules/Day1a.docx/#filtering-and-reshuffling-files","text":"A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if( \\(7<5e-8)\" and \"\\) 7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met.","title":"Filtering and Reshu\ufb04ing Files"},{"location":"old/modules/Day1a.docx/#introduction-to-r","text":"R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop.","title":"Introduction to R"},{"location":"old/modules/Day1a.docx/#basics","text":"If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R","title":"Basics"},{"location":"old/modules/Day1a.docx/#working-directory","text":"When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\")","title":"Working Directory"},{"location":"old/modules/Day1a.docx/#libraries","text":"Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet).","title":"Libraries"},{"location":"old/modules/Day1a.docx/#variables-in-r","text":"You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\")","title":"Variables in R"},{"location":"old/modules/Day1a.docx/#functions","text":"You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2)","title":"Functions"},{"location":"old/modules/Day1a.docx/#plotting","text":"While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point()","title":"Plotting"},{"location":"old/modules/Day1a.docx/#regression-models","text":"In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Regression Models"},{"location":"old/modules/Day1b.docx/","text":"Polygenic Risk Score Analyses Workshop 2024 Practical 1 Introduction to PLINK I: basics Key Learning Outcomes After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website 4. Select and exclude lists of samples and SNPs \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Introduction PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials. Command line basics In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click ) Let's begin Open up a terminal Navigate to the Day 1b working directory cd ~/data/Data_Day1b/ List all files in this directory by typing ls Test PLINK with no input by typing plink Note that you can see lots of PLINK options by using the built-in help function: plink --help \ud83d\udcdd Calling PLINK with no output will test if PLINK is installed and available in the directory, because you should see some output showing the PLINK license and some commands. If you do not see this, then please ask for help now! Exploring Data Sets Open an Explorer window ('Finder' on a Mac) and navigate to your PLINK working directory. \ud83d\udcdd An explorer window should show the same files as the ls command Open the file called 'D1D.map' with a Text Editor e.g. by typing right-click > Open . Open the file 'D1D.ped'. Note this is a large file - if it will not open or is very slow, skip this step. Go to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side) What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file? Create 'binary' format PLINK files using the recode command: plink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Open and examine files ending .bim and .fam. Do not open the .bed file. Open and skim the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary) Recoding alleles as counts Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be? PLINK website Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists. Write SNP list and extract SNPs You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!! Practical 2 Introduction to PLINK II: Performing QC & GWAS Key Learning Outcomes After completing this practical, you should be able to: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS Generate summaries to perform QC There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC. Individual missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"? SNP Missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful? Hardy-Weinberg Equilibrium Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why? Allele frequencies Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way Apply QC filters There are di\ufb00erent strategies for performing QC on your data: (a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice. Apply individual missingness thresholds Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening? Apply SNP missingness and MAF thresholds Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness? Apply Hardy-Weinberg thresholds Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP. Perform GWAS Case/Control GWAS - no covariates Run the following code, which performs a genetic association study using logistic regression on some case/control data: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations? Case/Control GWAS - with covariates Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs? License This work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode You are free to: Share --- copy and redistribute the material in any medium or format Adapt --- remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution --- You must give appropriate credit, providea link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial --- You may not use the material for commercial purposes. ShareAlike --- If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions --- You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Day1b.docx"},{"location":"old/modules/Day1b.docx/#practical-1","text":"","title":"Practical 1"},{"location":"old/modules/Day1b.docx/#introduction-to-plink-i-basics","text":"","title":"Introduction to PLINK I: basics"},{"location":"old/modules/Day1b.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website","title":"Key Learning Outcomes"},{"location":"old/modules/Day1b.docx/#4-select-and-exclude-lists-of-samples-and-snps","text":"\u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only.","title":"4.  Select and exclude lists of samples and SNPs"},{"location":"old/modules/Day1b.docx/#introduction","text":"PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials.","title":"Introduction"},{"location":"old/modules/Day1b.docx/#command-line-basics","text":"In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click )","title":"Command line basics"},{"location":"old/modules/Day1b.docx/#lets-begin","text":"Open up a terminal Navigate to the Day 1b working directory cd ~/data/Data_Day1b/ List all files in this directory by typing ls Test PLINK with no input by typing plink Note that you can see lots of PLINK options by using the built-in help function: plink --help \ud83d\udcdd Calling PLINK with no output will test if PLINK is installed and available in the directory, because you should see some output showing the PLINK license and some commands. If you do not see this, then please ask for help now!","title":"Let's begin"},{"location":"old/modules/Day1b.docx/#exploring-data-sets","text":"Open an Explorer window ('Finder' on a Mac) and navigate to your PLINK working directory. \ud83d\udcdd An explorer window should show the same files as the ls command Open the file called 'D1D.map' with a Text Editor e.g. by typing right-click > Open . Open the file 'D1D.ped'. Note this is a large file - if it will not open or is very slow, skip this step. Go to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side) What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file? Create 'binary' format PLINK files using the recode command: plink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Open and examine files ending .bim and .fam. Do not open the .bed file. Open and skim the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary)","title":"Exploring Data Sets"},{"location":"old/modules/Day1b.docx/#recoding-alleles-as-counts","text":"Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be?","title":"Recoding alleles as counts"},{"location":"old/modules/Day1b.docx/#plink-website","text":"Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists.","title":"PLINK website"},{"location":"old/modules/Day1b.docx/#write-snp-list-and-extract-snps","text":"You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!!","title":"Write SNP list and extract SNPs"},{"location":"old/modules/Day1b.docx/#practical-2","text":"","title":"Practical 2"},{"location":"old/modules/Day1b.docx/#introduction-to-plink-ii-performing-qc-gwas","text":"","title":"Introduction to PLINK II: Performing QC &amp; GWAS"},{"location":"old/modules/Day1b.docx/#key-learning-outcomes_1","text":"After completing this practical, you should be able to: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS","title":"Key Learning Outcomes"},{"location":"old/modules/Day1b.docx/#generate-summaries-to-perform-qc","text":"There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC.","title":"Generate summaries to perform QC"},{"location":"old/modules/Day1b.docx/#individual-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"?","title":"Individual missingness"},{"location":"old/modules/Day1b.docx/#snp-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful?","title":"SNP Missingness"},{"location":"old/modules/Day1b.docx/#hardy-weinberg-equilibrium","text":"Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why?","title":"Hardy-Weinberg Equilibrium"},{"location":"old/modules/Day1b.docx/#allele-frequencies","text":"Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way","title":"Allele frequencies"},{"location":"old/modules/Day1b.docx/#apply-qc-filters","text":"","title":"Apply QC filters"},{"location":"old/modules/Day1b.docx/#there-are-different-strategies-for-performing-qc-on-your-data","text":"(a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice.","title":"There are di\ufb00erent strategies for performing QC on your data:"},{"location":"old/modules/Day1b.docx/#apply-individual-missingness-thresholds","text":"Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening?","title":"Apply individual missingness thresholds"},{"location":"old/modules/Day1b.docx/#apply-snp-missingness-and-maf-thresholds","text":"Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness?","title":"Apply SNP missingness and MAF thresholds"},{"location":"old/modules/Day1b.docx/#apply-hardy-weinberg-thresholds","text":"Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP.","title":"Apply Hardy-Weinberg thresholds"},{"location":"old/modules/Day1b.docx/#perform-gwas","text":"","title":"Perform GWAS"},{"location":"old/modules/Day1b.docx/#casecontrol-gwas-no-covariates","text":"Run the following code, which performs a genetic association study using logistic regression on some case/control data: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations?","title":"Case/Control GWAS - no covariates"},{"location":"old/modules/Day1b.docx/#casecontrol-gwas-with-covariates","text":"Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Case/Control GWAS - with covariates"},{"location":"old/modules/Day1b.docx/#license","text":"This work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode","title":"License"},{"location":"old/modules/Day1b.docx/#you-are-free-to","text":"Share --- copy and redistribute the material in any medium or format Adapt --- remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to:"},{"location":"old/modules/Day1b.docx/#under-the-following-terms","text":"Attribution --- You must give appropriate credit, providea link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial --- You may not use the material for commercial purposes. ShareAlike --- If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions --- You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms:"},{"location":"old/modules/Day1b.docx/#notices","text":"You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Notices:"},{"location":"old/modules/Day2.docx/","text":"Introduction to Polygenic Risk Scores Table of Contents Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents Key Learning Outcomes After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results Resources you will be using To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link Data Structure You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents Introduction A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS. Understanding GWAS Summary Statistics When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents Matching the Base and Target Data sets The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents Linkage Disequilibrium in PRS Analyses GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents Performing Clumping Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents P-Value Thresholding Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds. Height PRS using GW-significant SNPs only Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5*\u00d7*10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5*\u00d7*10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents Height PRS across multiple P-value thresholds A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents High Resolution Scoring If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5*\u00d7*10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents Stratifying Samples by PRS An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents Case Control Studies In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres \u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status. Cross-Trait Analysis A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Introduction to Polygenic Risk Scores"},{"location":"old/modules/Day2.docx/#introduction-to-polygenic-risk-scores","text":"","title":"Introduction to Polygenic Risk Scores"},{"location":"old/modules/Day2.docx/#table-of-contents","text":"Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents","title":"Table of Contents"},{"location":"old/modules/Day2.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results","title":"Key Learning Outcomes"},{"location":"old/modules/Day2.docx/#resources-you-will-be-using","text":"To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link","title":"Resources you will be using"},{"location":"old/modules/Day2.docx/#data-structure","text":"You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents","title":"Data Structure"},{"location":"old/modules/Day2.docx/#introduction","text":"A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS.","title":"Introduction"},{"location":"old/modules/Day2.docx/#understanding-gwas-summary-statistics","text":"When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents","title":"Understanding GWAS Summary Statistics"},{"location":"old/modules/Day2.docx/#matching-the-base-and-target-data-sets","text":"The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents","title":"Matching the Base and Target Data sets"},{"location":"old/modules/Day2.docx/#linkage-disequilibrium-in-prs-analyses","text":"GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents","title":"Linkage Disequilibrium in PRS Analyses"},{"location":"old/modules/Day2.docx/#performing-clumping","text":"Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents","title":"Performing Clumping"},{"location":"old/modules/Day2.docx/#p-value-thresholding","text":"Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds.","title":"P-Value Thresholding"},{"location":"old/modules/Day2.docx/#height-prs-using-gw-significant-snps-only","text":"Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5*\u00d7*10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5*\u00d7*10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents","title":"Height PRS using GW-significant SNPs only"},{"location":"old/modules/Day2.docx/#height-prs-across-multiple-p-value-thresholds","text":"A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents","title":"Height PRS across multiple P-value thresholds"},{"location":"old/modules/Day2.docx/#high-resolution-scoring","text":"If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5*\u00d7*10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents","title":"High Resolution Scoring"},{"location":"old/modules/Day2.docx/#stratifying-samples-by-prs","text":"An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents","title":"Stratifying Samples by PRS"},{"location":"old/modules/Day2.docx/#case-control-studies","text":"In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary.","title":"Case Control Studies"},{"location":"old/modules/Day2.docx/#rscript-prsice_linuxprsicer-prsice-prsice_linuxprsice_linux-base-base_datacadaddtxt-target-target_datatar-snp-markername-a1-effect_allele-a2-noneffect_allele-chr-chr-bp-bp_hg19-stat-beta-beta-pvalue-p_dgc-pheno-target_datacadpheno-binary-target-t-out-resultscadhighres","text":"\u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status.","title":"Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres\n"},{"location":"old/modules/Day2.docx/#cross-trait-analysis","text":"A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis.","title":"Cross-Trait Analysis"},{"location":"old/modules/Day2.docx/#rscript-prsice_linuxprsicer-prsice-prsice_linuxprsice_linux-base-base_datagiant_heighttxt-target-target_datatar-snp-markername-a1-allele1-a2-allele2-stat-b-beta-pvalue-p-pheno-target_datacadpheno-binary-target-t-out-resultscrosshighres","text":"Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres\n"},{"location":"old/modules/Day3a.docx/","text":"Advanced Polygenic Risk Score Analyses Day 3 - Polygenic Risk Score Analyses Workshop 2024 Table of Contents Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2 Day 3a practical Key Learning Outcomes After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds. Base and Target datasets In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650) Downloading Datasets All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam Method for calculating PRS For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach. Exercise 1 Estimating R 2 Code Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2 Scenario 1: Predicting from EUR training to EUR target data: Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur Key code parameters The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389. Scenario 2: Predicting from AFR training to AFR target data: Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%). Scenario 3: Predicting from EUR training to AFR target data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%). Exercise 2 Visualising and comparing R 2 In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library ( ggplot2 ) library ( RColorBrewer ) # Create a function to read the files and add ancestry information read_and_label <- function ( file, ancestry ) { data <- read.table ( file, header = TRUE, sep = \"\\t\" ) data $Ancestry <- ancestry return ( data ) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label ( \"SBP.eur.eur.summary\" , \"EUR_EUR\" ) AFR_AFR <- read_and_label ( \"SBP.afr.afr.summary\" , \"AFR_AFR\" ) EUR_AFR <- read_and_label ( \"SBP.eur.afr.summary\" , \"EUR_AFR\" ) # Combine all data into one dataframe all_data <- rbind ( EUR_EUR, AFR_AFR, EUR_AFR ) # Create a bar graph with different colors for each ancestry png ( '/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) ancestry <- ggplot ( all_data, aes ( x = Ancestry, y = PRS.R2, fill = Ancestry )) + geom_bar ( stat = \"identity\" , position = \"dodge\" ) + labs ( title = \"R2 Values by Ancestry\" , x = \"Ancestry\" , y = \"R2 Value\" ) + theme_minimal () + scale_fill_brewer ( palette = \"Set3\" ) + theme ( plot.title = element_text ( hjust = 0 .5, size = 16 , face = \"bold\" ) , axis.title.x = element_text ( size = 14 , face = \"bold\" ) , axis.title.y = element_text ( size = 14 , face = \"bold\" ) , axis.text.x = element_text ( size = 12 , angle = 45 , hjust = 1) , axis.text.y = element_text ( size = 12) , legend.position = \"none\" ) print ( ancestry ) dev.off () Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Advanced Polygenic Risk Score Analyses"},{"location":"old/modules/Day3a.docx/#advanced-polygenic-risk-score-analyses","text":"","title":"Advanced Polygenic Risk Score Analyses"},{"location":"old/modules/Day3a.docx/#day-3-polygenic-risk-score-analyses-workshop-2024","text":"","title":"Day 3 - Polygenic Risk Score Analyses Workshop 2024"},{"location":"old/modules/Day3a.docx/#table-of-contents","text":"Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2","title":"Table of Contents"},{"location":"old/modules/Day3a.docx/#day-3a-practical","text":"","title":"Day 3a practical"},{"location":"old/modules/Day3a.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds.","title":"Key Learning Outcomes"},{"location":"old/modules/Day3a.docx/#base-and-target-datasets","text":"In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650)","title":"Base and Target datasets"},{"location":"old/modules/Day3a.docx/#downloading-datasets","text":"All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam","title":"Downloading Datasets"},{"location":"old/modules/Day3a.docx/#method-for-calculating-prs","text":"For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach.","title":"Method for calculating PRS"},{"location":"old/modules/Day3a.docx/#exercise-1-estimating-r2","text":"","title":"Exercise 1 Estimating R2"},{"location":"old/modules/Day3a.docx/#code","text":"Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2","title":"Code"},{"location":"old/modules/Day3a.docx/#scenario-1-predicting-from-eur-training-to-eur-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur","title":"Scenario 1: Predicting from EUR training to EUR target data:"},{"location":"old/modules/Day3a.docx/#key-code-parameters","text":"The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389.","title":"Key code parameters"},{"location":"old/modules/Day3a.docx/#scenario-2-predicting-from-afr-training-to-afr-target-data","text":"Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%).","title":"Scenario 2: Predicting from AFR training to AFR target data:"},{"location":"old/modules/Day3a.docx/#scenario-3-predicting-from-eur-training-to-afr-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%).","title":"Scenario 3: Predicting from EUR training to AFR target data"},{"location":"old/modules/Day3a.docx/#exercise-2-visualising-and-comparing-r2","text":"In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library ( ggplot2 ) library ( RColorBrewer ) # Create a function to read the files and add ancestry information read_and_label <- function ( file, ancestry ) { data <- read.table ( file, header = TRUE, sep = \"\\t\" ) data $Ancestry <- ancestry return ( data ) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label ( \"SBP.eur.eur.summary\" , \"EUR_EUR\" ) AFR_AFR <- read_and_label ( \"SBP.afr.afr.summary\" , \"AFR_AFR\" ) EUR_AFR <- read_and_label ( \"SBP.eur.afr.summary\" , \"EUR_AFR\" ) # Combine all data into one dataframe all_data <- rbind ( EUR_EUR, AFR_AFR, EUR_AFR ) # Create a bar graph with different colors for each ancestry png ( '/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) ancestry <- ggplot ( all_data, aes ( x = Ancestry, y = PRS.R2, fill = Ancestry )) + geom_bar ( stat = \"identity\" , position = \"dodge\" ) + labs ( title = \"R2 Values by Ancestry\" , x = \"Ancestry\" , y = \"R2 Value\" ) + theme_minimal () + scale_fill_brewer ( palette = \"Set3\" ) + theme ( plot.title = element_text ( hjust = 0 .5, size = 16 , face = \"bold\" ) , axis.title.x = element_text ( size = 14 , face = \"bold\" ) , axis.title.y = element_text ( size = 14 , face = \"bold\" ) , axis.text.x = element_text ( size = 12 , angle = 45 , hjust = 1) , axis.text.y = element_text ( size = 12) , legend.position = \"none\" ) print ( ancestry ) dev.off () Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Exercise 2 Visualising and comparing R2"},{"location":"old/modules/Day3b.docx/","text":"Day 3b practical We need to move into the directory you will be working in; cd ~/data/Data_Day4/data Introduction to Cross-Ancestry PRS computation Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python =3 .7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion. 1. The 1000 Genomes datasets The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week. 2. Cross-population allele frequency Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l Questions (i) Which population contains the most SNPs? (ii) What is the significance of the observed population order? 3. Distribution of allele frequencies In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages ( \"dplyr\" ) install.packages ( \"ggplot2\" ) # Load necessary libraries library ( dplyr ) library ( ggplot2 ) # Create a function to read the files and add ancestry information freq <-read.table ( \"~/data/Data_Day4/plink.frq.strat\" , header = T ) plotDat <- freq %>% mutate ( AlleleFrequency = cut ( MAF, seq (0 , 1 , 0 .25 ))) %>% group_by ( AlleleFrequency, CLST ) %>% summarise ( FractionOfSNPs = n () /nrow ( freq ) * 100) # Create a bar graph png ( '/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) maf_ancestry <- ggplot ( na.omit ( plotDat ) , aes ( AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST )) + geom_line () + scale_y_continuous ( limits = c (0 , 12)) + ggtitle ( \"Distribution of allele frequency across genome\" ) print ( maf_ancestry ) dev.off () Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png Questions (i) Which population has the most SNPs? (ii) What is the significance of the observed population ordering? (iii) What is the reason behind these two features? Introduction to PRS-CSx 5. Background to PRS-CSX PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and external LD reference panels from multiple populations to improve cross-population polygenic prediction. We will be using simulated trait data pertaininng to systolic blood pressure (SBP) to explore PRS performance using 2 target populations that consist of 650 individuals of African ancestry and 500 individuals of European ancestry. Please note when running PRSice that the object of the flag \"--prsice\" will change according to whether plink is being called within the linux-like environment of the virtual machine (PRSice_linux) or a mac (PRSice_mac). Both executables can be found in the /home/manager/data/Data_Day4 directory. 7. Running PRS-CSx To model the coupling of ect sizes at individual SNPs across ancestries PRS-CSx uses an MCMC (Bayesian) sampling algorithm to determine values of the global shrinkage parameter (\"phi\") by Maximum likelihood. For samples of mixed or admixed genetic ancestry (which ours are not) the optimal value of the shrinkage parameter is estimated autonomously from the data. Here we use the value of phi (1e-4), which is suggested by the software authors, given that our trait is simulated to have a relatively small number (N=110) causal variants, distributed genome-wide. Step 1: Set up environment First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE = \"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21 ..22 } ; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr ${ chr } _only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr ${ chr } ,/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr ${ chr } \\ --n_gwas=25732,4855 \\ --chrom= ${ chr } \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr ${ chr } .csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3) Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE) Task: Replace 'ancestry <- \"EUR\" ' with 'ancestry <- \"AFR\" ' and repeat the subsequent steps shown above Step 4: Merge genotype-phenotype data Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen) Step 5: Split data into validation and test sets Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices] Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights # Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur) Step 7: Prepare the variant weights matrices as vectors # Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat)) Step 8: Predict phenotype on validation and test dataset Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z Step 9: Plot phenotype distributions of validation and test data: Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal() Step 10: Plot true values against predicted values The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off() Step 11: Calculate deviance-based R 2 # Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2)) Results graph: pdf true against pred","title":"Day3b.docx"},{"location":"old/modules/Day3b.docx/#day-3b-practical","text":"We need to move into the directory you will be working in; cd ~/data/Data_Day4/data","title":"Day 3b practical"},{"location":"old/modules/Day3b.docx/#introduction-to-cross-ancestry-prs-computation","text":"Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python =3 .7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion.","title":"Introduction to Cross-Ancestry PRS computation"},{"location":"old/modules/Day3b.docx/#1-the-1000-genomes-datasets","text":"The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week.","title":"1. The 1000 Genomes datasets"},{"location":"old/modules/Day3b.docx/#2-cross-population-allele-frequency","text":"Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l","title":"2. Cross-population allele frequency"},{"location":"old/modules/Day3b.docx/#questions","text":"","title":"Questions"},{"location":"old/modules/Day3b.docx/#i-which-population-contains-the-most-snps","text":"","title":"(i) Which population contains the most SNPs?"},{"location":"old/modules/Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-order","text":"","title":"(ii) What  is the significance of the observed population order?"},{"location":"old/modules/Day3b.docx/#3-distribution-of-allele-frequencies","text":"In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages ( \"dplyr\" ) install.packages ( \"ggplot2\" ) # Load necessary libraries library ( dplyr ) library ( ggplot2 ) # Create a function to read the files and add ancestry information freq <-read.table ( \"~/data/Data_Day4/plink.frq.strat\" , header = T ) plotDat <- freq %>% mutate ( AlleleFrequency = cut ( MAF, seq (0 , 1 , 0 .25 ))) %>% group_by ( AlleleFrequency, CLST ) %>% summarise ( FractionOfSNPs = n () /nrow ( freq ) * 100) # Create a bar graph png ( '/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) maf_ancestry <- ggplot ( na.omit ( plotDat ) , aes ( AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST )) + geom_line () + scale_y_continuous ( limits = c (0 , 12)) + ggtitle ( \"Distribution of allele frequency across genome\" ) print ( maf_ancestry ) dev.off () Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png","title":"3. Distribution of allele frequencies"},{"location":"old/modules/Day3b.docx/#questions_1","text":"","title":"Questions"},{"location":"old/modules/Day3b.docx/#i-which-population-has-the-most-snps","text":"","title":"(i) Which population has the most SNPs?"},{"location":"old/modules/Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-ordering","text":"","title":"(ii) What  is the significance of the observed population ordering?"},{"location":"old/modules/Day3b.docx/#iii-what-is-the-reason-behind-these-two-features","text":"","title":"(iii) What is the reason behind these two features?"},{"location":"old/modules/Day3b.docx/#introduction-to-prs-csx","text":"","title":"Introduction to PRS-CSx"},{"location":"old/modules/Day3b.docx/#5-background-to-prs-csx","text":"PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and external LD reference panels from multiple populations to improve cross-population polygenic prediction. We will be using simulated trait data pertaininng to systolic blood pressure (SBP) to explore PRS performance using 2 target populations that consist of 650 individuals of African ancestry and 500 individuals of European ancestry. Please note when running PRSice that the object of the flag \"--prsice\" will change according to whether plink is being called within the linux-like environment of the virtual machine (PRSice_linux) or a mac (PRSice_mac). Both executables can be found in the /home/manager/data/Data_Day4 directory.","title":"5. Background to PRS-CSX"},{"location":"old/modules/Day3b.docx/#7-running-prs-csx","text":"To model the coupling of ect sizes at individual SNPs across ancestries PRS-CSx uses an MCMC (Bayesian) sampling algorithm to determine values of the global shrinkage parameter (\"phi\") by Maximum likelihood. For samples of mixed or admixed genetic ancestry (which ours are not) the optimal value of the shrinkage parameter is estimated autonomously from the data. Here we use the value of phi (1e-4), which is suggested by the software authors, given that our trait is simulated to have a relatively small number (N=110) causal variants, distributed genome-wide.","title":"7. Running PRS-CSx"},{"location":"old/modules/Day3b.docx/#step-1-set-up-environment","text":"First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS","title":"Step 1: Set up environment"},{"location":"old/modules/Day3b.docx/#step-2-run-csx-derive-new-snps-weights-trained-on-european-and-african-summary-stats","text":"Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE = \"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21 ..22 } ; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr ${ chr } _only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr ${ chr } ,/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr ${ chr } \\ --n_gwas=25732,4855 \\ --chrom= ${ chr } \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr ${ chr } .csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh","title":"Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats"},{"location":"old/modules/Day3b.docx/#step-3-combine-csx-derived-snp-weights-across-chromosomes-currently-excludes-chromosome-3","text":"Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE)","title":"Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3)"},{"location":"old/modules/Day3b.docx/#task-replace-ancestry-eur-with-ancestry-afr-and-repeat-the-subsequent-steps-shown-above","text":"","title":"Task: Replace 'ancestry &lt;- \"EUR\" ' with 'ancestry &lt;- \"AFR\" ' and repeat the subsequent steps shown above"},{"location":"old/modules/Day3b.docx/#step-4-merge-genotype-phenotype-data","text":"Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen)","title":"Step 4: Merge genotype-phenotype data"},{"location":"old/modules/Day3b.docx/#step-5-split-data-into-validation-and-test-sets","text":"Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices]","title":"Step 5: Split data into validation and test sets"},{"location":"old/modules/Day3b.docx/#step-6-prepare-the-regression-model-input-using-the-csx-derived-afr-and-eur-weights","text":"# Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur)","title":"Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights"},{"location":"old/modules/Day3b.docx/#step-7-prepare-the-variant-weights-matrices-as-vectors","text":"# Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat))","title":"Step 7: Prepare the variant weights matrices as vectors"},{"location":"old/modules/Day3b.docx/#step-8-predict-phenotype-on-validation-and-test-dataset","text":"Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z","title":"Step 8: Predict phenotype on validation and test dataset"},{"location":"old/modules/Day3b.docx/#step-9-plot-phenotype-distributions-of-validation-and-test-data","text":"Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal()","title":"Step 9: Plot phenotype distributions of validation and test data:"},{"location":"old/modules/Day3b.docx/#step-10-plot-true-values-against-predicted-values","text":"The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off() Step 11: Calculate deviance-based R 2 # Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2))","title":"Step 10: Plot true values against predicted values"},{"location":"old/modules/Day3b.docx/#results","text":"graph: pdf true against pred","title":"Results"},{"location":"old/modules/Day4a.docx/","text":"BridgePRS Learning Objectives The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method. Introduction to BridgePRS BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values. BridgePRS Scenario 1: Application of African GWAS weights to an African target group Create configuration file for the target-only analysis In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes). Questions From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ? Note In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step. Single ancestry BridgePRS analysis: Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y Task Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders Questions What evidence can you see that the analysis was successfully executed? BridgePRS Scenario 2: Prediction into African target data using European and African summary statistics BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations. Create configuration file for base and target populations bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat Question Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located? Tasks Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below. Multi-ancestry BRIDGEPRS analysis: Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y Tasks After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory. Questions In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model? Short Quiz I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis? File types ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat Answer Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Day4a.docx"},{"location":"old/modules/Day4a.docx/#bridgeprs","text":"","title":"BridgePRS"},{"location":"old/modules/Day4a.docx/#learning-objectives","text":"The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method.","title":"Learning Objectives"},{"location":"old/modules/Day4a.docx/#introduction-to-bridgeprs","text":"BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values.","title":"Introduction to BridgePRS"},{"location":"old/modules/Day4a.docx/#bridgeprs-scenario-1-application-of-african-gwas-weights-to-an-african-target-group","text":"","title":"BridgePRS Scenario 1: Application of African GWAS weights to an African target group"},{"location":"old/modules/Day4a.docx/#create-configuration-file-for-the-target-only-analysis","text":"In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes).","title":"Create configuration file for the target-only analysis"},{"location":"old/modules/Day4a.docx/#questions","text":"From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ?","title":"Questions"},{"location":"old/modules/Day4a.docx/#note","text":"In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step.","title":"Note"},{"location":"old/modules/Day4a.docx/#single-ancestry-bridgeprs-analysis","text":"Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y","title":"Single ancestry BridgePRS analysis:"},{"location":"old/modules/Day4a.docx/#task","text":"Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders","title":"Task"},{"location":"old/modules/Day4a.docx/#questions_1","text":"What evidence can you see that the analysis was successfully executed?","title":"Questions"},{"location":"old/modules/Day4a.docx/#bridgeprs-scenario-2-prediction-into-african-target-data-using-european-and-african-summary-statistics","text":"BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations.","title":"BridgePRS Scenario 2:  Prediction into African target data using European and African summary statistics"},{"location":"old/modules/Day4a.docx/#create-configuration-file-for-base-and-target-populations","text":"bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat","title":"Create configuration file for base and target populations"},{"location":"old/modules/Day4a.docx/#question","text":"Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located?","title":"Question"},{"location":"old/modules/Day4a.docx/#tasks","text":"Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below.","title":"Tasks"},{"location":"old/modules/Day4a.docx/#multi-ancestry-bridgeprs-analysis","text":"Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y","title":"Multi-ancestry BRIDGEPRS analysis:"},{"location":"old/modules/Day4a.docx/#tasks_1","text":"After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory.","title":"Tasks"},{"location":"old/modules/Day4a.docx/#questions_2","text":"In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model?","title":"Questions"},{"location":"old/modules/Day4a.docx/#short-quiz","text":"I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis?","title":"Short Quiz"},{"location":"old/modules/Day4a.docx/#file-types","text":"ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat","title":"File types"},{"location":"old/modules/Day4a.docx/#answer","text":"Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Answer"},{"location":"old/modules/Day4b.docx/","text":"Day 4 - Practical 2: Introduction to Admixture analysis Module Goals The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores Part1: Plot Decay of Ancestry LD over time Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" ) Questions (i) Describe what happens to admixture LD over time? (ii) Why does recombination also have an impact? Part 2: Global Ancestry Inference We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ./admixture samples_n28_qc_thin.bed 2 --supervised -j4 Questions (i) What do you think the number specified after the inout file represents? The next step is to create a plot the results # Plot Global Ancestry Results R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300) Questions (i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary) Part 3: Local Ancestry Inference Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm. # Run the following UNIX command from the home directory module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done Questions (i) What information is provided in the simulation results table displayed on-screen ? Part 4: Plot local admixture on chromosome 22 In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr) # Function to read the .msp.tsv file read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) } # Function to read the .Q file read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) } # Specification of file paths msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q' # Read in files msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file) # Check for NA or empty column names print(colnames(msp_df)) # Extract ancestry columns based on pattern recognition of column names ancestry_cols <- colnames(msp_df)[grep(\"\\\\.0$|\\\\.1$\", colnames(msp_df))] # Function to determine ancestry based on RFMix codes determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) } # Rename columns uniquely colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62]) # Prepare data for plotting plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) ) # Create plot of chromosome 22 across the sample plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10)) # Save the plot to a file ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8) Questions (i) How many different continental ancestries do you see represented across the 58 strands? (ii) Why is the number of different ancestry backgrounds higher than it was in the previous step? Part 5: Formatting of admixture files for analysis using PRSice Step 1 - Convert phased genotypes to GenomicRange format # Run from the home directory library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() } # Read in phased VCF file vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22) # Convert haplotypes to data frame haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\") # Convert SNP info to data frame snp_info_df <- data.frame(extracted_snp_info22) # Check for numerical and ordering consistency between haplotypes and SNP info if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") } # Merge haplotypes and SNP info merge_chr22 <- cbind(snp_info_df, haps_dt) # Convert to long format using panelr chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) # Insert 'end' column after 'POS' and create 'wave' column chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\")) # Remove row names and drop redundant columns using base R rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops] # Rename columns names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header # Create GRanges object gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Save the GRanges object saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\") # Clean up memory clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\")) Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix # Read in MSP file msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\") # Reformat genome-wide local ancestry output as GRanges object colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE) # Convert the elements of the GRange object into a dataframe chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL # Clean up column headers header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header # Convert local ancestry calls from wide to long format chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\")) # Drop redundant columns drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL # Convert the reconfigured dataframe file into a GRanges object chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Ensure chr22_haplo_long is a GRanges object before finding overlaps chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Find overlaps and store matching features # Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)])) # Local ancestry calls are now aligned with genotypic data and positional coordinates # Convert to dataframe without adding 'X' to numeric column names genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df)) # Output the dataframe write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F) # Clean up memory rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc() Part 5: Step 3 - Create separate Plink files # Partition genotype and local ancestry data chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))] # Reintegrate in interleaved format d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)] # Prepare headers indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx] # Convert LAnc data from long to wide format LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE))) # Check length of LA_wide length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers # Convert genotype calls to horizontal orientation geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes # Clean up and finalize geno_final geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal) # Name columns colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") # Write final tables write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE) Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22 # Perform general QC ahead of running PRSice on ancestry-deconvolved individuals # (i) Remove SNPs with low minor allele count (MAC) # Plink - Remove monomorphic SNPs (minor allele count 0-4). #AFR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done #EUR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done # PRSice - Generate ancestry-specific weights # AFR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base # EUR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base Part 5: Step 5 - Evaluate the Admixture-informed PRS library(dplyr) # Read the PRS files into dataframes file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F) # Add the fourth column of both files file1$PRS_SUM <- file1$PRS + file2$PRS # Load the phenotype data pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\") # Convert phenotype column to numeric pheno$phenotype <- as.numeric(pheno$phenotype) # Merge PRS data with phenotype data merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\")) # Rename columns names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\" # Perform linear regression for each PRS and the combined PRS model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data) # Extract R-squared values r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared # Print the R-squared values cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\") Questions (i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?","title":"Day4b.docx"},{"location":"old/modules/Day4b.docx/#day-4-practical-2-introduction-to-admixture-analysis","text":"","title":"Day 4 - Practical 2: Introduction to Admixture analysis"},{"location":"old/modules/Day4b.docx/#module-goals","text":"The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores","title":"Module Goals"},{"location":"old/modules/Day4b.docx/#part1-plot-decay-of-ancestry-ld-over-time","text":"Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" )","title":"Part1: Plot Decay of Ancestry LD over time"},{"location":"old/modules/Day4b.docx/#questions","text":"","title":"Questions"},{"location":"old/modules/Day4b.docx/#i-describe-what-happens-to-admixture-ld-over-time","text":"","title":"(i) Describe what happens to admixture LD over time?"},{"location":"old/modules/Day4b.docx/#ii-why-does-recombination-also-have-an-impact","text":"","title":"(ii) Why does recombination also have an impact?"},{"location":"old/modules/Day4b.docx/#part-2-global-ancestry-inference","text":"We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ./admixture samples_n28_qc_thin.bed 2 --supervised -j4","title":"Part 2: Global Ancestry Inference"},{"location":"old/modules/Day4b.docx/#questions_1","text":"","title":"Questions"},{"location":"old/modules/Day4b.docx/#i-what-do-you-think-the-number-specified-after-the-inout-file-represents","text":"The next step is to create a plot the results # Plot Global Ancestry Results R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300)","title":"(i) What do you think the number specified after the inout file represents?"},{"location":"old/modules/Day4b.docx/#questions_2","text":"","title":"Questions"},{"location":"old/modules/Day4b.docx/#i-what-are-the-ancestry-assignments-of-the-28-individuals-provide-estimated-proportions-where-necessary","text":"","title":"(i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary)"},{"location":"old/modules/Day4b.docx/#part-3-local-ancestry-inference","text":"Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm. # Run the following UNIX command from the home directory module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done","title":"Part 3: Local Ancestry Inference"},{"location":"old/modules/Day4b.docx/#questions_3","text":"","title":"Questions"},{"location":"old/modules/Day4b.docx/#i-what-information-is-provided-in-the-simulation-results-table-displayed-on-screen","text":"","title":"(i) What information is provided in the simulation results table displayed on-screen ?"},{"location":"old/modules/Day4b.docx/#part-4-plot-local-admixture-on-chromosome-22","text":"In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr) # Function to read the .msp.tsv file read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) } # Function to read the .Q file read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) } # Specification of file paths msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q' # Read in files msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file) # Check for NA or empty column names print(colnames(msp_df)) # Extract ancestry columns based on pattern recognition of column names ancestry_cols <- colnames(msp_df)[grep(\"\\\\.0$|\\\\.1$\", colnames(msp_df))] # Function to determine ancestry based on RFMix codes determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) } # Rename columns uniquely colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62]) # Prepare data for plotting plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) ) # Create plot of chromosome 22 across the sample plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10)) # Save the plot to a file ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8)","title":"Part 4: Plot local admixture on chromosome 22"},{"location":"old/modules/Day4b.docx/#questions_4","text":"","title":"Questions"},{"location":"old/modules/Day4b.docx/#i-how-many-different-continental-ancestries-do-you-see-represented-across-the-58-strands","text":"","title":"(i) How many different continental ancestries do you see represented across the 58 strands?"},{"location":"old/modules/Day4b.docx/#ii-why-is-the-number-of-different-ancestry-backgrounds-higher-than-it-was-in-the-previous-step","text":"","title":"(ii) Why is the number of different ancestry backgrounds higher than it was in the previous step?"},{"location":"old/modules/Day4b.docx/#part-5-formatting-of-admixture-files-for-analysis-using-prsice","text":"","title":"Part 5: Formatting of admixture files for analysis using PRSice"},{"location":"old/modules/Day4b.docx/#step-1-convert-phased-genotypes-to-genomicrange-format","text":"# Run from the home directory library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() } # Read in phased VCF file vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22) # Convert haplotypes to data frame haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\") # Convert SNP info to data frame snp_info_df <- data.frame(extracted_snp_info22) # Check for numerical and ordering consistency between haplotypes and SNP info if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") } # Merge haplotypes and SNP info merge_chr22 <- cbind(snp_info_df, haps_dt) # Convert to long format using panelr chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) # Insert 'end' column after 'POS' and create 'wave' column chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\")) # Remove row names and drop redundant columns using base R rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops] # Rename columns names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header # Create GRanges object gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Save the GRanges object saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\") # Clean up memory clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\"))","title":"Step 1 - Convert phased genotypes to GenomicRange format"},{"location":"old/modules/Day4b.docx/#part-5-step-2-merge-genotypes-from-step-1-with-local-ancestry-calls-by-rfmix","text":"# Read in MSP file msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\") # Reformat genome-wide local ancestry output as GRanges object colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE) # Convert the elements of the GRange object into a dataframe chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL # Clean up column headers header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header # Convert local ancestry calls from wide to long format chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\")) # Drop redundant columns drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL # Convert the reconfigured dataframe file into a GRanges object chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Ensure chr22_haplo_long is a GRanges object before finding overlaps chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Find overlaps and store matching features # Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)])) # Local ancestry calls are now aligned with genotypic data and positional coordinates # Convert to dataframe without adding 'X' to numeric column names genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df)) # Output the dataframe write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F) # Clean up memory rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc()","title":"Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix"},{"location":"old/modules/Day4b.docx/#part-5-step-3-create-separate-plink-files","text":"# Partition genotype and local ancestry data chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))] # Reintegrate in interleaved format d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)] # Prepare headers indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx] # Convert LAnc data from long to wide format LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE))) # Check length of LA_wide length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers # Convert genotype calls to horizontal orientation geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes # Clean up and finalize geno_final geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal) # Name columns colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") # Write final tables write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE)","title":"Part 5: Step 3 - Create separate Plink files"},{"location":"old/modules/Day4b.docx/#part-5-step-4-use-custom-software-rftransform-to-create-plink-files-for-input-into-prsice","text":"module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22 # Perform general QC ahead of running PRSice on ancestry-deconvolved individuals # (i) Remove SNPs with low minor allele count (MAC) # Plink - Remove monomorphic SNPs (minor allele count 0-4). #AFR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done #EUR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done # PRSice - Generate ancestry-specific weights # AFR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base # EUR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base","title":"Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice"},{"location":"old/modules/Day4b.docx/#part-5-step-5-evaluate-the-admixture-informed-prs","text":"library(dplyr) # Read the PRS files into dataframes file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F) # Add the fourth column of both files file1$PRS_SUM <- file1$PRS + file2$PRS # Load the phenotype data pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\") # Convert phenotype column to numeric pheno$phenotype <- as.numeric(pheno$phenotype) # Merge PRS data with phenotype data merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\")) # Rename columns names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\" # Perform linear regression for each PRS and the combined PRS model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data) # Extract R-squared values r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared # Print the R-squared values cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\")","title":"Part 5: Step 5 - Evaluate the Admixture-informed PRS"},{"location":"old/modules/Day4b.docx/#questions_5","text":"","title":"Questions"},{"location":"old/modules/Day4b.docx/#i-how-does-the-r-squared-of-the-combined-ancestry-prs-perform-relative-to-the-2-partial-genome-prss","text":"","title":"(i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?"},{"location":"old/modules/Day5_Projects/","text":"Integration of Polygenic Risk Scores Task \u2013 PRSmix calculation [3.5 hrs] You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024) Group project data files Group 1 https://github.com/tinashedoc/cvx/blob/main/monodta.txt Group 2 https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt Group 3 https://github.com/tinashedoc/cvx/blob/main/pltdta.txt Group 4 https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Integration of Polygenic Risk Scores"},{"location":"old/modules/Day5_Projects/#integration-of-polygenic-risk-scores","text":"","title":"Integration of Polygenic Risk Scores"},{"location":"old/modules/Day5_Projects/#task-prsmix-calculation-35-hrs","text":"You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024)","title":"Task \u2013 PRSmix calculation [3.5 hrs]"},{"location":"old/modules/Day5_Projects/#group-project-data-files","text":"","title":"Group project data files"},{"location":"old/modules/Day5_Projects/#group-1","text":"https://github.com/tinashedoc/cvx/blob/main/monodta.txt","title":"Group 1"},{"location":"old/modules/Day5_Projects/#group-2","text":"https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt","title":"Group 2"},{"location":"old/modules/Day5_Projects/#group-3","text":"https://github.com/tinashedoc/cvx/blob/main/pltdta.txt","title":"Group 3"},{"location":"old/modules/Day5_Projects/#group-4","text":"https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Group 4"},{"location":"old/modules/READme/","text":"Module directory Directory for placing the course module files - these should be markdown or PDF documents They include the presentations and practical manuals for the module. Converting between markdown to PDF can be performed using pandoc. Here is a tutorial and system for that: Converting with Pandoc There is an example markdown file - module_base.md","title":"Module directory"},{"location":"old/modules/READme/#module-directory","text":"Directory for placing the course module files - these should be markdown or PDF documents They include the presentations and practical manuals for the module. Converting between markdown to PDF can be performed using pandoc. Here is a tutorial and system for that: Converting with Pandoc There is an example markdown file - module_base.md","title":"Module directory"},{"location":"old/modules/module_base/","text":"This is a base file to modify with your module content Please rename this md file to suit your module Markdown guide is available https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax Example git https://github.com/WCSCourses/WWPG_2022/blob/main/manuals/module_linux_scripting/module_linux_scripting.md","title":"This is a base file to modify with your module content"},{"location":"old/modules/module_base/#this-is-a-base-file-to-modify-with-your-module-content","text":"","title":"This is a base file to modify with your module content"},{"location":"old/modules/module_base/#please-rename-this-md-file-to-suit-your-module","text":"","title":"Please rename this md file to suit your module"},{"location":"old/modules/module_base/#markdown-guide-is-available","text":"https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax","title":"Markdown guide is available"},{"location":"old/modules/module_base/#example-git","text":"https://github.com/WCSCourses/WWPG_2022/blob/main/manuals/module_linux_scripting/module_linux_scripting.md","title":"Example git"},{"location":"old/more/Day2.docx/","text":"Introduction to Polygenic Risk Scores Table of Contents Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents Key Learning Outcomes After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results Resources you will be using To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link Data Structure You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents Introduction A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS. Understanding GWAS Summary Statistics When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents Matching the Base and Target Data sets The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents Linkage Disequilibrium in PRS Analyses GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents Performing Clumping Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents P-Value Thresholding Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds. Height PRS using GW-significant SNPs only Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5*\u00d7*10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5*\u00d7*10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents Height PRS across multiple P-value thresholds A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents High Resolution Scoring If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5*\u00d7*10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents Stratifying Samples by PRS An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents Case Control Studies In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres \u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status. Cross-Trait Analysis A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Introduction to Polygenic Risk Scores"},{"location":"old/more/Day2.docx/#introduction-to-polygenic-risk-scores","text":"","title":"Introduction to Polygenic Risk Scores"},{"location":"old/more/Day2.docx/#table-of-contents","text":"Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents","title":"Table of Contents"},{"location":"old/more/Day2.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results","title":"Key Learning Outcomes"},{"location":"old/more/Day2.docx/#resources-you-will-be-using","text":"To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link","title":"Resources you will be using"},{"location":"old/more/Day2.docx/#data-structure","text":"You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents","title":"Data Structure"},{"location":"old/more/Day2.docx/#introduction","text":"A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS.","title":"Introduction"},{"location":"old/more/Day2.docx/#understanding-gwas-summary-statistics","text":"When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents","title":"Understanding GWAS Summary Statistics"},{"location":"old/more/Day2.docx/#matching-the-base-and-target-data-sets","text":"The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents","title":"Matching the Base and Target Data sets"},{"location":"old/more/Day2.docx/#linkage-disequilibrium-in-prs-analyses","text":"GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents","title":"Linkage Disequilibrium in PRS Analyses"},{"location":"old/more/Day2.docx/#performing-clumping","text":"Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents","title":"Performing Clumping"},{"location":"old/more/Day2.docx/#p-value-thresholding","text":"Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds.","title":"P-Value Thresholding"},{"location":"old/more/Day2.docx/#height-prs-using-gw-significant-snps-only","text":"Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5*\u00d7*10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5*\u00d7*10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents","title":"Height PRS using GW-significant SNPs only"},{"location":"old/more/Day2.docx/#height-prs-across-multiple-p-value-thresholds","text":"A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents","title":"Height PRS across multiple P-value thresholds"},{"location":"old/more/Day2.docx/#high-resolution-scoring","text":"If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5*\u00d7*10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents","title":"High Resolution Scoring"},{"location":"old/more/Day2.docx/#stratifying-samples-by-prs","text":"An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents","title":"Stratifying Samples by PRS"},{"location":"old/more/Day2.docx/#case-control-studies","text":"In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary.","title":"Case Control Studies"},{"location":"old/more/Day2.docx/#rscript-prsice_linuxprsicer-prsice-prsice_linuxprsice_linux-base-base_datacadaddtxt-target-target_datatar-snp-markername-a1-effect_allele-a2-noneffect_allele-chr-chr-bp-bp_hg19-stat-beta-beta-pvalue-p_dgc-pheno-target_datacadpheno-binary-target-t-out-resultscadhighres","text":"\u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status.","title":"Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres\n"},{"location":"old/more/Day2.docx/#cross-trait-analysis","text":"A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis.","title":"Cross-Trait Analysis"},{"location":"old/more/Day2.docx/#rscript-prsice_linuxprsicer-prsice-prsice_linuxprsice_linux-base-base_datagiant_heighttxt-target-target_datatar-snp-markername-a1-allele1-a2-allele2-stat-b-beta-pvalue-p-pheno-target_datacadpheno-binary-target-t-out-resultscrosshighres","text":"Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres\n"},{"location":"old/more/Day2b.docx/","text":"Advanced Polygenic Risk Score Analyses Day 2 - Afternoon Practical: Pathway PRS Analyses Table of Contents Introduction to gene set (pathway) PRS analysis Inputs required for gene-set PRS analysis Molecular Signatures Database MSigDB Other inputs that can be used for gene-set PRS using PRSet Exercise: Calculate gene-set PRS analysis Considerations when analysing and interpreting gene set PRSs Clumping for each gene set independently P-value thresholding in gene set PRS analyses Self-contained vs competitive testing Key Learning Outcomes After completing this practical, you should be able to: 1. Understand the motivation and rationale for calculating gene set PRS. 2. Identify the additional inputs required for gene set PRS analysis. 3. Calculate gene set based PRSs using PRSet. 5. Understand and interpret the outcomes of gene-set PRSs and how they differ from genome-wide PRS. Data Structure You will find all practical materials in the data/Day_2b directory. Relevant materials that you should find at the start of the practical are: \ud83d\udcc2: Base_Data - GIANT_Height.txt, \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.covariate \ud83d\udcc1: Reference - Homo_sapiens.GRCh38.109.gtf.gz - Sets.gmt Introduction to gene set (pathway) PRS analysis Most PRS methods summarize genetic risk to a single number, based on the aggregation of an individual\u2019s genome-wide risk alleles. This approach does not consider the different contributions of the various biological processes that can influence complex diseases and traits. During this session, we will learn how to run a gene set (a.k.a. pathway) based PRS analyses. The key difference between genome-wide PRS and gene set or pathway-based PRSs analyses is that, instead of aggregating the estimated effects of risk alleles across the entire genome, gene-set PRSs aggregate risk alleles across k gene sets separately (Figure 1). Figure 1: The pathway polygenic risk score approach. Coloured boxes represent genes, lines link genes that are within the same genomic pathway. See full description here . \ud83d\udccc In this practical, we will go through some of the additional input requirements and considerations for the analysis of gene-set PRS analysis, and will then calculate some gene-set based PRS using PRSet . By aggregating PRS across multiple gene sets (or pathways), these PRS analyses will allow us to determine the genetic contribution made by each biological process in complex traits and diseases. For more information about the rationale and the software that we are going to use, please see the PRSet publication PRSet: Pathway-based polygenic risk score analyses and software . \u2753 Why is it useful to have polygenic scores measured across gene-sets (or pathways) for individuals? Isn\u2019t it su\ufb03cient to just obtain a ranking of gene-sets according to GWAS-signal enrichment (using gene set enrichment tools such as MAGMA or partitioned LDSC)? Inputs required for gene-set PRS analysis Summary statistics from GWAS, as well as individual level genotype and phenotype data are required to perform gene-set PRS analyses. In this session, the following Base and Target data is used. Base data is publicly available. All Target data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Data Set Description Download Link Base from the GIANT Consortium GWAS of height on 253,288 individuals Link Simulated Target Data Individual-level phenotype and genotype files Data folder Additionally, to perform gene set PRS analyses, information about the genomic regions for which we want to calculate the PRSs are required. In this tutorial, we will use as input gene-sets from the Molecular Signatures Database . However, PRSet also takes as input BED and SNP files . Data Set Description Download Link Ensembl Human Genome GTF file A file containing the coordinates for genes in the human genome. Used by PRSet to map the SNPs onto genic regions Link to Homo_sapiens.GRCh38.109.gtf.gz MSigDB Gene Sets File containing the gene-set information. Free registration required. Download link after registration Molecular Signatures Database MSigDB + General Transfer Format file MSigDB o\ufb00ers an excellent source of gene sets, including the hallmark genes, gene-sets of di\ufb00erent biological processes, gene-sets of di\ufb00erent oncogenic signatures etc. All gene-sets from MSigDB follows the Gene Matrix Transposed file format (GMT), which consists of one line per gene-set, each containing at least 3 column of data: Set A Description Gene 1 Gene 2 ... Set A Description Gene 1 Gene 2 ... \ud83d\udcac While you can read the GMT file using Excel, we recommend exploring these files using bash. You should be aware that Excel has a tendency to convert gene names into dates (e.g. SEPT9 to Sep-9) ** Have a look at the Reference/Sets.gmt file. ** \u2753 How many gene sets are there in the Reference/Sets.gmt file? \u2753 How many genes does the largest gene set contain? As GMT format does not contain the chromosomal location for each individual gene, an additional file (General Transfer Format file) is required to provide the chromosomal location such that SNPs can be map to genes. The General Transfer Format (GTF) file contains the chromosomal coordinates for each gene. It is a tab separated file and all but the final field in each feature line must contain a value. You can read the full format specification here . Two columns in the GTF file that might be of particular interest are: - Column 3: feature , which indicates what feature that line of GTF represents. This allows us to select or ignore features that are of interest. Column 9: attribute , which contains a semicolon-separated list of tag-value pairs (separated by a space), providing additional information about each feature. A key can be repeated multiple times. > \ud83d\udccc Tip , to parse column 9 and split the additional information in separate columns, you can use the following code: library(data.table) library(magrittr) # Function to extract attributes from GTF files: extract_attribute = function(input, attribute) { strsplit(input, split = \";\") %>% unlist %>% .[grepl(attribute, .)] %>% gsub(\"\\\"\", \"\", .) %>% strsplit(., split = \" \") %>% unlist %>% tail(n = 1) %>% return } gtf38 = fread(\"./Reference/Homo_sapiens.GRCh38.109.gtf.gz\") gtf38_parsed = gtf38 %>% # Select genes only .[V3 == \"gene\"] %>% # Select genes located in autosomes .[`#!genebuild-last-updated 2022-11` %in% 1:22] %>% # Create colummns with Gene information .[, c(\"chr\", \"Gene_start\", \"Gene_end\", \"ensemblID\", \"Gene_Name\", \"Gene_biotype\") := data.table( `#!genebuild-last-updated 2022-11`, V4, V5, sapply(V9, extract_attribute, \"gene_id\"), sapply(V9, extract_attribute, \"gene_name\"), sapply(V9, extract_attribute, \"gene_biotype\"))] %>% # Filter the columns of interest .[, c(\"chr\", \"Gene_start\", \"Gene_end\", \"ensemblID\", \"Gene_Name\", \"Gene_biotype\")] ** Have a look at the Reference/Homo_sapiens.GRCh38.109.gtf.gz file. ** \u2753 How many instances of feature = \"gene\" can you find ? \u2753 How many protein-coding genes are there in the Reference/Homo_sapiens.GRCh38.109.gtf.gz file? Other inputs that can be used for gene set PRS using PRSet Browser Extensible Data BED Browser Extensible Data (BED) file (di\ufb00erent to the binary ped file from PLINK), is a file format to define genetic regions. It contains 3 required fields per line (chromosome, start coordinate and end coordinate) together with 9 additional optional field. A special property of BED is that it is a 0-based format, i.e. chromosome starts at 0, as opposed to the usual 1-based format such as the PLINK format. For example, a SNP on chr1:10000 will be represented as: 1 9999 10000 \u2753 How should we represent the coordinate of rs2980300 (chr1:785989) in BED format? List of SNPs Finally, PRSet also allow SNP sets, where the user have flexibility to decide what SNPs are included. The list of SNPs can have two different formats: - SNP list format, a file containing a single column of SNP ID. Name of the set will be the file name or can be provided using --snp-set File:Name - MSigDB format: Each row represent a single SNP set with the first column containing the name of the SNP set. Back to Top Exercise: Calculate gene set PRS analysis We are now ready to perform gene-set association analyses using PRSet. To perform the PRSet analysis and obtain the set based PRS and competitive P-value, simply provide the GTF file and the GMT file to PRSice and specify the number of permutation for competitive P-value calculation using the --set-perm option. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_linux \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --A1 Allele1 \\ --A2 Allele2 \\ --snp MarkerName \\ --pvalue p \\ --stat b \\ --beta \\ --binary-target F \\ --pheno Target_Data/TAR.height \\ --cov Target_Data/TAR.covariate \\ --out Height.set \\ --gtf Reference/Homo_sapiens.GRCh38.109.gtf.gz \\ --wind-5 5kb \\ --wind-3 1kb \\ --msigdb Reference/Sets.gmt \\ --multi-plot 10 \\ --set-perm 1000 > \ud83d\udccc If the --wind-5 and --wind-3 flag is not specified, PRSet will use the exact coordinates of each gene as the boundary. By specifying eg. --wind-5 5kb and --wind-3 1kb then the boundary of each gene will be extended 5 kb towards the 5\u2019 end and 1 kb towards the 3\u2019 end so that regulatory elements of the gene can be included. Results and Plots specific of gene set PRS analyses ** Check the .summary results file, with and without running the PRSet specific options ** \u2753 How does this file change? What extra information is incorporated when including the PRSet specific commands? Apart from the output files, running the PRSet options will provide extra information about the new gene set PRSs calculated. For example, a new figure with the results for each gene set PRS. Figure 2 : An example of the multi-set plot. Sets are sorted based on their self-contained R2. Base is the genome wide PRS. Considerations when analysing and interpreting gene-set PRSs Clumping for each gene set independently In standard clumping and P-value thresholding methods, clumping is performed to account for linkage disequilibrium between SNPs. If genome-wide clumping is performed at the gene-set level, we may remove signal as shown in this toy example . To maximize signal within each gene set, clumping is performed for each gene set separately. \u2753 Check the '.summary' file. How many SNPs are included in the top 9 gene sets? \u2753 Can you plot the relationship between the gene-set R2 and the number of SNPs in each gene-set? What general trend can be seen? P-value thresholding in gene set PRS analyses PRSet default option is to no not perform p-value thresholding. It will simply calculate the set based PRS at P-value threshold of 1. \u2753 Why do you think that the default option of PRSet is P-value threshold of 1? \u2753 In what cases would you like to apply P-value thresholding? What commands would you use for that? Self-contained vs competitive testing An important aspect when calculating gene-set based PRSs is the type of test used for association. Since we are only considering one region of the genome, self-contained and/or competitive tests can be performed. The null-hypothesis of self-contained and competitive test statistics is di\ufb00erent: \u2013 Self-Contained - None of the genes within the gene-set are associated with the phenotype \u2013 Competitive - Genes within the gene-set are no more associated with the phenotype than genes outside the gene-set Therefore, a bigger gene-set will have a higher likelihood of having a significant P -value from self-contained test, which is not desirable. \u2753 What are the self-contained P-value of the 3 gene-sets with the highest R2? And what are the competitive P-values? \u2753 Which P-value would you use to evaluate the importance of gene-set? \u2753 Considering the R2 vs number of SNPs plot as well as the competitive P-value results, what gene-sets do you think are most interesting and why? Back to Top","title":"Advanced Polygenic Risk Score Analyses"},{"location":"old/more/Day2b.docx/#advanced-polygenic-risk-score-analyses","text":"","title":"Advanced Polygenic Risk Score Analyses"},{"location":"old/more/Day2b.docx/#day-2-afternoon-practical-pathway-prs-analyses","text":"","title":"Day 2 - Afternoon Practical: Pathway PRS Analyses"},{"location":"old/more/Day2b.docx/#table-of-contents","text":"Introduction to gene set (pathway) PRS analysis Inputs required for gene-set PRS analysis Molecular Signatures Database MSigDB Other inputs that can be used for gene-set PRS using PRSet Exercise: Calculate gene-set PRS analysis Considerations when analysing and interpreting gene set PRSs Clumping for each gene set independently P-value thresholding in gene set PRS analyses Self-contained vs competitive testing","title":"Table of Contents"},{"location":"old/more/Day2b.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: 1. Understand the motivation and rationale for calculating gene set PRS. 2. Identify the additional inputs required for gene set PRS analysis. 3. Calculate gene set based PRSs using PRSet. 5. Understand and interpret the outcomes of gene-set PRSs and how they differ from genome-wide PRS.","title":"Key Learning Outcomes"},{"location":"old/more/Day2b.docx/#data-structure","text":"You will find all practical materials in the data/Day_2b directory. Relevant materials that you should find at the start of the practical are: \ud83d\udcc2: Base_Data - GIANT_Height.txt, \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.covariate \ud83d\udcc1: Reference - Homo_sapiens.GRCh38.109.gtf.gz - Sets.gmt","title":"Data Structure"},{"location":"old/more/Day2b.docx/#introduction-to-gene-set-pathway-prs-analysis","text":"Most PRS methods summarize genetic risk to a single number, based on the aggregation of an individual\u2019s genome-wide risk alleles. This approach does not consider the different contributions of the various biological processes that can influence complex diseases and traits. During this session, we will learn how to run a gene set (a.k.a. pathway) based PRS analyses. The key difference between genome-wide PRS and gene set or pathway-based PRSs analyses is that, instead of aggregating the estimated effects of risk alleles across the entire genome, gene-set PRSs aggregate risk alleles across k gene sets separately (Figure 1). Figure 1: The pathway polygenic risk score approach. Coloured boxes represent genes, lines link genes that are within the same genomic pathway. See full description here . \ud83d\udccc In this practical, we will go through some of the additional input requirements and considerations for the analysis of gene-set PRS analysis, and will then calculate some gene-set based PRS using PRSet . By aggregating PRS across multiple gene sets (or pathways), these PRS analyses will allow us to determine the genetic contribution made by each biological process in complex traits and diseases. For more information about the rationale and the software that we are going to use, please see the PRSet publication PRSet: Pathway-based polygenic risk score analyses and software . \u2753 Why is it useful to have polygenic scores measured across gene-sets (or pathways) for individuals? Isn\u2019t it su\ufb03cient to just obtain a ranking of gene-sets according to GWAS-signal enrichment (using gene set enrichment tools such as MAGMA or partitioned LDSC)?","title":"Introduction to gene set (pathway) PRS analysis"},{"location":"old/more/Day2b.docx/#inputs-required-for-gene-set-prs-analysis","text":"Summary statistics from GWAS, as well as individual level genotype and phenotype data are required to perform gene-set PRS analyses. In this session, the following Base and Target data is used. Base data is publicly available. All Target data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Data Set Description Download Link Base from the GIANT Consortium GWAS of height on 253,288 individuals Link Simulated Target Data Individual-level phenotype and genotype files Data folder Additionally, to perform gene set PRS analyses, information about the genomic regions for which we want to calculate the PRSs are required. In this tutorial, we will use as input gene-sets from the Molecular Signatures Database . However, PRSet also takes as input BED and SNP files . Data Set Description Download Link Ensembl Human Genome GTF file A file containing the coordinates for genes in the human genome. Used by PRSet to map the SNPs onto genic regions Link to Homo_sapiens.GRCh38.109.gtf.gz MSigDB Gene Sets File containing the gene-set information. Free registration required. Download link after registration","title":"Inputs required for gene-set PRS analysis"},{"location":"old/more/Day2b.docx/#molecular-signatures-database-msigdb-general-transfer-format-file","text":"MSigDB o\ufb00ers an excellent source of gene sets, including the hallmark genes, gene-sets of di\ufb00erent biological processes, gene-sets of di\ufb00erent oncogenic signatures etc. All gene-sets from MSigDB follows the Gene Matrix Transposed file format (GMT), which consists of one line per gene-set, each containing at least 3 column of data: Set A Description Gene 1 Gene 2 ... Set A Description Gene 1 Gene 2 ... \ud83d\udcac While you can read the GMT file using Excel, we recommend exploring these files using bash. You should be aware that Excel has a tendency to convert gene names into dates (e.g. SEPT9 to Sep-9) ** Have a look at the Reference/Sets.gmt file. ** \u2753 How many gene sets are there in the Reference/Sets.gmt file? \u2753 How many genes does the largest gene set contain? As GMT format does not contain the chromosomal location for each individual gene, an additional file (General Transfer Format file) is required to provide the chromosomal location such that SNPs can be map to genes. The General Transfer Format (GTF) file contains the chromosomal coordinates for each gene. It is a tab separated file and all but the final field in each feature line must contain a value. You can read the full format specification here . Two columns in the GTF file that might be of particular interest are: - Column 3: feature , which indicates what feature that line of GTF represents. This allows us to select or ignore features that are of interest. Column 9: attribute , which contains a semicolon-separated list of tag-value pairs (separated by a space), providing additional information about each feature. A key can be repeated multiple times.","title":"Molecular Signatures Database MSigDB + General Transfer Format file"},{"location":"old/more/Day2b.docx/#tip-to-parse-column-9-and-split-the-additional-information-in-separate-columns-you-can-use-the-following-code","text":"library(data.table) library(magrittr) # Function to extract attributes from GTF files: extract_attribute = function(input, attribute) { strsplit(input, split = \";\") %>% unlist %>% .[grepl(attribute, .)] %>% gsub(\"\\\"\", \"\", .) %>% strsplit(., split = \" \") %>% unlist %>% tail(n = 1) %>% return } gtf38 = fread(\"./Reference/Homo_sapiens.GRCh38.109.gtf.gz\") gtf38_parsed = gtf38 %>% # Select genes only .[V3 == \"gene\"] %>% # Select genes located in autosomes .[`#!genebuild-last-updated 2022-11` %in% 1:22] %>% # Create colummns with Gene information .[, c(\"chr\", \"Gene_start\", \"Gene_end\", \"ensemblID\", \"Gene_Name\", \"Gene_biotype\") := data.table( `#!genebuild-last-updated 2022-11`, V4, V5, sapply(V9, extract_attribute, \"gene_id\"), sapply(V9, extract_attribute, \"gene_name\"), sapply(V9, extract_attribute, \"gene_biotype\"))] %>% # Filter the columns of interest .[, c(\"chr\", \"Gene_start\", \"Gene_end\", \"ensemblID\", \"Gene_Name\", \"Gene_biotype\")] ** Have a look at the Reference/Homo_sapiens.GRCh38.109.gtf.gz file. ** \u2753 How many instances of feature = \"gene\" can you find ? \u2753 How many protein-coding genes are there in the Reference/Homo_sapiens.GRCh38.109.gtf.gz file?","title":"&gt; \ud83d\udccc Tip, to parse column 9 and split the additional information in separate columns, you can use the following code:"},{"location":"old/more/Day2b.docx/#other-inputs-that-can-be-used-for-gene-set-prs-using-prset","text":"","title":"Other inputs that can be used for gene set PRS using PRSet"},{"location":"old/more/Day2b.docx/#browser-extensible-data-bed","text":"Browser Extensible Data (BED) file (di\ufb00erent to the binary ped file from PLINK), is a file format to define genetic regions. It contains 3 required fields per line (chromosome, start coordinate and end coordinate) together with 9 additional optional field. A special property of BED is that it is a 0-based format, i.e. chromosome starts at 0, as opposed to the usual 1-based format such as the PLINK format. For example, a SNP on chr1:10000 will be represented as: 1 9999 10000 \u2753 How should we represent the coordinate of rs2980300 (chr1:785989) in BED format?","title":"Browser Extensible Data BED"},{"location":"old/more/Day2b.docx/#list-of-snps","text":"Finally, PRSet also allow SNP sets, where the user have flexibility to decide what SNPs are included. The list of SNPs can have two different formats: - SNP list format, a file containing a single column of SNP ID. Name of the set will be the file name or can be provided using --snp-set File:Name - MSigDB format: Each row represent a single SNP set with the first column containing the name of the SNP set. Back to Top","title":"List of SNPs"},{"location":"old/more/Day2b.docx/#exercise-calculate-gene-set-prs-analysis","text":"We are now ready to perform gene-set association analyses using PRSet. To perform the PRSet analysis and obtain the set based PRS and competitive P-value, simply provide the GTF file and the GMT file to PRSice and specify the number of permutation for competitive P-value calculation using the --set-perm option. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_linux \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --A1 Allele1 \\ --A2 Allele2 \\ --snp MarkerName \\ --pvalue p \\ --stat b \\ --beta \\ --binary-target F \\ --pheno Target_Data/TAR.height \\ --cov Target_Data/TAR.covariate \\ --out Height.set \\ --gtf Reference/Homo_sapiens.GRCh38.109.gtf.gz \\ --wind-5 5kb \\ --wind-3 1kb \\ --msigdb Reference/Sets.gmt \\ --multi-plot 10 \\ --set-perm 1000","title":"Exercise: Calculate gene set PRS analysis"},{"location":"old/more/Day2b.docx/#if-the-wind-5-and-wind-3-flag-is-not-specified-prset-will-use-the-exact-coordinates-of-each-gene-as-the-boundary-by-specifying-eg-wind-5-5kb-and-wind-3-1kb-then-the-boundary-of-each-gene-will-be-extended-5-kb-towards-the-5-end-and-1-kb-towards-the-3-end-so-that-regulatory-elements-of-the-gene-can-be-included","text":"","title":"&gt; \ud83d\udccc If the --wind-5 and --wind-3 flag is not specified, PRSet will use the exact coordinates of each gene as the boundary. By specifying eg. --wind-5 5kb and --wind-3 1kb then the boundary of each gene will be extended 5 kb towards the 5\u2019 end and 1 kb towards the 3\u2019 end so that regulatory elements of the gene can be included."},{"location":"old/more/Day2b.docx/#results-and-plots-specific-of-gene-set-prs-analyses","text":"** Check the .summary results file, with and without running the PRSet specific options ** \u2753 How does this file change? What extra information is incorporated when including the PRSet specific commands? Apart from the output files, running the PRSet options will provide extra information about the new gene set PRSs calculated. For example, a new figure with the results for each gene set PRS. Figure 2 : An example of the multi-set plot. Sets are sorted based on their self-contained R2. Base is the genome wide PRS.","title":"Results and Plots specific of gene set PRS analyses"},{"location":"old/more/Day2b.docx/#considerations-when-analysing-and-interpreting-gene-set-prss","text":"","title":"Considerations when analysing and interpreting gene-set PRSs"},{"location":"old/more/Day2b.docx/#clumping-for-each-gene-set-independently","text":"In standard clumping and P-value thresholding methods, clumping is performed to account for linkage disequilibrium between SNPs. If genome-wide clumping is performed at the gene-set level, we may remove signal as shown in this toy example . To maximize signal within each gene set, clumping is performed for each gene set separately. \u2753 Check the '.summary' file. How many SNPs are included in the top 9 gene sets? \u2753 Can you plot the relationship between the gene-set R2 and the number of SNPs in each gene-set? What general trend can be seen?","title":"Clumping for each gene set independently"},{"location":"old/more/Day2b.docx/#p-value-thresholding-in-gene-set-prs-analyses","text":"PRSet default option is to no not perform p-value thresholding. It will simply calculate the set based PRS at P-value threshold of 1. \u2753 Why do you think that the default option of PRSet is P-value threshold of 1? \u2753 In what cases would you like to apply P-value thresholding? What commands would you use for that?","title":"P-value thresholding in gene set PRS analyses"},{"location":"old/more/Day2b.docx/#self-contained-vs-competitive-testing","text":"An important aspect when calculating gene-set based PRSs is the type of test used for association. Since we are only considering one region of the genome, self-contained and/or competitive tests can be performed. The null-hypothesis of self-contained and competitive test statistics is di\ufb00erent: \u2013 Self-Contained - None of the genes within the gene-set are associated with the phenotype \u2013 Competitive - Genes within the gene-set are no more associated with the phenotype than genes outside the gene-set Therefore, a bigger gene-set will have a higher likelihood of having a significant P -value from self-contained test, which is not desirable. \u2753 What are the self-contained P-value of the 3 gene-sets with the highest R2? And what are the competitive P-values? \u2753 Which P-value would you use to evaluate the importance of gene-set? \u2753 Considering the R2 vs number of SNPs plot as well as the competitive P-value results, what gene-sets do you think are most interesting and why? Back to Top","title":"Self-contained vs competitive testing"},{"location":"old/more/Day3a.docx/","text":"Advanced Polygenic Risk Score Analyses Day 3 - Polygenic Risk Score Analyses Workshop 2024 Table of Contents Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2 Day 3a practical Key Learning Outcomes After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds. Base and Target datasets In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650) Downloading Datasets All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam Method for calculating PRS For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach. Exercise 1 Estimating R 2 Code Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2 Scenario 1: Predicting from EUR training to EUR target data: Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur Key code parameters The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389. Scenario 2: Predicting from AFR training to AFR target data: Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%). Scenario 3: Predicting from EUR training to AFR target data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%). Exercise 2 Visualising and comparing R 2 In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library ( ggplot2 ) library ( RColorBrewer ) # Create a function to read the files and add ancestry information read_and_label <- function ( file, ancestry ) { data <- read.table ( file, header = TRUE, sep = \"\\t\" ) data $Ancestry <- ancestry return ( data ) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label ( \"SBP.eur.eur.summary\" , \"EUR_EUR\" ) AFR_AFR <- read_and_label ( \"SBP.afr.afr.summary\" , \"AFR_AFR\" ) EUR_AFR <- read_and_label ( \"SBP.eur.afr.summary\" , \"EUR_AFR\" ) # Combine all data into one dataframe all_data <- rbind ( EUR_EUR, AFR_AFR, EUR_AFR ) # Create a bar graph with different colors for each ancestry png ( '/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) ancestry <- ggplot ( all_data, aes ( x = Ancestry, y = PRS.R2, fill = Ancestry )) + geom_bar ( stat = \"identity\" , position = \"dodge\" ) + labs ( title = \"R2 Values by Ancestry\" , x = \"Ancestry\" , y = \"R2 Value\" ) + theme_minimal () + scale_fill_brewer ( palette = \"Set3\" ) + theme ( plot.title = element_text ( hjust = 0 .5, size = 16 , face = \"bold\" ) , axis.title.x = element_text ( size = 14 , face = \"bold\" ) , axis.title.y = element_text ( size = 14 , face = \"bold\" ) , axis.text.x = element_text ( size = 12 , angle = 45 , hjust = 1) , axis.text.y = element_text ( size = 12) , legend.position = \"none\" ) print ( ancestry ) dev.off () Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Advanced Polygenic Risk Score Analyses"},{"location":"old/more/Day3a.docx/#advanced-polygenic-risk-score-analyses","text":"","title":"Advanced Polygenic Risk Score Analyses"},{"location":"old/more/Day3a.docx/#day-3-polygenic-risk-score-analyses-workshop-2024","text":"","title":"Day 3 - Polygenic Risk Score Analyses Workshop 2024"},{"location":"old/more/Day3a.docx/#table-of-contents","text":"Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2","title":"Table of Contents"},{"location":"old/more/Day3a.docx/#day-3a-practical","text":"","title":"Day 3a practical"},{"location":"old/more/Day3a.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds.","title":"Key Learning Outcomes"},{"location":"old/more/Day3a.docx/#base-and-target-datasets","text":"In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650)","title":"Base and Target datasets"},{"location":"old/more/Day3a.docx/#downloading-datasets","text":"All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam","title":"Downloading Datasets"},{"location":"old/more/Day3a.docx/#method-for-calculating-prs","text":"For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach.","title":"Method for calculating PRS"},{"location":"old/more/Day3a.docx/#exercise-1-estimating-r2","text":"","title":"Exercise 1 Estimating R2"},{"location":"old/more/Day3a.docx/#code","text":"Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2","title":"Code"},{"location":"old/more/Day3a.docx/#scenario-1-predicting-from-eur-training-to-eur-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur","title":"Scenario 1: Predicting from EUR training to EUR target data:"},{"location":"old/more/Day3a.docx/#key-code-parameters","text":"The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389.","title":"Key code parameters"},{"location":"old/more/Day3a.docx/#scenario-2-predicting-from-afr-training-to-afr-target-data","text":"Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%).","title":"Scenario 2: Predicting from AFR training to AFR target data:"},{"location":"old/more/Day3a.docx/#scenario-3-predicting-from-eur-training-to-afr-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%).","title":"Scenario 3: Predicting from EUR training to AFR target data"},{"location":"old/more/Day3a.docx/#exercise-2-visualising-and-comparing-r2","text":"In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library ( ggplot2 ) library ( RColorBrewer ) # Create a function to read the files and add ancestry information read_and_label <- function ( file, ancestry ) { data <- read.table ( file, header = TRUE, sep = \"\\t\" ) data $Ancestry <- ancestry return ( data ) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label ( \"SBP.eur.eur.summary\" , \"EUR_EUR\" ) AFR_AFR <- read_and_label ( \"SBP.afr.afr.summary\" , \"AFR_AFR\" ) EUR_AFR <- read_and_label ( \"SBP.eur.afr.summary\" , \"EUR_AFR\" ) # Combine all data into one dataframe all_data <- rbind ( EUR_EUR, AFR_AFR, EUR_AFR ) # Create a bar graph with different colors for each ancestry png ( '/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) ancestry <- ggplot ( all_data, aes ( x = Ancestry, y = PRS.R2, fill = Ancestry )) + geom_bar ( stat = \"identity\" , position = \"dodge\" ) + labs ( title = \"R2 Values by Ancestry\" , x = \"Ancestry\" , y = \"R2 Value\" ) + theme_minimal () + scale_fill_brewer ( palette = \"Set3\" ) + theme ( plot.title = element_text ( hjust = 0 .5, size = 16 , face = \"bold\" ) , axis.title.x = element_text ( size = 14 , face = \"bold\" ) , axis.title.y = element_text ( size = 14 , face = \"bold\" ) , axis.text.x = element_text ( size = 12 , angle = 45 , hjust = 1) , axis.text.y = element_text ( size = 12) , legend.position = \"none\" ) print ( ancestry ) dev.off () Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Exercise 2 Visualising and comparing R2"},{"location":"old/more/Day3b.docx/","text":"Day 3b practical We need to move into the directory you will be working in; cd ~/data/Data_Day4/data Introduction to Cross-Ancestry PRS computation Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python =3 .7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion. 1. The 1000 Genomes datasets The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week. 2. Cross-population allele frequency Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l Questions (i) Which population contains the most SNPs? (ii) What is the significance of the observed population order? 3. Distribution of allele frequencies In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages ( \"dplyr\" ) install.packages ( \"ggplot2\" ) # Load necessary libraries library ( dplyr ) library ( ggplot2 ) # Create a function to read the files and add ancestry information freq <-read.table ( \"~/data/Data_Day4/plink.frq.strat\" , header = T ) plotDat <- freq %>% mutate ( AlleleFrequency = cut ( MAF, seq (0 , 1 , 0 .25 ))) %>% group_by ( AlleleFrequency, CLST ) %>% summarise ( FractionOfSNPs = n () /nrow ( freq ) * 100) # Create a bar graph png ( '/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) maf_ancestry <- ggplot ( na.omit ( plotDat ) , aes ( AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST )) + geom_line () + scale_y_continuous ( limits = c (0 , 12)) + ggtitle ( \"Distribution of allele frequency across genome\" ) print ( maf_ancestry ) dev.off () Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png Questions (i) Which population has the most SNPs? (ii) What is the significance of the observed population ordering? (iii) What is the reason behind these two features? Introduction to PRS-CSx 5. Background to PRS-CSX PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and LD reference data from multiple populations to estimate population-specific PRS. PRS-CSx applies a Bayesian model with a continuous shrinkage prior to SNP effects genome-wide. Sparseness of the genetic architecture across populations is controlled by a parameter phi. For a given value of phi, PRS-CSx uses Markov chain Monte Carlo to sample from the posterior of SNP effects from which the mean SNP effects are calculated and used in the PRS. Step 1: Set up environment First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE = \"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21 ..22 } ; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr ${ chr } _only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr ${ chr } ,/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr ${ chr } \\ --n_gwas=25732,4855 \\ --chrom= ${ chr } \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr ${ chr } .csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3) Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE) Task: Replace 'ancestry <- \"EUR\" ' with 'ancestry <- \"AFR\" ' and repeat the subsequent steps shown above Step 4: Merge genotype-phenotype data Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen) Step 5: Split data into validation and test sets Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices] Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights # Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur) Step 7: Prepare the variant weights matrices as vectors # Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat)) Step 8: Predict phenotype on validation and test dataset Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z Step 9: Plot phenotype distributions of validation and test data: Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal() Step 10: Plot true values against predicted values The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off() Step 11: Calculate deviance-based R 2 # Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2)) Results graph: pdf true against pred","title":"Day3b.docx"},{"location":"old/more/Day3b.docx/#day-3b-practical","text":"We need to move into the directory you will be working in; cd ~/data/Data_Day4/data","title":"Day 3b practical"},{"location":"old/more/Day3b.docx/#introduction-to-cross-ancestry-prs-computation","text":"Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python =3 .7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion.","title":"Introduction to Cross-Ancestry PRS computation"},{"location":"old/more/Day3b.docx/#1-the-1000-genomes-datasets","text":"The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week.","title":"1. The 1000 Genomes datasets"},{"location":"old/more/Day3b.docx/#2-cross-population-allele-frequency","text":"Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l","title":"2. Cross-population allele frequency"},{"location":"old/more/Day3b.docx/#questions","text":"","title":"Questions"},{"location":"old/more/Day3b.docx/#i-which-population-contains-the-most-snps","text":"","title":"(i) Which population contains the most SNPs?"},{"location":"old/more/Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-order","text":"","title":"(ii) What  is the significance of the observed population order?"},{"location":"old/more/Day3b.docx/#3-distribution-of-allele-frequencies","text":"In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages ( \"dplyr\" ) install.packages ( \"ggplot2\" ) # Load necessary libraries library ( dplyr ) library ( ggplot2 ) # Create a function to read the files and add ancestry information freq <-read.table ( \"~/data/Data_Day4/plink.frq.strat\" , header = T ) plotDat <- freq %>% mutate ( AlleleFrequency = cut ( MAF, seq (0 , 1 , 0 .25 ))) %>% group_by ( AlleleFrequency, CLST ) %>% summarise ( FractionOfSNPs = n () /nrow ( freq ) * 100) # Create a bar graph png ( '/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png' , unit = 'px' , res =300 , width =3500 , height =4500) maf_ancestry <- ggplot ( na.omit ( plotDat ) , aes ( AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST )) + geom_line () + scale_y_continuous ( limits = c (0 , 12)) + ggtitle ( \"Distribution of allele frequency across genome\" ) print ( maf_ancestry ) dev.off () Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png","title":"3. Distribution of allele frequencies"},{"location":"old/more/Day3b.docx/#questions_1","text":"","title":"Questions"},{"location":"old/more/Day3b.docx/#i-which-population-has-the-most-snps","text":"","title":"(i) Which population has the most SNPs?"},{"location":"old/more/Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-ordering","text":"","title":"(ii) What  is the significance of the observed population ordering?"},{"location":"old/more/Day3b.docx/#iii-what-is-the-reason-behind-these-two-features","text":"","title":"(iii) What is the reason behind these two features?"},{"location":"old/more/Day3b.docx/#introduction-to-prs-csx","text":"","title":"Introduction to PRS-CSx"},{"location":"old/more/Day3b.docx/#5-background-to-prs-csx","text":"PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and LD reference data from multiple populations to estimate population-specific PRS. PRS-CSx applies a Bayesian model with a continuous shrinkage prior to SNP effects genome-wide. Sparseness of the genetic architecture across populations is controlled by a parameter phi. For a given value of phi, PRS-CSx uses Markov chain Monte Carlo to sample from the posterior of SNP effects from which the mean SNP effects are calculated and used in the PRS.","title":"5. Background to PRS-CSX"},{"location":"old/more/Day3b.docx/#step-1-set-up-environment","text":"First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS","title":"Step 1: Set up environment"},{"location":"old/more/Day3b.docx/#step-2-run-csx-derive-new-snps-weights-trained-on-european-and-african-summary-stats","text":"Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE = \"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21 ..22 } ; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr ${ chr } _only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr ${ chr } ,/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr ${ chr } \\ --n_gwas=25732,4855 \\ --chrom= ${ chr } \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr ${ chr } .csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh","title":"Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats"},{"location":"old/more/Day3b.docx/#step-3-combine-csx-derived-snp-weights-across-chromosomes-currently-excludes-chromosome-3","text":"Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE)","title":"Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3)"},{"location":"old/more/Day3b.docx/#task-replace-ancestry-eur-with-ancestry-afr-and-repeat-the-subsequent-steps-shown-above","text":"","title":"Task: Replace 'ancestry &lt;- \"EUR\" ' with 'ancestry &lt;- \"AFR\" ' and repeat the subsequent steps shown above"},{"location":"old/more/Day3b.docx/#step-4-merge-genotype-phenotype-data","text":"Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen)","title":"Step 4: Merge genotype-phenotype data"},{"location":"old/more/Day3b.docx/#step-5-split-data-into-validation-and-test-sets","text":"Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices]","title":"Step 5: Split data into validation and test sets"},{"location":"old/more/Day3b.docx/#step-6-prepare-the-regression-model-input-using-the-csx-derived-afr-and-eur-weights","text":"# Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur)","title":"Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights"},{"location":"old/more/Day3b.docx/#step-7-prepare-the-variant-weights-matrices-as-vectors","text":"# Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat))","title":"Step 7: Prepare the variant weights matrices as vectors"},{"location":"old/more/Day3b.docx/#step-8-predict-phenotype-on-validation-and-test-dataset","text":"Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z","title":"Step 8: Predict phenotype on validation and test dataset"},{"location":"old/more/Day3b.docx/#step-9-plot-phenotype-distributions-of-validation-and-test-data","text":"Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal()","title":"Step 9: Plot phenotype distributions of validation and test data:"},{"location":"old/more/Day3b.docx/#step-10-plot-true-values-against-predicted-values","text":"The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off() Step 11: Calculate deviance-based R 2 # Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2))","title":"Step 10: Plot true values against predicted values"},{"location":"old/more/Day3b.docx/#results","text":"graph: pdf true against pred","title":"Results"},{"location":"old/more/Day4a.docx/","text":"BridgePRS Learning Objectives The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method. Introduction to BridgePRS BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values. BridgePRS Scenario 1: Application of African GWAS weights to an African target group Create configuration file for the target-only analysis In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes). Questions From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ? Note In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step. Single ancestry BridgePRS analysis: Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y Task Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders Questions What evidence can you see that the analysis was successfully executed? BridgePRS Scenario 2: Prediction into African target data using European and African summary statistics BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations. Create configuration file for base and target populations bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat Question Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located? Tasks Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below. Multi-ancestry BRIDGEPRS analysis: Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y Tasks After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory. Questions In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model? Short Quiz I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis? File types ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat Answer Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Day4a.docx"},{"location":"old/more/Day4a.docx/#bridgeprs","text":"","title":"BridgePRS"},{"location":"old/more/Day4a.docx/#learning-objectives","text":"The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method.","title":"Learning Objectives"},{"location":"old/more/Day4a.docx/#introduction-to-bridgeprs","text":"BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values.","title":"Introduction to BridgePRS"},{"location":"old/more/Day4a.docx/#bridgeprs-scenario-1-application-of-african-gwas-weights-to-an-african-target-group","text":"","title":"BridgePRS Scenario 1: Application of African GWAS weights to an African target group"},{"location":"old/more/Day4a.docx/#create-configuration-file-for-the-target-only-analysis","text":"In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes).","title":"Create configuration file for the target-only analysis"},{"location":"old/more/Day4a.docx/#questions","text":"From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ?","title":"Questions"},{"location":"old/more/Day4a.docx/#note","text":"In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step.","title":"Note"},{"location":"old/more/Day4a.docx/#single-ancestry-bridgeprs-analysis","text":"Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y","title":"Single ancestry BridgePRS analysis:"},{"location":"old/more/Day4a.docx/#task","text":"Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders","title":"Task"},{"location":"old/more/Day4a.docx/#questions_1","text":"What evidence can you see that the analysis was successfully executed?","title":"Questions"},{"location":"old/more/Day4a.docx/#bridgeprs-scenario-2-prediction-into-african-target-data-using-european-and-african-summary-statistics","text":"BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations.","title":"BridgePRS Scenario 2:  Prediction into African target data using European and African summary statistics"},{"location":"old/more/Day4a.docx/#create-configuration-file-for-base-and-target-populations","text":"bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat","title":"Create configuration file for base and target populations"},{"location":"old/more/Day4a.docx/#question","text":"Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located?","title":"Question"},{"location":"old/more/Day4a.docx/#tasks","text":"Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below.","title":"Tasks"},{"location":"old/more/Day4a.docx/#multi-ancestry-bridgeprs-analysis","text":"Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y","title":"Multi-ancestry BRIDGEPRS analysis:"},{"location":"old/more/Day4a.docx/#tasks_1","text":"After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory.","title":"Tasks"},{"location":"old/more/Day4a.docx/#questions_2","text":"In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model?","title":"Questions"},{"location":"old/more/Day4a.docx/#short-quiz","text":"I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis?","title":"Short Quiz"},{"location":"old/more/Day4a.docx/#file-types","text":"ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat","title":"File types"},{"location":"old/more/Day4a.docx/#answer","text":"Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Answer"},{"location":"old/more/Day4b.docx/","text":"Day 4 - Practical 2: Introduction to Admixture analysis Module Goals The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores Part1: Plot Decay of Ancestry LD over time Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" ) Questions (i) Describe what happens to admixture LD over time? (ii) Why does recombination also have an impact? Part 2: Global Ancestry Inference We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ./admixture samples_n28_qc_thin.bed 2 --supervised -j4 Questions (i) What do you think the number specified after the inout file represents? The next step is to create a plot the results # Plot Global Ancestry Results R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300) Questions (i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary) Part 3: Local Ancestry Inference Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm. # Run the following UNIX command from the home directory module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done Questions (i) What information is provided in the simulation results table displayed on-screen ? Part 4: Plot local admixture on chromosome 22 In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr) # Function to read the .msp.tsv file read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) } # Function to read the .Q file read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) } # Specification of file paths msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q' # Read in files msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file) # Check for NA or empty column names print(colnames(msp_df)) # Extract ancestry columns based on pattern recognition of column names ancestry_cols <- colnames(msp_df)[grep(\"\\\\.0$|\\\\.1$\", colnames(msp_df))] # Function to determine ancestry based on RFMix codes determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) } # Rename columns uniquely colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62]) # Prepare data for plotting plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) ) # Create plot of chromosome 22 across the sample plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10)) # Save the plot to a file ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8) Questions (i) How many different continental ancestries do you see represented across the 58 strands? (ii) Why is the number of different ancestry backgrounds higher than it was in the previous step? Part 5: Formatting of admixture files for analysis using PRSice Step 1 - Convert phased genotypes to GenomicRange format # Run from the home directory library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() } # Read in phased VCF file vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22) # Convert haplotypes to data frame haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\") # Convert SNP info to data frame snp_info_df <- data.frame(extracted_snp_info22) # Check for numerical and ordering consistency between haplotypes and SNP info if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") } # Merge haplotypes and SNP info merge_chr22 <- cbind(snp_info_df, haps_dt) # Convert to long format using panelr chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) # Insert 'end' column after 'POS' and create 'wave' column chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\")) # Remove row names and drop redundant columns using base R rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops] # Rename columns names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header # Create GRanges object gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Save the GRanges object saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\") # Clean up memory clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\")) Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix # Read in MSP file msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\") # Reformat genome-wide local ancestry output as GRanges object colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE) # Convert the elements of the GRange object into a dataframe chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL # Clean up column headers header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header # Convert local ancestry calls from wide to long format chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\")) # Drop redundant columns drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL # Convert the reconfigured dataframe file into a GRanges object chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Ensure chr22_haplo_long is a GRanges object before finding overlaps chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Find overlaps and store matching features # Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)])) # Local ancestry calls are now aligned with genotypic data and positional coordinates # Convert to dataframe without adding 'X' to numeric column names genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df)) # Output the dataframe write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F) # Clean up memory rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc() Part 5: Step 3 - Create separate Plink files # Partition genotype and local ancestry data chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))] # Reintegrate in interleaved format d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)] # Prepare headers indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx] # Convert LAnc data from long to wide format LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE))) # Check length of LA_wide length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers # Convert genotype calls to horizontal orientation geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes # Clean up and finalize geno_final geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal) # Name columns colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") # Write final tables write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE) Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22 # Perform general QC ahead of running PRSice on ancestry-deconvolved individuals # (i) Remove SNPs with low minor allele count (MAC) # Plink - Remove monomorphic SNPs (minor allele count 0-4). #AFR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done #EUR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done # PRSice - Generate ancestry-specific weights # AFR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base # EUR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base Part 5: Step 5 - Evaluate the Admixture-informed PRS library(dplyr) # Read the PRS files into dataframes file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F) # Add the fourth column of both files file1$PRS_SUM <- file1$PRS + file2$PRS # Load the phenotype data pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\") # Convert phenotype column to numeric pheno$phenotype <- as.numeric(pheno$phenotype) # Merge PRS data with phenotype data merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\")) # Rename columns names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\" # Perform linear regression for each PRS and the combined PRS model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data) # Extract R-squared values r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared # Print the R-squared values cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\") Questions (i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?","title":"Day4b.docx"},{"location":"old/more/Day4b.docx/#day-4-practical-2-introduction-to-admixture-analysis","text":"","title":"Day 4 - Practical 2: Introduction to Admixture analysis"},{"location":"old/more/Day4b.docx/#module-goals","text":"The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores","title":"Module Goals"},{"location":"old/more/Day4b.docx/#part1-plot-decay-of-ancestry-ld-over-time","text":"Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" )","title":"Part1: Plot Decay of Ancestry LD over time"},{"location":"old/more/Day4b.docx/#questions","text":"","title":"Questions"},{"location":"old/more/Day4b.docx/#i-describe-what-happens-to-admixture-ld-over-time","text":"","title":"(i) Describe what happens to admixture LD over time?"},{"location":"old/more/Day4b.docx/#ii-why-does-recombination-also-have-an-impact","text":"","title":"(ii) Why does recombination also have an impact?"},{"location":"old/more/Day4b.docx/#part-2-global-ancestry-inference","text":"We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ./admixture samples_n28_qc_thin.bed 2 --supervised -j4","title":"Part 2: Global Ancestry Inference"},{"location":"old/more/Day4b.docx/#questions_1","text":"","title":"Questions"},{"location":"old/more/Day4b.docx/#i-what-do-you-think-the-number-specified-after-the-inout-file-represents","text":"The next step is to create a plot the results # Plot Global Ancestry Results R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300)","title":"(i) What do you think the number specified after the inout file represents?"},{"location":"old/more/Day4b.docx/#questions_2","text":"","title":"Questions"},{"location":"old/more/Day4b.docx/#i-what-are-the-ancestry-assignments-of-the-28-individuals-provide-estimated-proportions-where-necessary","text":"","title":"(i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary)"},{"location":"old/more/Day4b.docx/#part-3-local-ancestry-inference","text":"Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm. # Run the following UNIX command from the home directory module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done","title":"Part 3: Local Ancestry Inference"},{"location":"old/more/Day4b.docx/#questions_3","text":"","title":"Questions"},{"location":"old/more/Day4b.docx/#i-what-information-is-provided-in-the-simulation-results-table-displayed-on-screen","text":"","title":"(i) What information is provided in the simulation results table displayed on-screen ?"},{"location":"old/more/Day4b.docx/#part-4-plot-local-admixture-on-chromosome-22","text":"In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr) # Function to read the .msp.tsv file read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) } # Function to read the .Q file read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) } # Specification of file paths msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q' # Read in files msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file) # Check for NA or empty column names print(colnames(msp_df)) # Extract ancestry columns based on pattern recognition of column names ancestry_cols <- colnames(msp_df)[grep(\"\\\\.0$|\\\\.1$\", colnames(msp_df))] # Function to determine ancestry based on RFMix codes determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) } # Rename columns uniquely colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62]) # Prepare data for plotting plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) ) # Create plot of chromosome 22 across the sample plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10)) # Save the plot to a file ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8)","title":"Part 4: Plot local admixture on chromosome 22"},{"location":"old/more/Day4b.docx/#questions_4","text":"","title":"Questions"},{"location":"old/more/Day4b.docx/#i-how-many-different-continental-ancestries-do-you-see-represented-across-the-58-strands","text":"","title":"(i) How many different continental ancestries do you see represented across the 58 strands?"},{"location":"old/more/Day4b.docx/#ii-why-is-the-number-of-different-ancestry-backgrounds-higher-than-it-was-in-the-previous-step","text":"","title":"(ii) Why is the number of different ancestry backgrounds higher than it was in the previous step?"},{"location":"old/more/Day4b.docx/#part-5-formatting-of-admixture-files-for-analysis-using-prsice","text":"","title":"Part 5: Formatting of admixture files for analysis using PRSice"},{"location":"old/more/Day4b.docx/#step-1-convert-phased-genotypes-to-genomicrange-format","text":"# Run from the home directory library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() } # Read in phased VCF file vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22) # Convert haplotypes to data frame haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\") # Convert SNP info to data frame snp_info_df <- data.frame(extracted_snp_info22) # Check for numerical and ordering consistency between haplotypes and SNP info if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") } # Merge haplotypes and SNP info merge_chr22 <- cbind(snp_info_df, haps_dt) # Convert to long format using panelr chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) # Insert 'end' column after 'POS' and create 'wave' column chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\")) # Remove row names and drop redundant columns using base R rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops] # Rename columns names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header # Create GRanges object gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Save the GRanges object saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\") # Clean up memory clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\"))","title":"Step 1 - Convert phased genotypes to GenomicRange format"},{"location":"old/more/Day4b.docx/#part-5-step-2-merge-genotypes-from-step-1-with-local-ancestry-calls-by-rfmix","text":"# Read in MSP file msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\") # Reformat genome-wide local ancestry output as GRanges object colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE) # Convert the elements of the GRange object into a dataframe chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL # Clean up column headers header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header # Convert local ancestry calls from wide to long format chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\")) # Drop redundant columns drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL # Convert the reconfigured dataframe file into a GRanges object chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Ensure chr22_haplo_long is a GRanges object before finding overlaps chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) # Find overlaps and store matching features # Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)])) # Local ancestry calls are now aligned with genotypic data and positional coordinates # Convert to dataframe without adding 'X' to numeric column names genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df)) # Output the dataframe write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F) # Clean up memory rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc()","title":"Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix"},{"location":"old/more/Day4b.docx/#part-5-step-3-create-separate-plink-files","text":"# Partition genotype and local ancestry data chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))] # Reintegrate in interleaved format d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)] # Prepare headers indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx] # Convert LAnc data from long to wide format LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE))) # Check length of LA_wide length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers # Convert genotype calls to horizontal orientation geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes # Clean up and finalize geno_final geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal) # Name columns colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") # Write final tables write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE)","title":"Part 5: Step 3 - Create separate Plink files"},{"location":"old/more/Day4b.docx/#part-5-step-4-use-custom-software-rftransform-to-create-plink-files-for-input-into-prsice","text":"module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22 # Perform general QC ahead of running PRSice on ancestry-deconvolved individuals # (i) Remove SNPs with low minor allele count (MAC) # Plink - Remove monomorphic SNPs (minor allele count 0-4). #AFR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done #EUR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done # PRSice - Generate ancestry-specific weights # AFR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base # EUR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base","title":"Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice"},{"location":"old/more/Day4b.docx/#part-5-step-5-evaluate-the-admixture-informed-prs","text":"library(dplyr) # Read the PRS files into dataframes file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F) # Add the fourth column of both files file1$PRS_SUM <- file1$PRS + file2$PRS # Load the phenotype data pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\") # Convert phenotype column to numeric pheno$phenotype <- as.numeric(pheno$phenotype) # Merge PRS data with phenotype data merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\")) # Rename columns names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\" # Perform linear regression for each PRS and the combined PRS model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data) # Extract R-squared values r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared # Print the R-squared values cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\")","title":"Part 5: Step 5 - Evaluate the Admixture-informed PRS"},{"location":"old/more/Day4b.docx/#questions_5","text":"","title":"Questions"},{"location":"old/more/Day4b.docx/#i-how-does-the-r-squared-of-the-combined-ancestry-prs-perform-relative-to-the-2-partial-genome-prss","text":"","title":"(i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?"},{"location":"old/more/Day5_Projects/","text":"Integration of Polygenic Risk Scores Task \u2013 PRSmix calculation [3.5 hrs] You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024) Group project data files Group 1 https://github.com/tinashedoc/cvx/blob/main/monodta.txt Group 2 https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt Group 3 https://github.com/tinashedoc/cvx/blob/main/pltdta.txt Group 4 https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Integration of Polygenic Risk Scores"},{"location":"old/more/Day5_Projects/#integration-of-polygenic-risk-scores","text":"","title":"Integration of Polygenic Risk Scores"},{"location":"old/more/Day5_Projects/#task-prsmix-calculation-35-hrs","text":"You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024)","title":"Task \u2013 PRSmix calculation [3.5 hrs]"},{"location":"old/more/Day5_Projects/#group-project-data-files","text":"","title":"Group project data files"},{"location":"old/more/Day5_Projects/#group-1","text":"https://github.com/tinashedoc/cvx/blob/main/monodta.txt","title":"Group 1"},{"location":"old/more/Day5_Projects/#group-2","text":"https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt","title":"Group 2"},{"location":"old/more/Day5_Projects/#group-3","text":"https://github.com/tinashedoc/cvx/blob/main/pltdta.txt","title":"Group 3"},{"location":"old/more/Day5_Projects/#group-4","text":"https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Group 4"},{"location":"old/more/about_future/","text":"Future Work Coming Soon","title":"About future"},{"location":"old/more/about_future/#future-work","text":"Coming Soon","title":"Future Work"},{"location":"old/more/BPRS_DOCS/","text":"BridgePRS is a Bayesian method that utilises ridge regression developed to tackle the \" PRS Portability Problem\". The PRS portability problem causes lower PRS accuracy in target populations not included in the GWAS base used to estimate the PRS. This is due to differences in linkage disequilibrium (LD), allele frequency and gene\u2013environment interactions affecting causal effect sizes between the base and target populations. Download Links BridgePRS Packages Reference Panels OS Link Last Update Linux 64-Bit v1.0.3 7-12-2024 Mac 64-Bit v1.0.3 6-16-2024 Windows NA Not Available Download Link Size More Information HapMap Variants <1GB International HapMap Project 1000 Genomes Variants: MAF>5% 8GB International Genome Sample Resource 1000 Genomes variants: MAF>1% 14GB International Genome Sample Resource Latest Updates 2023-09-15 (v0.1.7) We have added sample thousand genomes data. update log can be found here Overview BridgePRS is written in R with a Python wrapper. Plink. is used in the first stage of the modelling for clumping and thresholding (all markers within clumps are retained for analysis). For more information on installing dependencies, please refer to Requirements . To get BridgePRS running using toy data see our Quick Start Tutorial. . Following the Quick Start, the full guide provides more realistic examples to help you get started with your own data. Citation: Our Manuscript is published in Nature Genetics Please cite our paper : Hoggart C, Choi SW, Garc\u00eda-Gonz\u00e1lez J, Souaiaia T, Preuss M, O'Reilly P. BridgePRS leverages shared genetic effects across ancestries to increase polygenic risk score portability. Nat Genet 56, 180\u2013186 (2024). https://doi.org/10.1038/s41588-023-01583-9 Contact For questions about the methodology, this website, or our manuscript please contact Dr Clive Hoggart , Dr Tade Souaiaia , or Dr Paul O'Reilly . For source code and coding issues please visit the bridgePRS github here . Acknowledgements We would like to thank Brian Fulton-Howard for his feedback and help with testing.","title":"Index"},{"location":"old/more/BPRS_DOCS/#download-links","text":"BridgePRS Packages Reference Panels OS Link Last Update Linux 64-Bit v1.0.3 7-12-2024 Mac 64-Bit v1.0.3 6-16-2024 Windows NA Not Available Download Link Size More Information HapMap Variants <1GB International HapMap Project 1000 Genomes Variants: MAF>5% 8GB International Genome Sample Resource 1000 Genomes variants: MAF>1% 14GB International Genome Sample Resource Latest Updates","title":"Download Links"},{"location":"old/more/BPRS_DOCS/#2023-09-15-v017","text":"We have added sample thousand genomes data. update log can be found here","title":"2023-09-15 (v0.1.7)"},{"location":"old/more/BPRS_DOCS/#overview","text":"BridgePRS is written in R with a Python wrapper. Plink. is used in the first stage of the modelling for clumping and thresholding (all markers within clumps are retained for analysis). For more information on installing dependencies, please refer to Requirements . To get BridgePRS running using toy data see our Quick Start Tutorial. . Following the Quick Start, the full guide provides more realistic examples to help you get started with your own data. Citation: Our Manuscript is published in Nature Genetics Please cite our paper : Hoggart C, Choi SW, Garc\u00eda-Gonz\u00e1lez J, Souaiaia T, Preuss M, O'Reilly P. BridgePRS leverages shared genetic effects across ancestries to increase polygenic risk score portability. Nat Genet 56, 180\u2013186 (2024). https://doi.org/10.1038/s41588-023-01583-9","title":"Overview"},{"location":"old/more/BPRS_DOCS/#contact","text":"For questions about the methodology, this website, or our manuscript please contact Dr Clive Hoggart , Dr Tade Souaiaia , or Dr Paul O'Reilly . For source code and coding issues please visit the bridgePRS github here .","title":"Contact"},{"location":"old/more/BPRS_DOCS/#acknowledgements","text":"We would like to thank Brian Fulton-Howard for his feedback and help with testing.","title":"Acknowledgements"},{"location":"old/more/BPRS_DOCS/guide_args/","text":"This page contains all command available in BridgePRS. Tips When constructing new parameters, we follow the following rule: if the command has effect on any file that is not the target, it will have a prefix of the file name. For example, --base-info applies INFO score filtering on the base file, --ld-info perform INFO score filtering on the LD reference file and --info applies the INFO score filtering on the target file. Input Population Files/Arguments --sumstats_prefix Path to sumstats data --sumstats_suffix Sumstats Suffix --phenotype_files phenotype test and validation data --pop_config(s) population configuration file(s) --pop or --pops population names (AFR, EUR, etc) --snp_file List of QCed SNP ids System Level Toggles/Arguments --cores By default bridgePRS is parralelized across (n-1) cores -o Output folder for BridgePRS --platform Force platform (Linux or MacOS) --verbose Toggle Verbose Mode On --noPlots Skip Post-Pipeline Analysis (Plotting) --restart Repeat previously completed steps Parameter Arguments --fst fst value --max_clump_size Max Size for Clumping --thinned_snplist Thinned snp list for large clumps Internal File Arguments --model_file model result generated from the build-model subprogram --clump_prefix prefix for files generated by clump step --beta_prefix prefix for files generated by beta step --predict_prefix prefix for files generated by predict step --result_file prs result files File Column Names Phenotype Files --phenotype Phenotype File Field: phenotype --covariates Phenotype File Field: comma separated list of covariates Sumstats Files --ssf-snpid Sumstats field: snp ID --ssf-ref Sumstats Field: reference allele --ssf-alt Sumstats Field: alt allele --ssf-maf Sumstats Field: MAF --ssf-beta Sumstats Field: linear regression effect or log odds --ssf-se Sumstats field: standard error --ssf-p Sumstats field: P-value --ssf-n Sumstats Field: Sample Size","title":"Guide args"},{"location":"old/more/BPRS_DOCS/guide_args/#input-population-filesarguments","text":"--sumstats_prefix Path to sumstats data --sumstats_suffix Sumstats Suffix --phenotype_files phenotype test and validation data --pop_config(s) population configuration file(s) --pop or --pops population names (AFR, EUR, etc) --snp_file List of QCed SNP ids","title":"Input Population Files/Arguments"},{"location":"old/more/BPRS_DOCS/guide_args/#system-level-togglesarguments","text":"--cores By default bridgePRS is parralelized across (n-1) cores -o Output folder for BridgePRS --platform Force platform (Linux or MacOS) --verbose Toggle Verbose Mode On --noPlots Skip Post-Pipeline Analysis (Plotting) --restart Repeat previously completed steps","title":"System Level Toggles/Arguments"},{"location":"old/more/BPRS_DOCS/guide_args/#parameter-arguments","text":"--fst fst value --max_clump_size Max Size for Clumping --thinned_snplist Thinned snp list for large clumps","title":"Parameter Arguments"},{"location":"old/more/BPRS_DOCS/guide_args/#internal-file-arguments","text":"--model_file model result generated from the build-model subprogram --clump_prefix prefix for files generated by clump step --beta_prefix prefix for files generated by beta step --predict_prefix prefix for files generated by predict step --result_file prs result files","title":"Internal File Arguments"},{"location":"old/more/BPRS_DOCS/guide_args/#file-column-names","text":"","title":"File Column Names"},{"location":"old/more/BPRS_DOCS/guide_args/#phenotype-files","text":"--phenotype Phenotype File Field: phenotype --covariates Phenotype File Field: comma separated list of covariates","title":"Phenotype Files"},{"location":"old/more/BPRS_DOCS/guide_args/#sumstats-files","text":"--ssf-snpid Sumstats field: snp ID --ssf-ref Sumstats Field: reference allele --ssf-alt Sumstats Field: alt allele --ssf-maf Sumstats Field: MAF --ssf-beta Sumstats Field: linear regression effect or log odds --ssf-se Sumstats field: standard error --ssf-p Sumstats field: P-value --ssf-n Sumstats Field: Sample Size","title":"Sumstats Files"},{"location":"old/more/BPRS_DOCS/guide_background/","text":"BridgePRS is a trans-ancestry PRS software which improves polygenic risk score analysis in under-represented target populations by combining summary statistics from under-represented target populations (e.g. Africans) with those from a powerful GWAS (e.g. Europeans). Background PRS require as input summary statistics from genome wide association studies. If you are unfamiliar with GWAS or need a refresher consider reading this paper . Genome-wide association studies (GWAS) involve analyzing the genomes of a large group of individuals. This involves testing millions of genetic variants (SNP) for association with a particular trait. For a binary trait (like blue or brown eyes) this involves comparing the frequency of genetic variations in each group to produce an odds-ratio (measure of association) and a p-value to measure the signficance of the realtionship at each SNP. For continuous measures (like height) this results in an effect size (measure of continuous association) and p-value that measure the significance of our relationship. GWAS results can be summarized in a sumstats file which looks like this: CHR ID REF A1 A1_FREQ OBS_CT SE BETA P 1 rs100 A T 0.1 100 0.01 1.10 0.01 2 rs200 C G 0.2 100 0.02 -1.10 0.05 3 rs300 G A 0.3 100 0.03 1.02 0.10 PRS basics Creating individual polygenic scores involves combine information across many genetic variants. A score is calculated by summing the genetic association values (beta-weights) for each allele in an individual, therefore, variants with stronger associations have a larger impact on the overall score. PRS software use different statistical methods to select variants and estimate their effect sizes beta for use in the PRS. BridgePRS, like many other PRS methods, require genotype and phenotype data from individuals (several hundred samples) in addition to GWAS summary statistics to estimate the PRS. The predictive power of PRS is assessed in genotype and phenotype data from unseen samples. BridgePRS reports PRS accuracy by the residual variance explained \\(R^2\\) (accounting for the variance explained by the non-genetic covariates included in the model). For binary traits Nagelkerke \\(R^2\\) is used. Cross Population Analysis The PRS Portability Problem Often when a PRS trained using data from one population, e.g. European, is applied to a population of different ancestry, e.g. African, the PRS model is less predictive. Often there is insufficient data in the non-European populations to estimate good ancestry specific PRS using this data alone. Using single ancestry PRS methods researchers are left with two choices: An underpowered PRS model estimated using only data the target non-European population. A PRS estimated using a well powered GWAS from another population The BridgePRS Solution BridgePRS solves this problem by combining GWAS summary statistics from two populations, typically a large well powered GWAS from a \"base\" population, e.g. European, and a smaller GWAS from the target population, eg African. This is implemented by estimating a PRS in the target population sing a prior effect-size distribution estimated from the base GWAS summary statistics. To capture ancestry specific effects in the target population a BridgePRS also estimates a PRS using only the target population. BridgePRS combines these models to produce a single weighted PRS solution.","title":"Guide background"},{"location":"old/more/BPRS_DOCS/guide_background/#background","text":"PRS require as input summary statistics from genome wide association studies. If you are unfamiliar with GWAS or need a refresher consider reading this paper . Genome-wide association studies (GWAS) involve analyzing the genomes of a large group of individuals. This involves testing millions of genetic variants (SNP) for association with a particular trait. For a binary trait (like blue or brown eyes) this involves comparing the frequency of genetic variations in each group to produce an odds-ratio (measure of association) and a p-value to measure the signficance of the realtionship at each SNP. For continuous measures (like height) this results in an effect size (measure of continuous association) and p-value that measure the significance of our relationship. GWAS results can be summarized in a sumstats file which looks like this: CHR ID REF A1 A1_FREQ OBS_CT SE BETA P 1 rs100 A T 0.1 100 0.01 1.10 0.01 2 rs200 C G 0.2 100 0.02 -1.10 0.05 3 rs300 G A 0.3 100 0.03 1.02 0.10 PRS basics Creating individual polygenic scores involves combine information across many genetic variants. A score is calculated by summing the genetic association values (beta-weights) for each allele in an individual, therefore, variants with stronger associations have a larger impact on the overall score. PRS software use different statistical methods to select variants and estimate their effect sizes beta for use in the PRS. BridgePRS, like many other PRS methods, require genotype and phenotype data from individuals (several hundred samples) in addition to GWAS summary statistics to estimate the PRS. The predictive power of PRS is assessed in genotype and phenotype data from unseen samples. BridgePRS reports PRS accuracy by the residual variance explained \\(R^2\\) (accounting for the variance explained by the non-genetic covariates included in the model). For binary traits Nagelkerke \\(R^2\\) is used.","title":"Background"},{"location":"old/more/BPRS_DOCS/guide_background/#cross-population-analysis","text":"The PRS Portability Problem Often when a PRS trained using data from one population, e.g. European, is applied to a population of different ancestry, e.g. African, the PRS model is less predictive. Often there is insufficient data in the non-European populations to estimate good ancestry specific PRS using this data alone. Using single ancestry PRS methods researchers are left with two choices: An underpowered PRS model estimated using only data the target non-European population. A PRS estimated using a well powered GWAS from another population The BridgePRS Solution BridgePRS solves this problem by combining GWAS summary statistics from two populations, typically a large well powered GWAS from a \"base\" population, e.g. European, and a smaller GWAS from the target population, eg African. This is implemented by estimating a PRS in the target population sing a prior effect-size distribution estimated from the base GWAS summary statistics. To capture ancestry specific effects in the target population a BridgePRS also estimates a PRS using only the target population. BridgePRS combines these models to produce a single weighted PRS solution.","title":"Cross Population Analysis"},{"location":"old/more/BPRS_DOCS/guide_customization/","text":"Input Data LD Reference Data: Custom Data BridgePRS estimates SNP-specific LD in the clumping step from multi-population target data in PLINK binary format. To pass user supplied LD reference data, for a custom population named POP , please create folder (named pop_ld ) which includes: A .bed , .bim , and .fam file for each chromosome with format \"chr\"[1-22]\".ext\" POP.ids.txt , a txt file that lists all the sample IDs for your population Then use the command line argument --ld_path to point to the location of this folder on your computer.","title":"Guide customization"},{"location":"old/more/BPRS_DOCS/guide_customization/#input-data","text":"","title":"Input Data"},{"location":"old/more/BPRS_DOCS/guide_customization/#ld-reference-data-custom-data","text":"BridgePRS estimates SNP-specific LD in the clumping step from multi-population target data in PLINK binary format. To pass user supplied LD reference data, for a custom population named POP , please create folder (named pop_ld ) which includes: A .bed , .bim , and .fam file for each chromosome with format \"chr\"[1-22]\".ext\" POP.ids.txt , a txt file that lists all the sample IDs for your population Then use the command line argument --ld_path to point to the location of this folder on your computer.","title":"LD Reference Data: Custom Data"},{"location":"old/more/BPRS_DOCS/guide_input/","text":"Input Data LD Reference Panel (--ld_path) BridgePRS requires representative genotype data in PLINK binary format to estimate LD. A miniature LD reference panel ( data/1000G_sample ), suitable for the quick start tutorial is included in the BridgePRS download. Three full-size 1000 Genomes panels covering AFR , EUR , EAS , SAS , and AMR super-populations with different SNP sets are available for download: (1) All HapMap Variants , (2) All 1000G variants with MAF>5% in any of the five 1000G super-populations , (3) All 1000G variants with MAF>1% in any of the five 1000G super-populations . If you wish to run bridgePRS using a custom LD reference panel please see customization . Target/Base Population Data: For the target and base populations BridgePRS requires that the following inputs be supplied on the command line or in a configuration file. files/names are supplied on the command line or in a configuation file: Name Command Line flag(s) Description Pop Name --pop Population Name (Required) LD Pop --ldpop LD Reference Population (Required if different from above) LD Panel --ld_path Path to LD Reference Panel (a) Sumstat File --sumstats_file GWAS Summary Stats (Text Format) (b) Sumstat Filess --sumstats_prefix GWAS Summary Stats Multiple File(s) Genotype Files --genotype_prefix Individual Level Genotype Files (Plink Format) Phenotype File --phenotype_file Individual Level Phenotypes (Text File) Validation File --validation_file Individual Level Phenotypes for Validation QC-snp List --snp_file List QCed SNP ids Creating a Configuration File The following command will validate the command line data and create a target configuration file ./bridgePRS check pop -o out --pop AFR --sumstats_prefix data/pop_africa/sumstats/afr.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_test.dat To create a target and base configuation file you can use the following command: ./bridgePRS check pops -o out --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat This command will create target and base configuration files that can be observed below: cat out/save/target.AFR.config POP = AFR LDPOP = AFR SUMSTATS_PREFIX =$ BRIDGEDIR / data / pop_africa / sumstats / afr.chr SUMSTATS_SUFFIX = .glm.linear.gz SNP_FILE =$ BRIDGEDIR / out / save / snps.AFR.txt GENOTYPE_PREFIX =$ BRIDGEDIR / data / pop_africa / genotypes / afr_genotypes PHENOTYPE_FILE =$ BRIDGEDIR / out / save / AFR.test_phenos.dat VALIDATION_FILE =$ BRIDGEDIR / out / save / AFR.valid_phenos.dat cat out/save/base.EUR.config POP = EUR LDPOP = EUR SUMSTATS_PREFIX =$ BRIDGEDIR / data / pop_europe / sumstats / eur.chr SUMSTATS_SUFFIX = .glm.linear.gz SNP_FILE = out / save / snps.EUR.txt GENOTYPE_PREFIX =$ BRIDGEDIR / data / pop_africa / genotypes / afr_genotypes PHENOTYPE_FILE =$ BRIDGEDIR / data / pop_africa / phenotypes / afr_pheno.dat File Specifications 1) Sumstats Data GWAS summary statistics are provided using a prefix to one or many (per chromosome) files with the --sumstats_prefix argument and the --sumstats_suffix argument when nevecessary. GWAS summary statistics must be provided as a whitespace delimited file containing the results of an association study for a given phenotype. BridgePRS has no problem reading in a gzipped base file (need to have a .gz suffix) or splitting the file by chromosome if necessary. An example of a sumstats file with default column headers is shown: Default Headers #CHR ID REF A1 A1_FREQ OBS_CT BETA SE T_STAT P Argument --ssf-snpid --ssf-ref --ssf-alt --ssf-maf --ssf-n --ssf-beta --ssf-se --ssf-p Data 1 rs121 T G 0.0257573 4853 0.820864 0.413692 1.98424 0.0472871 Data 1 rs497 C A 0.483495 4847 0.0011142 0.128347 0.00868116 0.993074 Data 1 rs271 G G 0.424387 4814 0.108094 0.132225 0.817497 0.413687 The --ssf arguments can be used to specify column headers for different files. 2) Genotype Files Genotype files must be in Plink Format. 3) Phenotype Files Phenotype files can be provided to BridgePRS using the --phenotype_files flag. This must be a tab / space delimited file and missing data must be represented by either NA or -9 (only for binary traits). The first two column of the phenotype file should be the FID and the IID, and the rest can be phenotypes/covariates: FID IID y y.binary PC1 PC2 afr1_1 afr2_1 24.4 1 0.53 0.950 afr1_2 afr2_2 4.10 0 0.59 0.450 afr1_3 afr2_3 37.2 1 0.73 -0.13 afr1_4 afr2_4 5.40 0 0.44 -0.55 The phenotype of interest can be specified with the --phenotype flag and the covariates can be given as a comma separated list after the --covariates flag: ``` ./bridgePRS check data -o out --pop AFR --phenotype y --covariates PC1,PC2 --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat ``` Warning The column name(s) should not contain space nor comma 4) QC SNP List To select only SNPS that have passed QC, you can include a single column text file using the --snp_file flag.","title":"Guide input"},{"location":"old/more/BPRS_DOCS/guide_input/#input-data","text":"","title":"Input Data"},{"location":"old/more/BPRS_DOCS/guide_input/#ld-reference-panel-ld_path","text":"BridgePRS requires representative genotype data in PLINK binary format to estimate LD. A miniature LD reference panel ( data/1000G_sample ), suitable for the quick start tutorial is included in the BridgePRS download. Three full-size 1000 Genomes panels covering AFR , EUR , EAS , SAS , and AMR super-populations with different SNP sets are available for download: (1) All HapMap Variants , (2) All 1000G variants with MAF>5% in any of the five 1000G super-populations , (3) All 1000G variants with MAF>1% in any of the five 1000G super-populations . If you wish to run bridgePRS using a custom LD reference panel please see customization .","title":"LD Reference Panel (--ld_path)"},{"location":"old/more/BPRS_DOCS/guide_input/#targetbase-population-data","text":"For the target and base populations BridgePRS requires that the following inputs be supplied on the command line or in a configuration file. files/names are supplied on the command line or in a configuation file: Name Command Line flag(s) Description Pop Name --pop Population Name (Required) LD Pop --ldpop LD Reference Population (Required if different from above) LD Panel --ld_path Path to LD Reference Panel (a) Sumstat File --sumstats_file GWAS Summary Stats (Text Format) (b) Sumstat Filess --sumstats_prefix GWAS Summary Stats Multiple File(s) Genotype Files --genotype_prefix Individual Level Genotype Files (Plink Format) Phenotype File --phenotype_file Individual Level Phenotypes (Text File) Validation File --validation_file Individual Level Phenotypes for Validation QC-snp List --snp_file List QCed SNP ids Creating a Configuration File The following command will validate the command line data and create a target configuration file ./bridgePRS check pop -o out --pop AFR --sumstats_prefix data/pop_africa/sumstats/afr.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_test.dat To create a target and base configuation file you can use the following command: ./bridgePRS check pops -o out --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat This command will create target and base configuration files that can be observed below: cat out/save/target.AFR.config POP = AFR LDPOP = AFR SUMSTATS_PREFIX =$ BRIDGEDIR / data / pop_africa / sumstats / afr.chr SUMSTATS_SUFFIX = .glm.linear.gz SNP_FILE =$ BRIDGEDIR / out / save / snps.AFR.txt GENOTYPE_PREFIX =$ BRIDGEDIR / data / pop_africa / genotypes / afr_genotypes PHENOTYPE_FILE =$ BRIDGEDIR / out / save / AFR.test_phenos.dat VALIDATION_FILE =$ BRIDGEDIR / out / save / AFR.valid_phenos.dat cat out/save/base.EUR.config POP = EUR LDPOP = EUR SUMSTATS_PREFIX =$ BRIDGEDIR / data / pop_europe / sumstats / eur.chr SUMSTATS_SUFFIX = .glm.linear.gz SNP_FILE = out / save / snps.EUR.txt GENOTYPE_PREFIX =$ BRIDGEDIR / data / pop_africa / genotypes / afr_genotypes PHENOTYPE_FILE =$ BRIDGEDIR / data / pop_africa / phenotypes / afr_pheno.dat","title":"Target/Base Population Data:"},{"location":"old/more/BPRS_DOCS/guide_input/#file-specifications","text":"","title":"File Specifications"},{"location":"old/more/BPRS_DOCS/guide_input/#1-sumstats-data","text":"GWAS summary statistics are provided using a prefix to one or many (per chromosome) files with the --sumstats_prefix argument and the --sumstats_suffix argument when nevecessary. GWAS summary statistics must be provided as a whitespace delimited file containing the results of an association study for a given phenotype. BridgePRS has no problem reading in a gzipped base file (need to have a .gz suffix) or splitting the file by chromosome if necessary. An example of a sumstats file with default column headers is shown: Default Headers #CHR ID REF A1 A1_FREQ OBS_CT BETA SE T_STAT P Argument --ssf-snpid --ssf-ref --ssf-alt --ssf-maf --ssf-n --ssf-beta --ssf-se --ssf-p Data 1 rs121 T G 0.0257573 4853 0.820864 0.413692 1.98424 0.0472871 Data 1 rs497 C A 0.483495 4847 0.0011142 0.128347 0.00868116 0.993074 Data 1 rs271 G G 0.424387 4814 0.108094 0.132225 0.817497 0.413687 The --ssf arguments can be used to specify column headers for different files.","title":"1) Sumstats Data"},{"location":"old/more/BPRS_DOCS/guide_input/#2-genotype-files","text":"Genotype files must be in Plink Format.","title":"2) Genotype Files"},{"location":"old/more/BPRS_DOCS/guide_input/#3-phenotype-files","text":"Phenotype files can be provided to BridgePRS using the --phenotype_files flag. This must be a tab / space delimited file and missing data must be represented by either NA or -9 (only for binary traits). The first two column of the phenotype file should be the FID and the IID, and the rest can be phenotypes/covariates: FID IID y y.binary PC1 PC2 afr1_1 afr2_1 24.4 1 0.53 0.950 afr1_2 afr2_2 4.10 0 0.59 0.450 afr1_3 afr2_3 37.2 1 0.73 -0.13 afr1_4 afr2_4 5.40 0 0.44 -0.55 The phenotype of interest can be specified with the --phenotype flag and the covariates can be given as a comma separated list after the --covariates flag: ``` ./bridgePRS check data -o out --pop AFR --phenotype y --covariates PC1,PC2 --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat ``` Warning The column name(s) should not contain space nor comma","title":"3) Phenotype Files"},{"location":"old/more/BPRS_DOCS/guide_input/#4-qc-snp-list","text":"To select only SNPS that have passed QC, you can include a single column text file using the --snp_file flag.","title":"4) QC SNP List"},{"location":"old/more/BPRS_DOCS/guide_output/","text":"Output Data For every PRS invocation of BridgePRS (single, port, and prior) BridgePRS produces the following output files. Weighted combined versions of each files of also produced by BridgePRS. Name file extension Brief Description SNP Weights .snp_weights.dat Snp weights PRS Values .preds.dat Predicted Phenotypes Summary Result .var_explained.txt Performance Summary Progress Plot bridgePRS-{subProgram}.png Performance Summary","title":"Guide output"},{"location":"old/more/BPRS_DOCS/guide_output/#output-data","text":"For every PRS invocation of BridgePRS (single, port, and prior) BridgePRS produces the following output files. Weighted combined versions of each files of also produced by BridgePRS. Name file extension Brief Description SNP Weights .snp_weights.dat Snp weights PRS Values .preds.dat Predicted Phenotypes Summary Result .var_explained.txt Performance Summary Progress Plot bridgePRS-{subProgram}.png Performance Summary","title":"Output Data"},{"location":"old/more/BPRS_DOCS/guide_pipeline/","text":"Main Program A pipeline implementin the BridgePRS method desribed in our Nature Genetics paper can be run with the following command: BridgePRS easyrun go The pipeline requires the following input from both the target and base populations: --pop: The population name --ldpop: The ld reference population, if different from population name --sumstats_prefix: Sumstats data --genotype_prefix: Target Genotype Data --phenotype_file: Target Phenotype File This information can be provided on the command line or using config files. The pipeline runs multiple subprograms that are described below. Modelling overview Figure 1 of our Nature Genetics paper provides an overview of the modelling implemented by BridgePRS. The BridgePRS pipeline consists of five related subprograms which correspond to Stage 1 and Stage 2 analyses described in Figure 1. Stage 1 analysis is a single population analysis. Stage 2 uses the output from a Stage 1 analysis of the base population as a prior for the target population, this analysis produces model M1 in Figure 1. Stage 1 analysis of the target population produces model M2. Model M3 combines PRS from both Stage 1 and Stage 2 analyses. The final BridgePRS model is a weighted sum of models M1, M2 and M3. Model M1 reflects the belief that the target population GWAS is only informative in conjugtion with the base population GWAS. Model M2 reflects the belief that the target population GWAS is informative and the base population GWAS gives no addition information. Model M3 reflects the belief both the base and target population GWAS contribute independent information. Since apriori we do not know which of the three scenarios corresponding to models M1, M2 and M3 are true, BridgePRS weights these models to produce a single target population PRS. However, BridgePRS reports SNP weights and R2 in the target population for both the weighted model and M1, M2 and M3 enabling the user to use any of the four models. Subprograms 1) BridgePRS prs-single -- Stage 1 analysis to estimate models M2 and M3 2) BridgePRS build-model -- Stage 1 analysis for input into Stage 2 3) BridgePRS prs-prior -- Stage 2 analysis to estimate models M1 and M3 4) BridgePRS analyse combine -- Estimates models M1, M2, M3 and weighted model 5) BridgePRS prs-port -- Estimates target PRS without target GWAS prs-single -- Stage 1 analysis in the NG paper prs-single performs single population analysis. It is used by the main pipeline to capture target population specific effects. prs-single can be run on its own with the command ./bridgePRS prs-single run with the following arguments on the command line or configuration file: --pop: The name of your target population --ldpop: The ld reference name, if different from target population name --sumstats_prefix: Sumstats data --genotype_prefix: Target Genotype Data --phenotype_file: Target Phenotype File prs-port prs-port estimates a target population PRS without GWAS summary statistics from the target population by optimising base population PRS using individual level data from the target population. prs-port is run with the command ./bridgePRS prs-port run with the following arguments on the command line or configuration file:","title":"Guide pipeline"},{"location":"old/more/BPRS_DOCS/guide_pipeline/#main-program","text":"A pipeline implementin the BridgePRS method desribed in our Nature Genetics paper can be run with the following command: BridgePRS easyrun go The pipeline requires the following input from both the target and base populations: --pop: The population name --ldpop: The ld reference population, if different from population name --sumstats_prefix: Sumstats data --genotype_prefix: Target Genotype Data --phenotype_file: Target Phenotype File This information can be provided on the command line or using config files. The pipeline runs multiple subprograms that are described below.","title":"Main Program"},{"location":"old/more/BPRS_DOCS/guide_pipeline/#modelling-overview","text":"Figure 1 of our Nature Genetics paper provides an overview of the modelling implemented by BridgePRS. The BridgePRS pipeline consists of five related subprograms which correspond to Stage 1 and Stage 2 analyses described in Figure 1. Stage 1 analysis is a single population analysis. Stage 2 uses the output from a Stage 1 analysis of the base population as a prior for the target population, this analysis produces model M1 in Figure 1. Stage 1 analysis of the target population produces model M2. Model M3 combines PRS from both Stage 1 and Stage 2 analyses. The final BridgePRS model is a weighted sum of models M1, M2 and M3. Model M1 reflects the belief that the target population GWAS is only informative in conjugtion with the base population GWAS. Model M2 reflects the belief that the target population GWAS is informative and the base population GWAS gives no addition information. Model M3 reflects the belief both the base and target population GWAS contribute independent information. Since apriori we do not know which of the three scenarios corresponding to models M1, M2 and M3 are true, BridgePRS weights these models to produce a single target population PRS. However, BridgePRS reports SNP weights and R2 in the target population for both the weighted model and M1, M2 and M3 enabling the user to use any of the four models.","title":"Modelling overview"},{"location":"old/more/BPRS_DOCS/guide_pipeline/#subprograms","text":"1) BridgePRS prs-single -- Stage 1 analysis to estimate models M2 and M3 2) BridgePRS build-model -- Stage 1 analysis for input into Stage 2 3) BridgePRS prs-prior -- Stage 2 analysis to estimate models M1 and M3 4) BridgePRS analyse combine -- Estimates models M1, M2, M3 and weighted model 5) BridgePRS prs-port -- Estimates target PRS without target GWAS","title":"Subprograms"},{"location":"old/more/BPRS_DOCS/guide_pipeline/#prs-single-stage-1-analysis-in-the-ng-paper","text":"prs-single performs single population analysis. It is used by the main pipeline to capture target population specific effects. prs-single can be run on its own with the command ./bridgePRS prs-single run with the following arguments on the command line or configuration file: --pop: The name of your target population --ldpop: The ld reference name, if different from target population name --sumstats_prefix: Sumstats data --genotype_prefix: Target Genotype Data --phenotype_file: Target Phenotype File","title":"prs-single -- Stage 1 analysis in the NG paper"},{"location":"old/more/BPRS_DOCS/guide_pipeline/#prs-port","text":"prs-port estimates a target population PRS without GWAS summary statistics from the target population by optimising base population PRS using individual level data from the target population. prs-port is run with the command ./bridgePRS prs-port run with the following arguments on the command line or configuration file:","title":"prs-port"},{"location":"old/more/BPRS_DOCS/guide_shell/","text":"Using BridgePRS without Python See","title":"Guide shell"},{"location":"old/more/BPRS_DOCS/guide_shell/#using-bridgeprs-without-python","text":"See","title":"Using BridgePRS without Python"},{"location":"old/more/BPRS_DOCS/guide_usecases/","text":"Training In the two toy-data examples the target population was African ( AFR ) and the base population was European ( EUR ). In these examples each population had a corresponding: LD Reference Panel GWAS Summary Stat File Individual levels genotype data in plink format Phenotype data Here we consider using BridgePRS in other scenarios with less available data. In each of these challeges the task is to create valid configuration files given the described scenario. Challenge 1: I have access to genotype/phenotype data (<1000 samples, continuous phenotype) from a diverse population in Central Africa (1,2). I would like ot run PRS as accurately as possible, but my dataset is not large enough to conduct GWAS, however, I do have access to a moderately sized GWAS in a related West African population (3) and access to large-scale GWAS data from a European population (4). I also have access to the 1000G reference panels, how can I analyze my data? Available Data: caf/genotypes/caf_genotype.bed, caf_genotype.bim, caf_genotype.fam caf/phenotypes/caf_test.dat, ukb/phenotypes/caf_validation.dat yoruba/sumstats/yoruba.chr1.glm.gz,...,yoruba.chr22.glm.gz ukb/sumstats/ukb.sumstats.gz 1000G Reference Panel Can you fill out the configuration files to carry this out? Target Config File POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILE = VALIDATION_FILE = [Answer] POP = CAF LDPOP = AFR SUMSTATS_PREFIX = yoruba / sumstats / yoruba.chr SUMSTATS_SUFFIX = .glm.gz GENOTYPE_PREFIX = caf / genotypes / caf_genotype PHENOTYPE_FILE = caf / phenotypes / caf_test.dat VALIDATION_FILE = caf / phenotypes / caf_validation.dat Model Config FIle POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILES = Answer POP = EUR LDPOP = EUR SUMSTATS_PREFIX = ukb / sumstats / ukb.sumstats GENOTYPE_PREFIX = caf / genotypes / caf_genotype PHENOTYPE_FILE = caf / phenotypes / caf_test.dat VALIDATION_FILE = caf / phenotypes / caf_validation.dat Challenge 2: I have access to GWAS data (1) and genotype/phenotype (2,3) data (>2000 samples, binary phenotype) from a moderate sized population in Eastern Europe. I have reason to believe that this population has unique LD structure and would like to incorporate this into my model. I also have GWAS (4) and genotype/phenotype data from the UKB biobank (5,6) that I wish to use to improve my results, how should I do this? ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat Target Config File POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILE = VALIDATION_FILE = [Answer] POP = ukr LDPOP = ukr * SUMSTATS_PREFIX = ukr / sumstats / ukr.sumstats GENOTYPE_PREFIX = ukr / phenotypes / chr PHENOTYPE_FILE = ukr / phenotypes / ukr_test.dat VALIDATION_FILE = ukr / phenotypes / ukr_validation.dat Model Config FIle POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILE = VALIDATION_FILE = Answer POP = UKB LDPOP = EUR SUMSTATS_PREFIX = ukb / sumstats / ukb.sumstats GENOTYPE_PREFIX = ukb / genotypes / chr PHENOTYPE_FILE = ukb / phenotypes / ukb_test.dat VALIDATION_FILE = ukb / phenotypes / ukb_validation.dat","title":"Guide usecases"},{"location":"old/more/BPRS_DOCS/guide_usecases/#training","text":"In the two toy-data examples the target population was African ( AFR ) and the base population was European ( EUR ). In these examples each population had a corresponding: LD Reference Panel GWAS Summary Stat File Individual levels genotype data in plink format Phenotype data Here we consider using BridgePRS in other scenarios with less available data. In each of these challeges the task is to create valid configuration files given the described scenario.","title":"Training"},{"location":"old/more/BPRS_DOCS/guide_usecases/#challenge-1","text":"I have access to genotype/phenotype data (<1000 samples, continuous phenotype) from a diverse population in Central Africa (1,2). I would like ot run PRS as accurately as possible, but my dataset is not large enough to conduct GWAS, however, I do have access to a moderately sized GWAS in a related West African population (3) and access to large-scale GWAS data from a European population (4). I also have access to the 1000G reference panels, how can I analyze my data? Available Data: caf/genotypes/caf_genotype.bed, caf_genotype.bim, caf_genotype.fam caf/phenotypes/caf_test.dat, ukb/phenotypes/caf_validation.dat yoruba/sumstats/yoruba.chr1.glm.gz,...,yoruba.chr22.glm.gz ukb/sumstats/ukb.sumstats.gz 1000G Reference Panel Can you fill out the configuration files to carry this out? Target Config File POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILE = VALIDATION_FILE = [Answer] POP = CAF LDPOP = AFR SUMSTATS_PREFIX = yoruba / sumstats / yoruba.chr SUMSTATS_SUFFIX = .glm.gz GENOTYPE_PREFIX = caf / genotypes / caf_genotype PHENOTYPE_FILE = caf / phenotypes / caf_test.dat VALIDATION_FILE = caf / phenotypes / caf_validation.dat Model Config FIle POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILES = Answer POP = EUR LDPOP = EUR SUMSTATS_PREFIX = ukb / sumstats / ukb.sumstats GENOTYPE_PREFIX = caf / genotypes / caf_genotype PHENOTYPE_FILE = caf / phenotypes / caf_test.dat VALIDATION_FILE = caf / phenotypes / caf_validation.dat","title":"Challenge 1:"},{"location":"old/more/BPRS_DOCS/guide_usecases/#challenge-2","text":"I have access to GWAS data (1) and genotype/phenotype (2,3) data (>2000 samples, binary phenotype) from a moderate sized population in Eastern Europe. I have reason to believe that this population has unique LD structure and would like to incorporate this into my model. I also have GWAS (4) and genotype/phenotype data from the UKB biobank (5,6) that I wish to use to improve my results, how should I do this? ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat Target Config File POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILE = VALIDATION_FILE = [Answer] POP = ukr LDPOP = ukr * SUMSTATS_PREFIX = ukr / sumstats / ukr.sumstats GENOTYPE_PREFIX = ukr / phenotypes / chr PHENOTYPE_FILE = ukr / phenotypes / ukr_test.dat VALIDATION_FILE = ukr / phenotypes / ukr_validation.dat Model Config FIle POP = LDPOP = SUMSTATS_PREFIX = GENOTYPE_PREFIX = PHENOTYPE_FILE = VALIDATION_FILE = Answer POP = UKB LDPOP = EUR SUMSTATS_PREFIX = ukb / sumstats / ukb.sumstats GENOTYPE_PREFIX = ukb / genotypes / chr PHENOTYPE_FILE = ukb / phenotypes / ukb_test.dat VALIDATION_FILE = ukb / phenotypes / ukb_validation.dat","title":"Challenge 2:"},{"location":"old/more/BPRS_DOCS/misc_faq/","text":"Frequently Asked Questions We will continue to update this list to address the more common questions. I've receive the following error message, what should I do? No permission You must fix permission.","title":"Misc faq"},{"location":"old/more/BPRS_DOCS/misc_faq/#frequently-asked-questions","text":"We will continue to update this list to address the more common questions. I've receive the following error message, what should I do? No permission You must fix permission.","title":"Frequently Asked Questions"},{"location":"old/more/BPRS_DOCS/misc_log/","text":"Update Log 2023-08-15 v0.1.7 Added Sample Thousand Genomes Data. 2023-08-05 v0.1.7 Identified and fixed a bug for binary traits.","title":"Misc log"},{"location":"old/more/BPRS_DOCS/misc_log/#update-log","text":"","title":"Update Log"},{"location":"old/more/BPRS_DOCS/misc_log/#2023-08-15-v017","text":"Added Sample Thousand Genomes Data.","title":"2023-08-15 v0.1.7"},{"location":"old/more/BPRS_DOCS/misc_log/#2023-08-05-v017","text":"Identified and fixed a bug for binary traits.","title":"2023-08-05 v0.1.7"},{"location":"old/more/BPRS_DOCS/quikstart_data/","text":"Toy Data Toy data reflecting African, European and East Asian populations are located in the directories data/pop_AFR/ , data/pop_EUR/ and data/pop_EAS/ . Each directory contains (1) GWAS summary stats file(s), (2) genotype file(s) (plink format), and (3) phenotype file(s) (txt format), as well as a population config file that lists the path to each file. For more information on file type requirements or how to create population configuration files for your own data, see Guide: Input Data . To run the demo tutorial go to the next page.","title":"Quikstart data"},{"location":"old/more/BPRS_DOCS/quikstart_data/#toy-data","text":"Toy data reflecting African, European and East Asian populations are located in the directories data/pop_AFR/ , data/pop_EUR/ and data/pop_EAS/ . Each directory contains (1) GWAS summary stats file(s), (2) genotype file(s) (plink format), and (3) phenotype file(s) (txt format), as well as a population config file that lists the path to each file. For more information on file type requirements or how to create population configuration files for your own data, see Guide: Input Data . To run the demo tutorial go to the next page.","title":"Toy Data"},{"location":"old/more/BPRS_DOCS/quikstart_demo/","text":"Input Data This demo runs using the following population config files data/pop_AFR/afr.config and data/pop_EUR/eur.config that list the paths for all required population data. Run The Demo: To run BridgePRS using a continuous phenotype: Easyrun Command: Continuous Trait (y) Run BridgePRS on the toy phenotype \"y\" with the following command: ./bridgePRS easyrun go -o out1 --config_files data/afr.config data/eur.config --phenotype y And verify that a summary figure out1/bridgeSummary.png shown below is created. Then repeat the process using a binary phenotype: Easyrun Command Run BridgePRS on the toy binary phenotype \"y.binary\" with the following command: ./bridgePRS easyrun go -o out2 --config_files data/afr.config data/eur.config --phenotype y.binary Demo Results: If BridgePRS runs successfully on the toy data, please go to the next page for information on interpretation of the results. Before running BridgePRS with larger real data, please consider reading our most common use case examples to understand how best to use BridgePRS in realistic scenarios.","title":"Quikstart demo"},{"location":"old/more/BPRS_DOCS/quikstart_demo/#input-data","text":"This demo runs using the following population config files data/pop_AFR/afr.config and data/pop_EUR/eur.config that list the paths for all required population data.","title":"Input Data"},{"location":"old/more/BPRS_DOCS/quikstart_demo/#run-the-demo","text":"To run BridgePRS using a continuous phenotype: Easyrun Command: Continuous Trait (y) Run BridgePRS on the toy phenotype \"y\" with the following command: ./bridgePRS easyrun go -o out1 --config_files data/afr.config data/eur.config --phenotype y And verify that a summary figure out1/bridgeSummary.png shown below is created. Then repeat the process using a binary phenotype: Easyrun Command Run BridgePRS on the toy binary phenotype \"y.binary\" with the following command: ./bridgePRS easyrun go -o out2 --config_files data/afr.config data/eur.config --phenotype y.binary","title":"Run The Demo:"},{"location":"old/more/BPRS_DOCS/quikstart_demo/#demo-results","text":"If BridgePRS runs successfully on the toy data, please go to the next page for information on interpretation of the results. Before running BridgePRS with larger real data, please consider reading our most common use case examples to understand how best to use BridgePRS in realistic scenarios.","title":"Demo Results:"},{"location":"old/more/BPRS_DOCS/quikstart_result/","text":"Interpreting results The easyrun command used in the previous step runs the following subprograms : prs-single (stage 1): Applied to the base population (Eur). Used to define prior for target population PRS. prs-prior (stage 2): Applied to the target population (AFR) PRS using prior estimated from base population (Eur). prs-single (stage 1): Applied to the target population (Afr). PRS estimated using target population data only. analyse Combine Afr stage 1 and 2 results to produce a weighted target PRS result. prs-port: Base population PRS applied to target population. And produces output in the following five subdirectories: prs-single_AFR/quantify/: Weights, predictions, and performance metrics. model_EUR/prior: Model weights and priors. prs-prior_AFR/quantify/: Weights, predictions, and performance metrics. prs-combined_AFR/: Analysis that combined the previous three steps. prs-port_AFR/quantify/: Weights, predictions, and performance metrics.","title":"Quikstart result"},{"location":"old/more/BPRS_DOCS/quikstart_result/#interpreting-results","text":"The easyrun command used in the previous step runs the following subprograms : prs-single (stage 1): Applied to the base population (Eur). Used to define prior for target population PRS. prs-prior (stage 2): Applied to the target population (AFR) PRS using prior estimated from base population (Eur). prs-single (stage 1): Applied to the target population (Afr). PRS estimated using target population data only. analyse Combine Afr stage 1 and 2 results to produce a weighted target PRS result. prs-port: Base population PRS applied to target population. And produces output in the following five subdirectories: prs-single_AFR/quantify/: Weights, predictions, and performance metrics. model_EUR/prior: Model weights and priors. prs-prior_AFR/quantify/: Weights, predictions, and performance metrics. prs-combined_AFR/: Analysis that combined the previous three steps. prs-port_AFR/quantify/: Weights, predictions, and performance metrics.","title":"Interpreting results"},{"location":"old/more/BPRS_DOCS/req_mac/","text":"Plink: Developer Cannot Be Verified If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running BridgePRS and follow these instructions to give your system permission to call plink from BridgePRS: 1- Default Settings By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below): 2- Allowing Exceptions Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future. 3- Downstream BridgePRS Errors If you have moved plink to the trash you will have to recover it, additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS easyrun go -o out1 --pop_configs data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"Req mac"},{"location":"old/more/BPRS_DOCS/req_mac/#plink-developer-cannot-be-verified","text":"If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running BridgePRS and follow these instructions to give your system permission to call plink from BridgePRS:","title":"Plink: Developer Cannot Be Verified"},{"location":"old/more/BPRS_DOCS/req_mac/#1-default-settings","text":"By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below):","title":"1- Default Settings"},{"location":"old/more/BPRS_DOCS/req_mac/#2-allowing-exceptions","text":"Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future.","title":"2- Allowing Exceptions"},{"location":"old/more/BPRS_DOCS/req_mac/#3-downstream-bridgeprs-errors","text":"If you have moved plink to the trash you will have to recover it, additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS easyrun go -o out1 --pop_configs data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"3- Downstream BridgePRS Errors"},{"location":"old/more/BPRS_DOCS/req_software/","text":"Requirements BridgePRS depends on R, plink, and runs using a Python3 wrapper. R BridgePRS requires the following R [ version 3.6.3+ ] ( Download ) packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils R Packages These packages can be installed from inside an R terminal using the command: $ R install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\")) Plink BridgePRS requires a version of plink compatible with your machine. ( download ) By default BridgePRS includes versions of plink for Linux and MacOs and will attempt to locate the correct version. To override this behavior and use a specific version of plink please use the flag --plinkPath $PLINKPATH to direct BridgePRS to the file location. Extra MacOs Security: MacOs often block executables if they are not approved from the app store. You may have to change your settings to allow Plink to be called For instructions on how to do so, please click here . Python The BridgePRS wrapper requires python3+ ( download ) and the matplotlib library ( download ) is required to create plots (options). Bash BridgePRS can also be run using a shell script as described here . BridgePRS check requirements If BridgePRS has been downloaded and made executable (as described previousely ) then the following command can be used to allow bridgePRS to check system compatibility and prompt you to install missing software: ./bridgePRS check requirements BridgePRS Begins at Sat Sep 30 23:45:46 2023 Bridge Command-Line: ../bridgePRS check requirements Checking Requirements: System: platform=linux, cores(available)=8, cores(used)=1 (TIP: Using More Than One Core Will Improve Performace (e.g. ---cores 7)) Plink: found=true, path=/home/tade/Bin/plink R: found=true, path=/usr/bin/R, version=3.6.3 (packages=up to date) Python3: found=true, path=/usr/bin/python3, matplotlib=true Complete","title":"Req software"},{"location":"old/more/BPRS_DOCS/req_software/#requirements","text":"BridgePRS depends on R, plink, and runs using a Python3 wrapper.","title":"Requirements"},{"location":"old/more/BPRS_DOCS/req_software/#r","text":"BridgePRS requires the following R [ version 3.6.3+ ] ( Download ) packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils R Packages These packages can be installed from inside an R terminal using the command: $ R install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\"))","title":"R"},{"location":"old/more/BPRS_DOCS/req_software/#plink","text":"BridgePRS requires a version of plink compatible with your machine. ( download ) By default BridgePRS includes versions of plink for Linux and MacOs and will attempt to locate the correct version. To override this behavior and use a specific version of plink please use the flag --plinkPath $PLINKPATH to direct BridgePRS to the file location. Extra MacOs Security: MacOs often block executables if they are not approved from the app store. You may have to change your settings to allow Plink to be called For instructions on how to do so, please click here .","title":"Plink"},{"location":"old/more/BPRS_DOCS/req_software/#python","text":"The BridgePRS wrapper requires python3+ ( download ) and the matplotlib library ( download ) is required to create plots (options).","title":"Python"},{"location":"old/more/BPRS_DOCS/req_software/#bash","text":"BridgePRS can also be run using a shell script as described here . BridgePRS check requirements If BridgePRS has been downloaded and made executable (as described previousely ) then the following command can be used to allow bridgePRS to check system compatibility and prompt you to install missing software: ./bridgePRS check requirements BridgePRS Begins at Sat Sep 30 23:45:46 2023 Bridge Command-Line: ../bridgePRS check requirements Checking Requirements: System: platform=linux, cores(available)=8, cores(used)=1 (TIP: Using More Than One Core Will Improve Performace (e.g. ---cores 7)) Plink: found=true, path=/home/tade/Bin/plink R: found=true, path=/usr/bin/R, version=3.6.3 (packages=up to date) Python3: found=true, path=/usr/bin/python3, matplotlib=true Complete","title":"Bash"},{"location":"old/more/BPRS_DOCS/req_system/","text":"Preparation After downloading and unzipping BridgePRS a directory with the following contents will be created: bridgePRS <--- program executable data/ <--- input data LICENSE README.me src/ <--- source code tests/ <--- test directory Using the terminal, type the following command from within the directory: chmod +x bridgePRS chmod +x src/Python/Xtra/plink* to make bridgePRS and plink executable Requirements Next, confirm that the required libraries and dependencies are installed and available by following the instructions in Software. Alternatively, you can type: ./bridgePRS check requirements and bridgePRS will check your system and provide further instructions to help you install missing dependencies. Warning: Extra MacOs Security: If you see this msg when running 'check requirements' (or any other time): You will have to change your system settings to allow bridgePRS to call plink. For instructions on how to do so, please click here .","title":"Req system"},{"location":"old/more/BPRS_DOCS/req_system/#preparation","text":"After downloading and unzipping BridgePRS a directory with the following contents will be created: bridgePRS <--- program executable data/ <--- input data LICENSE README.me src/ <--- source code tests/ <--- test directory Using the terminal, type the following command from within the directory: chmod +x bridgePRS chmod +x src/Python/Xtra/plink* to make bridgePRS and plink executable","title":"Preparation"},{"location":"old/more/BPRS_DOCS/req_system/#requirements","text":"Next, confirm that the required libraries and dependencies are installed and available by following the instructions in Software. Alternatively, you can type: ./bridgePRS check requirements and bridgePRS will check your system and provide further instructions to help you install missing dependencies. Warning: Extra MacOs Security: If you see this msg when running 'check requirements' (or any other time): You will have to change your system settings to allow bridgePRS to call plink. For instructions on how to do so, please click here .","title":"Requirements"},{"location":"old/more/group_projects/gpbase/","text":"Please add your code from the group projects here as a .Rmd","title":"Gpbase"},{"location":"old/more/scripts/base/","text":"scripts base","title":"Base"},{"location":"old/more/scripts/day4update/","text":"wget https://raw.githubusercontent.com/WCSCourses/PRS2024/main/scripts/prs24_day4_builds.sh chmod +x prs24_day4_builds.sh ./prs24_day4_builds.sh","title":"Day4update"}]}