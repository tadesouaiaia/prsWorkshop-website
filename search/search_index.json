{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PRS Workshop is coming to Japan: Aug 30, 2024 Overview Contact For questions about the methodology, this website, or our manuscript please contact Dr Tade Souaiaia , Dr Clive Hoggart , or Dr Paul O'Reilly . For source code and coding issues please visit our github page . Acknowledgements","title":"Home"},{"location":"#prs-workshop-is-coming-to-japan-aug-30-2024","text":"","title":"PRS Workshop is coming to Japan: Aug 30, 2024"},{"location":"#overview","text":"","title":"Overview"},{"location":"#contact","text":"For questions about the methodology, this website, or our manuscript please contact Dr Tade Souaiaia , Dr Clive Hoggart , or Dr Paul O'Reilly . For source code and coding issues please visit our github page .","title":"Contact"},{"location":"#acknowledgements","text":"","title":"Acknowledgements"},{"location":"Day1a.docx/","text":"Polygenic Risk Score Analyses Workshop 2024 Day 1a: GWAS & relevant Statistics Introduction to Bash Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment. Moving around the File System To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/ Looking at the Current Directory Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size Counting Number of Lines in File We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt Search File Content Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l Filtering and Reshu\ufb04ing Files A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if($7<5e-8)\" and \"$7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met. Introduction to R R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop. Basics If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R Working Directory When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\") Libraries Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet). Variables in R You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\") Functions You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2) Plotting While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point() Regression Models In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Polygenic Risk Score Analyses Workshop 2024"},{"location":"Day1a.docx/#polygenic-risk-score-analyses-workshop-2024","text":"","title":"Polygenic Risk Score Analyses Workshop 2024"},{"location":"Day1a.docx/#day-1a-gwas-relevant-statistics","text":"","title":"Day 1a: GWAS &amp; relevant Statistics"},{"location":"Day1a.docx/#introduction-to-bash","text":"Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment.","title":"Introduction to Bash"},{"location":"Day1a.docx/#moving-around-the-file-system","text":"To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/","title":"Moving around the File System"},{"location":"Day1a.docx/#looking-at-the-current-directory","text":"Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size","title":"Looking at the Current Directory"},{"location":"Day1a.docx/#counting-number-of-lines-in-file","text":"We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt","title":"Counting Number of Lines in File"},{"location":"Day1a.docx/#search-file-content","text":"Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l","title":"Search File Content"},{"location":"Day1a.docx/#filtering-and-reshuffling-files","text":"A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if($7<5e-8)\" and \"$7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met.","title":"Filtering and Reshu\ufb04ing Files"},{"location":"Day1a.docx/#introduction-to-r","text":"R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop.","title":"Introduction to R"},{"location":"Day1a.docx/#basics","text":"If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R","title":"Basics"},{"location":"Day1a.docx/#working-directory","text":"When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\")","title":"Working Directory"},{"location":"Day1a.docx/#libraries","text":"Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet).","title":"Libraries"},{"location":"Day1a.docx/#variables-in-r","text":"You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\")","title":"Variables in R"},{"location":"Day1a.docx/#functions","text":"You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2)","title":"Functions"},{"location":"Day1a.docx/#plotting","text":"While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point()","title":"Plotting"},{"location":"Day1a.docx/#regression-models","text":"In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Regression Models"},{"location":"Day1b.docx/","text":"Polygenic Risk Score Analyses Workshop 2024 Practical 1 Introduction to PLINK I: basics Key Learning Outcomes After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website 4. Select and exclude lists of samples and SNPs \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Introduction PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials. Command line basics In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click ) Let's begin Open up a terminal Navigate to the Day 1b working directory cd ~/data/Data_Day1b/ List all files in this directory by typing ls Test PLINK with no input by typing plink Note that you can see lots of PLINK options by using the built-in help function: plink --help \ud83d\udcdd Calling PLINK with no output will test if PLINK is installed and available in the directory, because you should see some output showing the PLINK license and some commands. If you do not see this, then please ask for help now! Exploring Data Sets Open an Explorer window ('Finder' on a Mac) and navigate to your PLINK working directory. \ud83d\udcdd An explorer window should show the same files as the ls command Open the file called 'D1D.map' with a Text Editor e.g. by typing right-click > Open . Open the file 'D1D.ped'. Note this is a large file - if it will not open or is very slow, skip this step. Go to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side) What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file? Create 'binary' format PLINK files using the recode command: plink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Open and examine files ending .bim and .fam. Do not open the .bed file. Open and skim the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary) Recoding alleles as counts Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be? PLINK website Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists. Write SNP list and extract SNPs You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!! Practical 2 Introduction to PLINK II: Performing QC & GWAS Key Learning Outcomes After completing this practical, you should be able to: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS Generate summaries to perform QC There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC. Individual missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"? SNP Missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful? Hardy-Weinberg Equilibrium Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why? Allele frequencies Generate allele frequencies using the command -- freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way Apply QC filters There are di\ufb00erent strategies for performing QC on your data: (a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice. Apply individual missingness thresholds Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening? Apply SNP missingness and MAF thresholds Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness? Apply Hardy-Weinberg thresholds Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP. Perform GWAS Case/Control GWAS - no covariates Run the following code, which performs a genetic association study using logistic regression on some case/control data: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations? Case/Control GWAS - with covariates Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs? License This work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode You are free to: Share --- copy and redistribute the material in any medium or format Adapt --- remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: {#under-the-following-terms .unnumbered} Attribution --- You must give appropriate credit, providea link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial --- You may not use the material for commercial purposes. ShareAlike --- If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions --- You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Day 1b"},{"location":"Day1b.docx/#practical-1","text":"","title":"Practical 1"},{"location":"Day1b.docx/#introduction-to-plink-i-basics","text":"","title":"Introduction to PLINK I: basics"},{"location":"Day1b.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website","title":"Key Learning Outcomes"},{"location":"Day1b.docx/#4-select-and-exclude-lists-of-samples-and-snps","text":"\u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only.","title":"4.  Select and exclude lists of samples and SNPs"},{"location":"Day1b.docx/#introduction","text":"PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials.","title":"Introduction"},{"location":"Day1b.docx/#command-line-basics","text":"In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click )","title":"Command line basics"},{"location":"Day1b.docx/#lets-begin","text":"Open up a terminal Navigate to the Day 1b working directory cd ~/data/Data_Day1b/ List all files in this directory by typing ls Test PLINK with no input by typing plink Note that you can see lots of PLINK options by using the built-in help function: plink --help \ud83d\udcdd Calling PLINK with no output will test if PLINK is installed and available in the directory, because you should see some output showing the PLINK license and some commands. If you do not see this, then please ask for help now!","title":"Let's begin"},{"location":"Day1b.docx/#exploring-data-sets","text":"Open an Explorer window ('Finder' on a Mac) and navigate to your PLINK working directory. \ud83d\udcdd An explorer window should show the same files as the ls command Open the file called 'D1D.map' with a Text Editor e.g. by typing right-click > Open . Open the file 'D1D.ped'. Note this is a large file - if it will not open or is very slow, skip this step. Go to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side) What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file? Create 'binary' format PLINK files using the recode command: plink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Open and examine files ending .bim and .fam. Do not open the .bed file. Open and skim the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary)","title":"Exploring Data Sets"},{"location":"Day1b.docx/#recoding-alleles-as-counts","text":"Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be?","title":"Recoding alleles as counts"},{"location":"Day1b.docx/#plink-website","text":"Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists.","title":"PLINK website"},{"location":"Day1b.docx/#write-snp-list-and-extract-snps","text":"You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!!","title":"Write SNP list and extract SNPs"},{"location":"Day1b.docx/#practical-2","text":"","title":"Practical 2"},{"location":"Day1b.docx/#introduction-to-plink-ii-performing-qc-gwas","text":"","title":"Introduction to PLINK II: Performing QC &amp; GWAS"},{"location":"Day1b.docx/#key-learning-outcomes_1","text":"After completing this practical, you should be able to: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS","title":"Key Learning Outcomes"},{"location":"Day1b.docx/#generate-summaries-to-perform-qc","text":"There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC.","title":"Generate summaries to perform QC"},{"location":"Day1b.docx/#individual-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"?","title":"Individual missingness"},{"location":"Day1b.docx/#snp-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful?","title":"SNP Missingness"},{"location":"Day1b.docx/#hardy-weinberg-equilibrium","text":"Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why?","title":"Hardy-Weinberg Equilibrium"},{"location":"Day1b.docx/#allele-frequencies","text":"Generate allele frequencies using the command -- freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way","title":"Allele frequencies"},{"location":"Day1b.docx/#apply-qc-filters","text":"","title":"Apply QC filters"},{"location":"Day1b.docx/#there-are-different-strategies-for-performing-qc-on-your-data","text":"(a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice.","title":"There are di\ufb00erent strategies for performing QC on your data:"},{"location":"Day1b.docx/#apply-individual-missingness-thresholds","text":"Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening?","title":"Apply individual missingness thresholds"},{"location":"Day1b.docx/#apply-snp-missingness-and-maf-thresholds","text":"Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness?","title":"Apply SNP missingness and MAF thresholds"},{"location":"Day1b.docx/#apply-hardy-weinberg-thresholds","text":"Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP.","title":"Apply Hardy-Weinberg thresholds"},{"location":"Day1b.docx/#perform-gwas","text":"","title":"Perform GWAS"},{"location":"Day1b.docx/#casecontrol-gwas-no-covariates","text":"Run the following code, which performs a genetic association study using logistic regression on some case/control data: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations?","title":"Case/Control GWAS - no covariates"},{"location":"Day1b.docx/#casecontrol-gwas-with-covariates","text":"Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Case/Control GWAS - with covariates"},{"location":"Day1b.docx/#license","text":"This work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode","title":"License"},{"location":"Day1b.docx/#you-are-free-to","text":"Share --- copy and redistribute the material in any medium or format Adapt --- remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to:"},{"location":"Day1b.docx/#under-the-following-terms-under-the-following-terms-unnumbered","text":"Attribution --- You must give appropriate credit, providea link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial --- You may not use the material for commercial purposes. ShareAlike --- If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions --- You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms: {#under-the-following-terms .unnumbered}"},{"location":"Day1b.docx/#notices","text":"You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Notices:"},{"location":"Day2.docx/","text":"Introduction to Polygenic Risk Scores Table of Contents Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents Key Learning Outcomes After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results Resources you will be using To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link Data Structure You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents Introduction A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS. Understanding GWAS Summary Statistics When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents Matching the Base and Target Data sets The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents Linkage Disequilibrium in PRS Analyses GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents Performing Clumping Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents P-Value Thresholding Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds. Height PRS using GW-significant SNPs only Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5 \u00d7 10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5 \u00d7 10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents Height PRS across multiple P-value thresholds A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents High Resolution Scoring If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5 \u00d7 10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents Stratifying Samples by PRS An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents Case Control Studies In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres \u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status. Cross-Trait Analysis A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Day 2"},{"location":"Day2.docx/#introduction-to-polygenic-risk-scores","text":"","title":"Introduction to Polygenic Risk Scores"},{"location":"Day2.docx/#table-of-contents","text":"Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents","title":"Table of Contents"},{"location":"Day2.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results","title":"Key Learning Outcomes"},{"location":"Day2.docx/#resources-you-will-be-using","text":"To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link","title":"Resources you will be using"},{"location":"Day2.docx/#data-structure","text":"You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents","title":"Data Structure"},{"location":"Day2.docx/#introduction","text":"A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS.","title":"Introduction"},{"location":"Day2.docx/#understanding-gwas-summary-statistics","text":"When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents","title":"Understanding GWAS Summary Statistics"},{"location":"Day2.docx/#matching-the-base-and-target-data-sets","text":"The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents","title":"Matching the Base and Target Data sets"},{"location":"Day2.docx/#linkage-disequilibrium-in-prs-analyses","text":"GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents","title":"Linkage Disequilibrium in PRS Analyses"},{"location":"Day2.docx/#performing-clumping","text":"Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents","title":"Performing Clumping"},{"location":"Day2.docx/#p-value-thresholding","text":"Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds.","title":"P-Value Thresholding"},{"location":"Day2.docx/#height-prs-using-gw-significant-snps-only","text":"Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5 \u00d7 10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5 \u00d7 10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents","title":"Height PRS using GW-significant SNPs only"},{"location":"Day2.docx/#height-prs-across-multiple-p-value-thresholds","text":"A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents","title":"Height PRS across multiple P-value thresholds"},{"location":"Day2.docx/#high-resolution-scoring","text":"If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5 \u00d7 10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents","title":"High Resolution Scoring"},{"location":"Day2.docx/#stratifying-samples-by-prs","text":"An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents","title":"Stratifying Samples by PRS"},{"location":"Day2.docx/#case-control-studies","text":"In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres \u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status.","title":"Case Control Studies"},{"location":"Day2.docx/#cross-trait-analysis","text":"A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Cross-Trait Analysis"},{"location":"Day3a.docx/","text":"Advanced Polygenic Risk Score Analyses Day 3 - Polygenic Risk Score Analyses Workshop 2024 Table of Contents Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2 Day 3a practical Key Learning Outcomes After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds. Base and Target datasets In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650) Downloading Datasets All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam Method for calculating PRS For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach. Exercise 1 Estimating R 2 Code Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2 Scenario 1: Predicting from EUR training to EUR target data: Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur Key code parameters The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389. Scenario 2: Predicting from AFR training to AFR target data: Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%). Scenario 3: Predicting from EUR training to AFR target data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%). Exercise 2 Visualising and comparing R 2 In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library(ggplot2) library(RColorBrewer) # Create a function to read the files and add ancestry information read_and_label <- function(file, ancestry) { data <- read.table(file, header = TRUE, sep = \"\\t\") data$Ancestry <- ancestry return(data) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label(\"SBP.eur.eur.summary\", \"EUR_EUR\") AFR_AFR <- read_and_label(\"SBP.afr.afr.summary\", \"AFR_AFR\") EUR_AFR <- read_and_label(\"SBP.eur.afr.summary\", \"EUR_AFR\") # Combine all data into one dataframe all_data <- rbind(EUR_EUR, AFR_AFR, EUR_AFR) # Create a bar graph with different colors for each ancestry png('/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) ancestry <- ggplot(all_data, aes(x = Ancestry, y = PRS.R2, fill = Ancestry)) + geom_bar(stat = \"identity\", position = \"dodge\") + labs(title = \"R2 Values by Ancestry\", x = \"Ancestry\", y = \"R2 Value\") + theme_minimal() + scale_fill_brewer(palette = \"Set3\") + theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"), axis.title.x = element_text(size = 14, face = \"bold\"), axis.title.y = element_text(size = 14, face = \"bold\"), axis.text.x = element_text(size = 12, angle = 45, hjust = 1), axis.text.y = element_text(size = 12), legend.position = \"none\") print(ancestry) dev.off() Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Day 3a"},{"location":"Day3a.docx/#advanced-polygenic-risk-score-analyses","text":"","title":"Advanced Polygenic Risk Score Analyses"},{"location":"Day3a.docx/#day-3-polygenic-risk-score-analyses-workshop-2024","text":"","title":"Day 3 - Polygenic Risk Score Analyses Workshop 2024"},{"location":"Day3a.docx/#table-of-contents","text":"Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2","title":"Table of Contents"},{"location":"Day3a.docx/#day-3a-practical","text":"","title":"Day 3a practical"},{"location":"Day3a.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds.","title":"Key Learning Outcomes"},{"location":"Day3a.docx/#base-and-target-datasets","text":"In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650)","title":"Base and Target datasets"},{"location":"Day3a.docx/#downloading-datasets","text":"All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam","title":"Downloading Datasets"},{"location":"Day3a.docx/#method-for-calculating-prs","text":"For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach.","title":"Method for calculating PRS"},{"location":"Day3a.docx/#exercise-1-estimating-r2","text":"","title":"Exercise 1 Estimating R2"},{"location":"Day3a.docx/#code","text":"Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2","title":"Code"},{"location":"Day3a.docx/#scenario-1-predicting-from-eur-training-to-eur-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur","title":"Scenario 1: Predicting from EUR training to EUR target data:"},{"location":"Day3a.docx/#key-code-parameters","text":"The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389.","title":"Key code parameters"},{"location":"Day3a.docx/#scenario-2-predicting-from-afr-training-to-afr-target-data","text":"Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%).","title":"Scenario 2: Predicting from AFR training to AFR target data:"},{"location":"Day3a.docx/#scenario-3-predicting-from-eur-training-to-afr-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%).","title":"Scenario 3: Predicting from EUR training to AFR target data"},{"location":"Day3a.docx/#exercise-2-visualising-and-comparing-r2","text":"In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library(ggplot2) library(RColorBrewer) # Create a function to read the files and add ancestry information read_and_label <- function(file, ancestry) { data <- read.table(file, header = TRUE, sep = \"\\t\") data$Ancestry <- ancestry return(data) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label(\"SBP.eur.eur.summary\", \"EUR_EUR\") AFR_AFR <- read_and_label(\"SBP.afr.afr.summary\", \"AFR_AFR\") EUR_AFR <- read_and_label(\"SBP.eur.afr.summary\", \"EUR_AFR\") # Combine all data into one dataframe all_data <- rbind(EUR_EUR, AFR_AFR, EUR_AFR) # Create a bar graph with different colors for each ancestry png('/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) ancestry <- ggplot(all_data, aes(x = Ancestry, y = PRS.R2, fill = Ancestry)) + geom_bar(stat = \"identity\", position = \"dodge\") + labs(title = \"R2 Values by Ancestry\", x = \"Ancestry\", y = \"R2 Value\") + theme_minimal() + scale_fill_brewer(palette = \"Set3\") + theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"), axis.title.x = element_text(size = 14, face = \"bold\"), axis.title.y = element_text(size = 14, face = \"bold\"), axis.text.x = element_text(size = 12, angle = 45, hjust = 1), axis.text.y = element_text(size = 12), legend.position = \"none\") print(ancestry) dev.off() Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Exercise 2 Visualising and comparing R2"},{"location":"Day3b.docx/","text":"Day 3b practical We need to move into the directory you will be working in; cd ~/data/Data_Day4/data Introduction to Cross-Ancestry PRS computation Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python=3.7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion. 1. The 1000 Genomes datasets The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week. 2. Cross-population allele frequency Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l Questions (i) Which population contains the most SNPs? (ii) What is the significance of the observed population order? 3. Distribution of allele frequencies In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages(\"dplyr\") install.packages(\"ggplot2\") # Load necessary libraries library(dplyr) library(ggplot2) # Create a function to read the files and add ancestry information freq <-read.table(\"~/data/Data_Day4/plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) # Create a bar graph png('/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) maf_ancestry <- ggplot(na.omit(plotDat), aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") print(maf_ancestry) dev.off() Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png Questions (i) Which population has the most SNPs? (ii) What is the significance of the observed population ordering? (iii) What is the reason behind these two features? Introduction to PRS-CSx 5. Background to PRS-CSX PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and external LD reference panels from multiple populations to improve cross-population polygenic prediction. We will be using simulated trait data pertaininng to systolic blood pressure (SBP) to explore PRS performance using 2 target populations that consist of 650 individuals of African ancestry and 500 individuals of European ancestry. Please note when running PRSice that the object of the flag \"--prsice\" will change according to whether plink is being called within the linux-like environment of the virtual machine (PRSice_linux) or a mac (PRSice_mac). Both executables can be found in the /home/manager/data/Data_Day4 directory. 7. Running PRS-CSx To model the coupling of ect sizes at individual SNPs across ancestries PRS-CSx uses an MCMC (Bayesian) sampling algorithm to determine values of the global shrinkage parameter (\"phi\") by Maximum likelihood. For samples of mixed or admixed genetic ancestry (which ours are not) the optimal value of the shrinkage parameter is estimated autonomously from the data. Here we use the value of phi (1e-4), which is suggested by the software authors, given that our trait is simulated to have a relatively small number (N=110) causal variants, distributed genome-wide. Step 1: Set up environment First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE=\"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21..22}; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr${chr},/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr${chr} \\ --n_gwas=25732,4855 \\ --chrom=${chr} \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr${chr}.csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3) Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE) Task: Replace 'ancestry <- \"EUR\" ' with 'ancestry <- \"AFR\" ' and repeat the subsequent steps shown above Step 4: Merge genotype-phenotype data Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen) Step 5: Split data into validation and test sets Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices] Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights # Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur) Step 7: Prepare the variant weights matrices as vectors # Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat)) Step 8: Predict phenotype on validation and test dataset Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z Step 9: Plot phenotype distributions of validation and test data: Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal() Step 10: Plot true values against predicted values The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off() Step 11: Calculate deviance-based R 2 # Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2)) Results graph: pdf true against pred","title":"Day 3b"},{"location":"Day3b.docx/#day-3b-practical","text":"We need to move into the directory you will be working in; cd ~/data/Data_Day4/data","title":"Day 3b practical"},{"location":"Day3b.docx/#introduction-to-cross-ancestry-prs-computation","text":"Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python=3.7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion.","title":"Introduction to Cross-Ancestry PRS computation"},{"location":"Day3b.docx/#1-the-1000-genomes-datasets","text":"The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week.","title":"1. The 1000 Genomes datasets"},{"location":"Day3b.docx/#2-cross-population-allele-frequency","text":"Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l","title":"2. Cross-population allele frequency"},{"location":"Day3b.docx/#questions","text":"","title":"Questions"},{"location":"Day3b.docx/#i-which-population-contains-the-most-snps","text":"","title":"(i) Which population contains the most SNPs?"},{"location":"Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-order","text":"","title":"(ii) What  is the significance of the observed population order?"},{"location":"Day3b.docx/#3-distribution-of-allele-frequencies","text":"In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages(\"dplyr\") install.packages(\"ggplot2\") # Load necessary libraries library(dplyr) library(ggplot2) # Create a function to read the files and add ancestry information freq <-read.table(\"~/data/Data_Day4/plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) # Create a bar graph png('/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) maf_ancestry <- ggplot(na.omit(plotDat), aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") print(maf_ancestry) dev.off() Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png","title":"3. Distribution of allele frequencies"},{"location":"Day3b.docx/#questions_1","text":"","title":"Questions"},{"location":"Day3b.docx/#i-which-population-has-the-most-snps","text":"","title":"(i) Which population has the most SNPs?"},{"location":"Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-ordering","text":"","title":"(ii) What  is the significance of the observed population ordering?"},{"location":"Day3b.docx/#iii-what-is-the-reason-behind-these-two-features","text":"","title":"(iii) What is the reason behind these two features?"},{"location":"Day3b.docx/#introduction-to-prs-csx","text":"","title":"Introduction to PRS-CSx"},{"location":"Day3b.docx/#5-background-to-prs-csx","text":"PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and external LD reference panels from multiple populations to improve cross-population polygenic prediction. We will be using simulated trait data pertaininng to systolic blood pressure (SBP) to explore PRS performance using 2 target populations that consist of 650 individuals of African ancestry and 500 individuals of European ancestry. Please note when running PRSice that the object of the flag \"--prsice\" will change according to whether plink is being called within the linux-like environment of the virtual machine (PRSice_linux) or a mac (PRSice_mac). Both executables can be found in the /home/manager/data/Data_Day4 directory.","title":"5. Background to PRS-CSX"},{"location":"Day3b.docx/#7-running-prs-csx","text":"To model the coupling of ect sizes at individual SNPs across ancestries PRS-CSx uses an MCMC (Bayesian) sampling algorithm to determine values of the global shrinkage parameter (\"phi\") by Maximum likelihood. For samples of mixed or admixed genetic ancestry (which ours are not) the optimal value of the shrinkage parameter is estimated autonomously from the data. Here we use the value of phi (1e-4), which is suggested by the software authors, given that our trait is simulated to have a relatively small number (N=110) causal variants, distributed genome-wide.","title":"7. Running PRS-CSx"},{"location":"Day3b.docx/#step-1-set-up-environment","text":"First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS","title":"Step 1: Set up environment"},{"location":"Day3b.docx/#step-2-run-csx-derive-new-snps-weights-trained-on-european-and-african-summary-stats","text":"Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE=\"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21..22}; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr${chr},/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr${chr} \\ --n_gwas=25732,4855 \\ --chrom=${chr} \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr${chr}.csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh","title":"Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats"},{"location":"Day3b.docx/#step-3-combine-csx-derived-snp-weights-across-chromosomes-currently-excludes-chromosome-3","text":"Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE)","title":"Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3)"},{"location":"Day3b.docx/#task-replace-ancestry-eur-with-ancestry-afr-and-repeat-the-subsequent-steps-shown-above","text":"","title":"Task: Replace 'ancestry &lt;- \"EUR\" ' with 'ancestry &lt;- \"AFR\" ' and repeat the subsequent steps shown above"},{"location":"Day3b.docx/#step-4-merge-genotype-phenotype-data","text":"Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen)","title":"Step 4: Merge genotype-phenotype data"},{"location":"Day3b.docx/#step-5-split-data-into-validation-and-test-sets","text":"Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices]","title":"Step 5: Split data into validation and test sets"},{"location":"Day3b.docx/#step-6-prepare-the-regression-model-input-using-the-csx-derived-afr-and-eur-weights","text":"# Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur)","title":"Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights"},{"location":"Day3b.docx/#step-7-prepare-the-variant-weights-matrices-as-vectors","text":"# Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat))","title":"Step 7: Prepare the variant weights matrices as vectors"},{"location":"Day3b.docx/#step-8-predict-phenotype-on-validation-and-test-dataset","text":"Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z","title":"Step 8: Predict phenotype on validation and test dataset"},{"location":"Day3b.docx/#step-9-plot-phenotype-distributions-of-validation-and-test-data","text":"Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal()","title":"Step 9: Plot phenotype distributions of validation and test data:"},{"location":"Day3b.docx/#step-10-plot-true-values-against-predicted-values","text":"The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off()","title":"Step 10: Plot true values against predicted values"},{"location":"Day3b.docx/#step-11-calculate-deviance-based-r2","text":"# Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2))","title":"Step 11: Calculate deviance-based R2"},{"location":"Day3b.docx/#results","text":"graph: pdf true against pred","title":"Results"},{"location":"Day4a.docx/","text":"BridgePRS Learning Objectives The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method. Introduction to BridgePRS BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values. BridgePRS Scenario 1: Application of African GWAS weights to an African target group Create configuration file for the target-only analysis In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes). Questions From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ? Note In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step. Single ancestry BridgePRS analysis: Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y Task Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders Questions What evidence can you see that the analysis was successfully executed? BridgePRS Scenario 2: Prediction into African target data using European and African summary statistics BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations. Create configuration file for base and target populations bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat Question Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located? Tasks Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below. Multi-ancestry BRIDGEPRS analysis: Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y Tasks After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory. Questions In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model? Short Quiz I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis? File types ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat Answer Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Day 4a"},{"location":"Day4a.docx/#bridgeprs","text":"","title":"BridgePRS"},{"location":"Day4a.docx/#learning-objectives","text":"The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method.","title":"Learning Objectives"},{"location":"Day4a.docx/#introduction-to-bridgeprs","text":"BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values.","title":"Introduction to BridgePRS"},{"location":"Day4a.docx/#bridgeprs-scenario-1-application-of-african-gwas-weights-to-an-african-target-group","text":"","title":"BridgePRS Scenario 1: Application of African GWAS weights to an African target group"},{"location":"Day4a.docx/#create-configuration-file-for-the-target-only-analysis","text":"In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes).","title":"Create configuration file for the target-only analysis"},{"location":"Day4a.docx/#questions","text":"From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ?","title":"Questions"},{"location":"Day4a.docx/#note","text":"In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step.","title":"Note"},{"location":"Day4a.docx/#single-ancestry-bridgeprs-analysis","text":"Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y","title":"Single ancestry BridgePRS analysis:"},{"location":"Day4a.docx/#task","text":"Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders","title":"Task"},{"location":"Day4a.docx/#questions_1","text":"What evidence can you see that the analysis was successfully executed?","title":"Questions"},{"location":"Day4a.docx/#bridgeprs-scenario-2-prediction-into-african-target-data-using-european-and-african-summary-statistics","text":"BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations.","title":"BridgePRS Scenario 2:  Prediction into African target data using European and African summary statistics"},{"location":"Day4a.docx/#create-configuration-file-for-base-and-target-populations","text":"bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat","title":"Create configuration file for base and target populations"},{"location":"Day4a.docx/#question","text":"Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located?","title":"Question"},{"location":"Day4a.docx/#tasks","text":"Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below.","title":"Tasks"},{"location":"Day4a.docx/#multi-ancestry-bridgeprs-analysis","text":"Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y","title":"Multi-ancestry BRIDGEPRS analysis:"},{"location":"Day4a.docx/#tasks_1","text":"After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory.","title":"Tasks"},{"location":"Day4a.docx/#questions_2","text":"In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model?","title":"Questions"},{"location":"Day4a.docx/#short-quiz","text":"I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis?","title":"Short Quiz"},{"location":"Day4a.docx/#file-types","text":"ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat","title":"File types"},{"location":"Day4a.docx/#answer","text":"Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Answer"},{"location":"Day4b.docx/","text":"Day 4 - Practical 2: Introduction to Admixture analysis Module Goals The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores Part1: Plot Decay of Ancestry LD over time Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" ) Questions (i) Describe what happens to admixture LD over time? (ii) Why does recombination also have an impact? Part 2: Global Ancestry Inference We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ``` ./admixture samples_n28_qc_thin.bed 2 --supervised -j4 #### **Questions** ##### (i) What do you think the number specified after the inout file represents? The next step is to create a plot the results Plot Global Ancestry Results R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300) #### Questions ##### (i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary) ### Part 3: Local Ancestry Inference Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm. Run the following UNIX command from the home directory module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done #### Questions ##### (i) What information is provided in the simulation results table displayed on-screen ? ### Part 4: Plot local admixture on chromosome 22 In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr) Function to read the .msp.tsv file read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) } Function to read the .Q file read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) } Specification of file paths msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q' Read in files msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file) Check for NA or empty column names print(colnames(msp_df)) Extract ancestry columns based on pattern recognition of column names ancestry_cols <- colnames(msp_df)[grep(\"\\.0$|\\.1$\", colnames(msp_df))] Function to determine ancestry based on RFMix codes determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) } Rename columns uniquely colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62]) Prepare data for plotting plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) ) Create plot of chromosome 22 across the sample plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10)) Save the plot to a file ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8) #### Questions ##### (i) How many different continental ancestries do you see represented across the 58 strands? ##### (ii) Why is the number of different ancestry backgrounds higher than it was in the previous step? ### Part 5: Formatting of admixture files for analysis using PRSice #### Step 1 - Convert phased genotypes to GenomicRange format Run from the home directory library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() } Read in phased VCF file vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22) Convert haplotypes to data frame haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\") Convert SNP info to data frame snp_info_df <- data.frame(extracted_snp_info22) Check for numerical and ordering consistency between haplotypes and SNP info if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") } Merge haplotypes and SNP info merge_chr22 <- cbind(snp_info_df, haps_dt) Convert to long format using panelr chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) Insert 'end' column after 'POS' and create 'wave' column chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\")) Remove row names and drop redundant columns using base R rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops] Rename columns names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header Create GRanges object gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) Save the GRanges object saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\") Clean up memory clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\")) ### Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix Read in MSP file msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\") Reformat genome-wide local ancestry output as GRanges object colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE) Convert the elements of the GRange object into a dataframe chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL Clean up column headers header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header Convert local ancestry calls from wide to long format chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\")) Drop redundant columns drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL Convert the reconfigured dataframe file into a GRanges object chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE) Ensure chr22_haplo_long is a GRanges object before finding overlaps chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) Find overlaps and store matching features Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)])) Local ancestry calls are now aligned with genotypic data and positional coordinates Convert to dataframe without adding 'X' to numeric column names genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df)) Output the dataframe write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F) Clean up memory rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc() #### Part 5: Step 3 - Create separate Plink files Partition genotype and local ancestry data chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))] Reintegrate in interleaved format d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)] Prepare headers indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx] Convert LAnc data from long to wide format LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE))) Check length of LA_wide length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers Convert genotype calls to horizontal orientation geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes Clean up and finalize geno_final geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal) Name columns colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") Write final tables write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE) #### Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22 Perform general QC ahead of running PRSice on ancestry-deconvolved individuals (i) Remove SNPs with low minor allele count (MAC) Plink - Remove monomorphic SNPs (minor allele count 0-4). AFR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done EUR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done PRSice - Generate ancestry-specific weights AFR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base EUR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base #### Part 5: Step 5 - Evaluate the Admixture-informed PRS library(dplyr) Read the PRS files into dataframes file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F) Add the fourth column of both files file1$PRS_SUM <- file1$PRS + file2$PRS Load the phenotype data pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\") Convert phenotype column to numeric pheno$phenotype <- as.numeric(pheno$phenotype) Merge PRS data with phenotype data merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\")) Rename columns names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\" Perform linear regression for each PRS and the combined PRS model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data) Extract R-squared values r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared Print the R-squared values cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\") ``` Questions (i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?","title":"Day 4b"},{"location":"Day4b.docx/#day-4-practical-2-introduction-to-admixture-analysis","text":"","title":"Day 4 - Practical 2: Introduction to Admixture analysis"},{"location":"Day4b.docx/#module-goals","text":"The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores","title":"Module Goals"},{"location":"Day4b.docx/#part1-plot-decay-of-ancestry-ld-over-time","text":"Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" )","title":"Part1: Plot Decay of Ancestry LD over time"},{"location":"Day4b.docx/#questions","text":"","title":"Questions"},{"location":"Day4b.docx/#i-describe-what-happens-to-admixture-ld-over-time","text":"","title":"(i) Describe what happens to admixture LD over time?"},{"location":"Day4b.docx/#ii-why-does-recombination-also-have-an-impact","text":"","title":"(ii) Why does recombination also have an impact?"},{"location":"Day4b.docx/#part-2-global-ancestry-inference","text":"We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ``` ./admixture samples_n28_qc_thin.bed 2 --supervised -j4 #### **Questions** ##### (i) What do you think the number specified after the inout file represents? The next step is to create a plot the results","title":"Part 2: Global Ancestry Inference"},{"location":"Day4b.docx/#plot-global-ancestry-results","text":"R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300) #### Questions ##### (i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary) ### Part 3: Local Ancestry Inference Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm.","title":"Plot Global Ancestry Results"},{"location":"Day4b.docx/#run-the-following-unix-command-from-the-home-directory","text":"module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done #### Questions ##### (i) What information is provided in the simulation results table displayed on-screen ? ### Part 4: Plot local admixture on chromosome 22 In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr)","title":"Run the following UNIX command from the home directory"},{"location":"Day4b.docx/#function-to-read-the-msptsv-file","text":"read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) }","title":"Function to read the .msp.tsv file"},{"location":"Day4b.docx/#function-to-read-the-q-file","text":"read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) }","title":"Function to read the .Q file"},{"location":"Day4b.docx/#specification-of-file-paths","text":"msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q'","title":"Specification of file paths"},{"location":"Day4b.docx/#read-in-files","text":"msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file)","title":"Read in files"},{"location":"Day4b.docx/#check-for-na-or-empty-column-names","text":"print(colnames(msp_df))","title":"Check for NA or empty column names"},{"location":"Day4b.docx/#extract-ancestry-columns-based-on-pattern-recognition-of-column-names","text":"ancestry_cols <- colnames(msp_df)[grep(\"\\.0$|\\.1$\", colnames(msp_df))]","title":"Extract ancestry columns based on pattern recognition of column names"},{"location":"Day4b.docx/#function-to-determine-ancestry-based-on-rfmix-codes","text":"determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) }","title":"Function to determine ancestry based on RFMix codes"},{"location":"Day4b.docx/#rename-columns-uniquely","text":"colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62])","title":"Rename columns uniquely"},{"location":"Day4b.docx/#prepare-data-for-plotting","text":"plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) )","title":"Prepare data for plotting"},{"location":"Day4b.docx/#create-plot-of-chromosome-22-across-the-sample","text":"plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10))","title":"Create plot of chromosome 22 across the sample"},{"location":"Day4b.docx/#save-the-plot-to-a-file","text":"ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8) #### Questions ##### (i) How many different continental ancestries do you see represented across the 58 strands? ##### (ii) Why is the number of different ancestry backgrounds higher than it was in the previous step? ### Part 5: Formatting of admixture files for analysis using PRSice #### Step 1 - Convert phased genotypes to GenomicRange format","title":"Save the plot to a file"},{"location":"Day4b.docx/#run-from-the-home-directory","text":"library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() }","title":"Run from the home directory"},{"location":"Day4b.docx/#read-in-phased-vcf-file","text":"vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22)","title":"Read in phased VCF file"},{"location":"Day4b.docx/#convert-haplotypes-to-data-frame","text":"haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\")","title":"Convert haplotypes to data frame"},{"location":"Day4b.docx/#convert-snp-info-to-data-frame","text":"snp_info_df <- data.frame(extracted_snp_info22)","title":"Convert SNP info to data frame"},{"location":"Day4b.docx/#check-for-numerical-and-ordering-consistency-between-haplotypes-and-snp-info","text":"if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") }","title":"Check for numerical and ordering consistency between haplotypes and SNP info"},{"location":"Day4b.docx/#merge-haplotypes-and-snp-info","text":"merge_chr22 <- cbind(snp_info_df, haps_dt)","title":"Merge haplotypes and SNP info"},{"location":"Day4b.docx/#convert-to-long-format-using-panelr","text":"chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE)","title":"Convert to long format using panelr"},{"location":"Day4b.docx/#insert-end-column-after-pos-and-create-wave-column","text":"chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\"))","title":"Insert 'end' column after 'POS' and create 'wave' column"},{"location":"Day4b.docx/#remove-row-names-and-drop-redundant-columns-using-base-r","text":"rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops]","title":"Remove row names and drop redundant columns using base R"},{"location":"Day4b.docx/#rename-columns","text":"names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header","title":"Rename columns"},{"location":"Day4b.docx/#create-granges-object","text":"gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE)","title":"Create GRanges object"},{"location":"Day4b.docx/#save-the-granges-object","text":"saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\")","title":"Save the GRanges object"},{"location":"Day4b.docx/#clean-up-memory","text":"clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\")) ### Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix","title":"Clean up memory"},{"location":"Day4b.docx/#read-in-msp-file","text":"msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\")","title":"Read in MSP file"},{"location":"Day4b.docx/#reformat-genome-wide-local-ancestry-output-as-granges-object","text":"colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE)","title":"Reformat genome-wide local ancestry output as GRanges object"},{"location":"Day4b.docx/#convert-the-elements-of-the-grange-object-into-a-dataframe","text":"chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL","title":"Convert the elements of the GRange object into a dataframe"},{"location":"Day4b.docx/#clean-up-column-headers","text":"header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header","title":"Clean up column headers"},{"location":"Day4b.docx/#convert-local-ancestry-calls-from-wide-to-long-format","text":"chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\"))","title":"Convert local ancestry calls from wide to long format"},{"location":"Day4b.docx/#drop-redundant-columns","text":"drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL","title":"Drop redundant columns"},{"location":"Day4b.docx/#convert-the-reconfigured-dataframe-file-into-a-granges-object","text":"chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE)","title":"Convert the reconfigured dataframe file into a GRanges object"},{"location":"Day4b.docx/#ensure-chr22_haplo_long-is-a-granges-object-before-finding-overlaps","text":"chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE)","title":"Ensure chr22_haplo_long is a GRanges object before finding overlaps"},{"location":"Day4b.docx/#find-overlaps-and-store-matching-features","text":"","title":"Find overlaps and store matching features"},{"location":"Day4b.docx/#matching-is-coordinates-based-base-position-startendis-used-to-match-the-two-files","text":"matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)]))","title":"Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files"},{"location":"Day4b.docx/#local-ancestry-calls-are-now-aligned-with-genotypic-data-and-positional-coordinates","text":"","title":"Local ancestry calls are now aligned with genotypic data and positional coordinates"},{"location":"Day4b.docx/#convert-to-dataframe-without-adding-x-to-numeric-column-names","text":"genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df))","title":"Convert to dataframe without adding 'X' to numeric column names"},{"location":"Day4b.docx/#output-the-dataframe","text":"write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F)","title":"Output the dataframe"},{"location":"Day4b.docx/#clean-up-memory_1","text":"rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc() #### Part 5: Step 3 - Create separate Plink files","title":"Clean up memory"},{"location":"Day4b.docx/#partition-genotype-and-local-ancestry-data","text":"chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))]","title":"Partition genotype and local ancestry data"},{"location":"Day4b.docx/#reintegrate-in-interleaved-format","text":"d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)]","title":"Reintegrate in interleaved format"},{"location":"Day4b.docx/#prepare-headers","text":"indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx]","title":"Prepare headers"},{"location":"Day4b.docx/#convert-lanc-data-from-long-to-wide-format","text":"LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE)))","title":"Convert LAnc data from long to wide format"},{"location":"Day4b.docx/#check-length-of-la_wide","text":"length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers","title":"Check length of LA_wide"},{"location":"Day4b.docx/#convert-genotype-calls-to-horizontal-orientation","text":"geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes","title":"Convert genotype calls to horizontal orientation"},{"location":"Day4b.docx/#clean-up-and-finalize-geno_final","text":"geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal)","title":"Clean up and finalize geno_final"},{"location":"Day4b.docx/#name-columns","text":"colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\")","title":"Name columns"},{"location":"Day4b.docx/#write-final-tables","text":"write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE) #### Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22","title":"Write final tables"},{"location":"Day4b.docx/#perform-general-qc-ahead-of-running-prsice-on-ancestry-deconvolved-individuals","text":"","title":"Perform general QC ahead of running PRSice on ancestry-deconvolved individuals"},{"location":"Day4b.docx/#i-remove-snps-with-low-minor-allele-count-mac","text":"","title":"(i) Remove SNPs with low minor allele count (MAC)"},{"location":"Day4b.docx/#plink-remove-monomorphic-snps-minor-allele-count-0-4","text":"","title":"Plink - Remove monomorphic SNPs (minor allele count 0-4)."},{"location":"Day4b.docx/#afr","text":"for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done","title":"AFR"},{"location":"Day4b.docx/#eur","text":"for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done","title":"EUR"},{"location":"Day4b.docx/#prsice-generate-ancestry-specific-weights","text":"","title":"PRSice - Generate ancestry-specific weights"},{"location":"Day4b.docx/#afr_1","text":"Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base","title":"AFR"},{"location":"Day4b.docx/#eur_1","text":"Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base #### Part 5: Step 5 - Evaluate the Admixture-informed PRS library(dplyr)","title":"EUR"},{"location":"Day4b.docx/#read-the-prs-files-into-dataframes","text":"file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F)","title":"Read the PRS files into dataframes"},{"location":"Day4b.docx/#add-the-fourth-column-of-both-files","text":"file1$PRS_SUM <- file1$PRS + file2$PRS","title":"Add the fourth column of both files"},{"location":"Day4b.docx/#load-the-phenotype-data","text":"pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\")","title":"Load the phenotype data"},{"location":"Day4b.docx/#convert-phenotype-column-to-numeric","text":"pheno$phenotype <- as.numeric(pheno$phenotype)","title":"Convert phenotype column to numeric"},{"location":"Day4b.docx/#merge-prs-data-with-phenotype-data","text":"merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\"))","title":"Merge PRS data with phenotype data"},{"location":"Day4b.docx/#rename-columns_1","text":"names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\"","title":"Rename columns"},{"location":"Day4b.docx/#perform-linear-regression-for-each-prs-and-the-combined-prs","text":"model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data)","title":"Perform linear regression for each PRS and the combined PRS"},{"location":"Day4b.docx/#extract-r-squared-values","text":"r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared","title":"Extract R-squared values"},{"location":"Day4b.docx/#print-the-r-squared-values","text":"cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\") ```","title":"Print the R-squared values"},{"location":"Day4b.docx/#questions_1","text":"","title":"Questions"},{"location":"Day4b.docx/#i-how-does-the-r-squared-of-the-combined-ancestry-prs-perform-relative-to-the-2-partial-genome-prss","text":"","title":"(i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?"},{"location":"Day5_Projects/","text":"Integration of Polygenic Risk Scores Task \u2013 PRSmix calculation [3.5 hrs] You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024) Group project data files Group 1 https://github.com/tinashedoc/cvx/blob/main/monodta.txt Group 2 https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt Group 3 https://github.com/tinashedoc/cvx/blob/main/pltdta.txt Group 4 https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Day5"},{"location":"Day5_Projects/#integration-of-polygenic-risk-scores","text":"","title":"Integration of Polygenic Risk Scores"},{"location":"Day5_Projects/#task-prsmix-calculation-35-hrs","text":"You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024)","title":"Task \u2013 PRSmix calculation [3.5 hrs]"},{"location":"Day5_Projects/#group-project-data-files","text":"","title":"Group project data files"},{"location":"Day5_Projects/#group-1","text":"https://github.com/tinashedoc/cvx/blob/main/monodta.txt","title":"Group 1"},{"location":"Day5_Projects/#group-2","text":"https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt","title":"Group 2"},{"location":"Day5_Projects/#group-3","text":"https://github.com/tinashedoc/cvx/blob/main/pltdta.txt","title":"Group 3"},{"location":"Day5_Projects/#group-4","text":"https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Group 4"},{"location":"about_future/","text":"Future Work Coming Soon","title":"Future Work"},{"location":"about_future/#future-work","text":"Coming Soon","title":"Future Work"},{"location":"quik_demo/","text":"Running SibArc: !!! tips \"Running SibArc on a trait with de-novo tail architecture\" From the test directory type: ./sibArc.py ../data/trait1-test.csv --out denovoExample This will produce the following output files: denovoExample.result.out, denovoExample.fig.png, denovoExample.fig.pdf !!! tips \"Running SibArc on a trait with Mendelian tail architecture\" From the test directory type: ./sibArc.py ../data/trait2-test.csv --out mendelianExample This will produce the following output files: mendelianExample.result.out, mendelianExample.fig.png, mendelianExample.fig.pdf","title":"Demos"},{"location":"quik_demo/#running-sibarc","text":"!!! tips \"Running SibArc on a trait with de-novo tail architecture\" From the test directory type: ./sibArc.py ../data/trait1-test.csv --out denovoExample This will produce the following output files: denovoExample.result.out, denovoExample.fig.png, denovoExample.fig.pdf !!! tips \"Running SibArc on a trait with Mendelian tail architecture\" From the test directory type: ./sibArc.py ../data/trait2-test.csv --out mendelianExample This will produce the following output files: mendelianExample.result.out, mendelianExample.fig.png, mendelianExample.fig.pdf","title":"Running SibArc:"},{"location":"quik_install/","text":"Preparation After downloading and unzipping sibArc into a suitable directory on your machine you will observe that a folder with the following contents: sibArc <--- program executable data/ <--- input data LICENSE README.me tests/ <--- test directory !!! tip \"For Mac/Linux, using the terminal, type the following command from within the directory:\" chmod +x sibArc.py to make sibArc executable Input Data SibArc requires two column sibling phenotype data. The columns can be separated by a comma or whitespace, a header is optional: Phenotype1,Phenotype2 -2.23511,-2.33331 -1.23041,-1.03325 -1.55322,-0.03213 0.32353,0.991132 0.53233,2.124533 1.23345,0.936323 2.35326,1.323531 !!! tips \"Sample Sibling Data\" Sample Sibling Data can be found in the data folder data/trait1-test.csv data/trait2-test.csv","title":"Checklist"},{"location":"quik_install/#preparation","text":"After downloading and unzipping sibArc into a suitable directory on your machine you will observe that a folder with the following contents: sibArc <--- program executable data/ <--- input data LICENSE README.me tests/ <--- test directory !!! tip \"For Mac/Linux, using the terminal, type the following command from within the directory:\" chmod +x sibArc.py to make sibArc executable","title":"Preparation"},{"location":"quik_install/#input-data","text":"SibArc requires two column sibling phenotype data. The columns can be separated by a comma or whitespace, a header is optional: Phenotype1,Phenotype2 -2.23511,-2.33331 -1.23041,-1.03325 -1.55322,-0.03213 0.32353,0.991132 0.53233,2.124533 1.23345,0.936323 2.35326,1.323531 !!! tips \"Sample Sibling Data\" Sample Sibling Data can be found in the data folder data/trait1-test.csv data/trait2-test.csv","title":"Input Data"},{"location":"quik_result/","text":"Interpreting SibArc Results !!! tips \"A trait with denovo tail architecture in both tails (denovoExample.fig.png)\" ![Screenshot](img/denovoExample.fig.png) Notice: 1. Evidence of De Novo architecture in both trait tails. 2. Lower heritability in both trait tails. !!! tips \"A trait with Mendelian tail architecture in both tails (mendelianExample.fig.png)\" ![Screenshot](img/mendelianExample.fig.png) Notice: 1. Polygenic architecture in the lower tail, Mendelian heritability in the upper tail. 2. Increased heritability in the upper tail.","title":"Interpreting SibArc Results"},{"location":"quik_result/#interpreting-sibarc-results","text":"!!! tips \"A trait with denovo tail architecture in both tails (denovoExample.fig.png)\" ![Screenshot](img/denovoExample.fig.png) Notice: 1. Evidence of De Novo architecture in both trait tails. 2. Lower heritability in both trait tails. !!! tips \"A trait with Mendelian tail architecture in both tails (mendelianExample.fig.png)\" ![Screenshot](img/mendelianExample.fig.png) Notice: 1. Polygenic architecture in the lower tail, Mendelian heritability in the upper tail. 2. Increased heritability in the upper tail.","title":"Interpreting SibArc Results"},{"location":"course_data/readME/","text":"This is a directory for course data Files can be uploaded via command line or web portal You can upload large files via git-lfs https://git-lfs.github.com","title":"This is a directory for course data"},{"location":"course_data/readME/#this-is-a-directory-for-course-data","text":"Files can be uploaded via command line or web portal You can upload large files via git-lfs https://git-lfs.github.com","title":"This is a directory for course data"},{"location":"group_projects/gpbase/","text":"Please add your code from the group projects here as a .Rmd","title":"Gpbase"},{"location":"images/base/","text":"A place to put the images for the course","title":"Base"},{"location":"images/Day2.docxfolder/Day2.docx/","text":"Polygenic Risk Score Analyses Workshop 2022 {width=\"2.939998906386702in\" height=\"2.8874989063867016in\"} Day 2: Introduction to Polygenic Risk Scores Day 2 Timetable {#day-2-timetable .unnumbered} +------------+--------+-------------------------+---------------------+ | > Time | > T | | > Presenter | | | itle | | | +============+========+=========================+=====================+ | > 9:00 - | A | | > - | | > 9:15 | rrival | | | +------------+--------+-------------------------+---------------------+ | > 9:15 - | [Lec | > Introduction to PRS I | > Dr Paul O'Reilly | | > 10:30 | ture]{ | | | | | .under | | | | | line}: | | | +------------+--------+-------------------------+---------------------+ | 10:30 - | > | | > - | | 11:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 11:00 - | > [Lec | | > Dr Paul O'Reilly | | 12:00 | ture]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 12:00 - | > | | > - | | 13:30 | Lunch | | | +------------+--------+-------------------------+---------------------+ | 13:30 - | > | | > Dr Conrad Iyegbe | | 14:30 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > I | | | +------------+--------+-------------------------+---------------------+ | 14:30 - | > | | > - | | 15:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 15:00 - | > | | > Dr Conrad Iyegbe | | 16:00 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 16:00 - | > [S | | > Dr Nicki Tiffin | | 17:00 | pecial | | | | | > Sem | | | | | inar]{ | | | | | .under | | | | | line}: | | | | | > Pol | | | | | ygenic | | | | | > Risk | | | | | > | | | | | Scores | | | | | > | | | | | > + | | | | | > and | | | | | > | | | | | Ethics | | | +------------+--------+-------------------------+---------------------+ Contents Day 2 Timetable 1 Day 2 Timetable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Introduction to Polygenic Score > Analyses 3 Key Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . 3 Resources you will be using . . . . . . . . . . . . . . . . . . . . . . 3 Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Understanding GWAS Summary Statistics . . . . . . . . . . . . . . 5 Matching the Base and Target Data sets . . . . . . . . . . . . . . . 6 Linkage Disequilibrium in PRS Analyses . . . . . . . . . . . . . . . 7 Performing Clumping . . . . . . . . . . . . . . . . . . . . . . 7 P-Value Thresholding . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Height PRS using GW-significant SNPs only . . . . . . . . . 8 Height PRS across multiple P-value thresholds 10 High Resolution Scoring 12 Stratifying Samples by PRS 15 Case Control Studies 17 Cross-Trait Analysis 18 Introduction to Polygenic Score Analyses Key Learning Outcomes After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice (Euesden, Lewis & O'Reilly 2015; Choi & O'Reilly 2019) Interpret results generated from PRS analyses Customise visualisation of results Resources you will be using To perform PRS analyses, summary statistics from Genome Wide Association Stud- ies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals ( wood_defining_2014 ) Download Link Coronary artery disease (CAD) CARDIoGRAM plus C4D consortium GWAS on 60,801 CAD cases and 123,504 controls Download Link (consortium_comprehensive_2015) {#consortium_comprehensive_2015 .unnumbered} Data Structure You will find all practical materials in the PRS_Workshop/Day_2 directory. Relevant materials that you should see there at the start of the practical are as follows: Practical Base_Data GIANT_Height.txt cad.add.txt cad.add.readme Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.cad TAR.covariate Software plink_mac plink_linux plink.exe PRSice.R PRSice_mac PRSice_linux PRSice_win64.exe {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"} Introduction A PRS is a (usually weak) estimate of an individual's genetic propensity to a pheno- type, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS. Understanding GWAS Summary Statistics When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymor- phism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Matching the Base and Target Data sets The first step in PRS calculation is to ensure consistency between the GWAS sum- mary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs - and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed, where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"} Linkage Disequilibrium in PRS Analyses GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes iden- tifying the independent genetic effects (or their best proxies if these are not geno- typed/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of meth- ods using the different options to date ( mak_polygenic_2017 ). In this work- shop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LD- pred ( vilhjalmsson_modeling_2015 ) and lassosum ( mak_polygenic_2017 ) papers. Performing Clumping Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f^2^ > 0.1, with \ud835\udc5f^2^ typically calculated from \ud835\udc5d\u210e\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \u210e\ud835\udc4e\ud835\udc5d\ud835\udc59\ud835\udc5c\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52 data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( chang_second_2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: 1 cd \\~/Desktop/PRS\\_Workshop/ Next type the following command (NB. See warning below): 1 ./Software/plink_linux 2 --bfile Target_Data/TAR 3 --clump Base_Data/GIANT_Height.txt 4 --clump-p1 1 5 --clump-snp-field MarkerName 6 --clump-field p 7 --clump-kb 250 8 --clump-r2 0.1 9 --out Results/Height {width=\"0.3229155730533683in\" height=\"0.3229166666666667in\"} The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f^2^ > 0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} P-Value Thresholding Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43 -value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43 -value thresholds. Height PRS using GW-significant SNPs only Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the PRS_Workshop/Day_2 directory, run the following command in the terminal: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --bar-levels 5e-8 14 --no-full 15 --fastscore 16 --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID (--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43 -value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addi- tion, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43 -value \\< 5 \u00d7 10^ \u2212 8^. {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the em- pirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"} Height PRS across multiple P-value thresholds A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43 -value threshold provides the \\\"best\\\" predic- tion for our particular data, then we can calculate the PRS under several \ud835\udc43 -value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. []{#_bookmark13 .anchor}Figure 1.1: BARPLOT generated by PRSice {width=\"2.7440616797900264in\" height=\"2.761874453193351in\"} See dudbridge_power_2013 for theory on factors affecting the best-fit PRS). This process is implemented in PRSice and can be performed automatically as fol- lows: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --fastscore 15 --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT (fig. 1.1) generated by PRSice {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} High Resolution Scoring If we limit ourselves to a small number of \ud835\udc43 -value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a large number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot (fig. 1.2) presenting the model fit of PRS calculated at all P-value thresholds. []{#_bookmark15 .anchor}Figure 1.2: High Resolution Plot generated by PRSice {width=\"2.463332239720035in\" height=\"2.455in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user in- puts, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --out Results/Height.sex When covariates were include in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43 \ud835\udc45\ud835\udc46.\ud835\udc45^2^ is calculated by minusing the \ud835\udc45^2^ of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45^2^ of the full model (e.g. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"}\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43 \ud835\udc45\ud835\udc46). Usually, with categorical variables, dummy variables have to be generated to rep- resent the different categories. Alternatively, residualized phenotype, generated by regresing the covariates against the phenotype, can be used for downstream anal- yses. A useful feature of PRSice is to automatically generate the dummy variable for users. This can be achieved with the following command: 1 Rscript.exe ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --cov-factor Sex 16 --out Results/Height.sex []{#_bookmark16 .anchor}Figure 1.3: Example of a quantile plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"} Stratifying Samples by PRS An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot (fig. 1.3). To generate quantile plots in PRSice, simply add --quantile 10 option. {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 10 17 --out Results/Height.sex {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} Figure 1.4: Example of a strata plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"} A disadvantage of the quantile plot is that it only seperate samples into quantiles of equal size. However, it is somtimes interesting to investigate whether a specific strata (e.g. top 5% of samples), contain a higher PRS than the reference strata. For example, mavaddat_prediction_2015 found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break, which represents the upper bound of each strata, and --quant-ref, which represents the upper bound of the reference quantile: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 100 17 --quant-break 1,5,10,20,40,60,80,90,95,99,100 18 --quant-ref 60 19 --out Results/Height.sex {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} Case Control Studies In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Tar- get_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/cad.add.txt 4 --target Target_Data/TAR 5 --snp markername 6 --A1 effect_allele 7 --A2 noneffect_allele 8 --chr chr 9 --bp bp_hg19 10 --stat beta 11 --beta 12 --pvalue p_dgc 13 --pheno Target_Data/CAD.pheno 14 --binary-target T 15 --out Results/CAD.highres {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"3.5316666666666667in\" height=\"2.066457786526684in\"} []{#_bookmark19 .anchor}Figure 1.5: Plot taken from Ruderfer et al. 2014 {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Cross-Trait Analysis A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ruderfer_polygenic_2014 (fig. 1.5), which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/CAD.pheno 12 --binary-target T 13 --out Results/Cross.highres {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Day2.docx"},{"location":"images/Day2.docxfolder/Day2.docx/#day-2-timetable-day-2-timetable-unnumbered","text":"+------------+--------+-------------------------+---------------------+ | > Time | > T | | > Presenter | | | itle | | | +============+========+=========================+=====================+ | > 9:00 - | A | | > - | | > 9:15 | rrival | | | +------------+--------+-------------------------+---------------------+ | > 9:15 - | [Lec | > Introduction to PRS I | > Dr Paul O'Reilly | | > 10:30 | ture]{ | | | | | .under | | | | | line}: | | | +------------+--------+-------------------------+---------------------+ | 10:30 - | > | | > - | | 11:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 11:00 - | > [Lec | | > Dr Paul O'Reilly | | 12:00 | ture]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 12:00 - | > | | > - | | 13:30 | Lunch | | | +------------+--------+-------------------------+---------------------+ | 13:30 - | > | | > Dr Conrad Iyegbe | | 14:30 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > I | | | +------------+--------+-------------------------+---------------------+ | 14:30 - | > | | > - | | 15:00 | Coffee | | | | | > | | | | | Break | | | | | > and | | | | | > Q&A | | | +------------+--------+-------------------------+---------------------+ | 15:00 - | > | | > Dr Conrad Iyegbe | | 16:00 | [Pract | | > & Tutors | | | ical]{ | | | | | .under | | | | | line}: | | | | | > | | | | | Introd | | | | | uction | | | | | > to | | | | | > PRS | | | | | > II | | | +------------+--------+-------------------------+---------------------+ | 16:00 - | > [S | | > Dr Nicki Tiffin | | 17:00 | pecial | | | | | > Sem | | | | | inar]{ | | | | | .under | | | | | line}: | | | | | > Pol | | | | | ygenic | | | | | > Risk | | | | | > | | | | | Scores | | | | | > | | | | | > + | | | | | > and | | | | | > | | | | | Ethics | | | +------------+--------+-------------------------+---------------------+ Contents Day 2 Timetable 1 Day 2 Timetable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Introduction to Polygenic Score > Analyses 3 Key Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . 3 Resources you will be using . . . . . . . . . . . . . . . . . . . . . . 3 Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Understanding GWAS Summary Statistics . . . . . . . . . . . . . . 5 Matching the Base and Target Data sets . . . . . . . . . . . . . . . 6 Linkage Disequilibrium in PRS Analyses . . . . . . . . . . . . . . . 7 Performing Clumping . . . . . . . . . . . . . . . . . . . . . . 7 P-Value Thresholding . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Height PRS using GW-significant SNPs only . . . . . . . . . 8 Height PRS across multiple P-value thresholds 10 High Resolution Scoring 12 Stratifying Samples by PRS 15 Case Control Studies 17 Cross-Trait Analysis 18","title":"Day 2 Timetable {#day-2-timetable .unnumbered}"},{"location":"images/Day2.docxfolder/Day2.docx/#introduction-to-polygenic-score-analyses","text":"","title":"Introduction to Polygenic Score Analyses"},{"location":"images/Day2.docxfolder/Day2.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice (Euesden, Lewis & O'Reilly 2015; Choi & O'Reilly 2019) Interpret results generated from PRS analyses Customise visualisation of results","title":"Key Learning Outcomes"},{"location":"images/Day2.docxfolder/Day2.docx/#resources-you-will-be-using","text":"To perform PRS analyses, summary statistics from Genome Wide Association Stud- ies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals ( wood_defining_2014 ) Download Link Coronary artery disease (CAD) CARDIoGRAM plus C4D consortium GWAS on 60,801 CAD cases and 123,504 controls Download Link","title":"Resources you will be using"},{"location":"images/Day2.docxfolder/Day2.docx/#consortium_comprehensive_2015-consortium_comprehensive_2015-unnumbered","text":"","title":"(consortium_comprehensive_2015) {#consortium_comprehensive_2015 .unnumbered}"},{"location":"images/Day2.docxfolder/Day2.docx/#data-structure","text":"You will find all practical materials in the PRS_Workshop/Day_2 directory. Relevant materials that you should see there at the start of the practical are as follows: Practical Base_Data GIANT_Height.txt cad.add.txt cad.add.readme Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.cad TAR.covariate Software plink_mac plink_linux plink.exe PRSice.R PRSice_mac PRSice_linux PRSice_win64.exe {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"}","title":"Data Structure"},{"location":"images/Day2.docxfolder/Day2.docx/#introduction","text":"A PRS is a (usually weak) estimate of an individual's genetic propensity to a pheno- type, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS.","title":"Introduction"},{"location":"images/Day2.docxfolder/Day2.docx/#understanding-gwas-summary-statistics","text":"When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymor- phism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Understanding GWAS Summary Statistics"},{"location":"images/Day2.docxfolder/Day2.docx/#matching-the-base-and-target-data-sets","text":"The first step in PRS calculation is to ensure consistency between the GWAS sum- mary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs - and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed, where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. {width=\"0.3229166666666667in\" height=\"0.3229166666666667in\"}","title":"Matching the Base and Target Data sets"},{"location":"images/Day2.docxfolder/Day2.docx/#linkage-disequilibrium-in-prs-analyses","text":"GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes iden- tifying the independent genetic effects (or their best proxies if these are not geno- typed/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of meth- ods using the different options to date ( mak_polygenic_2017 ). In this work- shop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LD- pred ( vilhjalmsson_modeling_2015 ) and lassosum ( mak_polygenic_2017 ) papers.","title":"Linkage Disequilibrium in PRS Analyses"},{"location":"images/Day2.docxfolder/Day2.docx/#performing-clumping","text":"Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f^2^ > 0.1, with \ud835\udc5f^2^ typically calculated from \ud835\udc5d\u210e\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \u210e\ud835\udc4e\ud835\udc5d\ud835\udc59\ud835\udc5c\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52 data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( chang_second_2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: 1 cd \\~/Desktop/PRS\\_Workshop/ Next type the following command (NB. See warning below): 1 ./Software/plink_linux 2 --bfile Target_Data/TAR 3 --clump Base_Data/GIANT_Height.txt 4 --clump-p1 1 5 --clump-snp-field MarkerName 6 --clump-field p 7 --clump-kb 250 8 --clump-r2 0.1 9 --out Results/Height {width=\"0.3229155730533683in\" height=\"0.3229166666666667in\"} The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f^2^ > 0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Performing Clumping"},{"location":"images/Day2.docxfolder/Day2.docx/#p-value-thresholding","text":"Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43 -value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43 -value thresholds.","title":"P-Value Thresholding"},{"location":"images/Day2.docxfolder/Day2.docx/#height-prs-using-gw-significant-snps-only","text":"Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the PRS_Workshop/Day_2 directory, run the following command in the terminal: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --bar-levels 5e-8 14 --no-full 15 --fastscore 16 --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID (--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43 -value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addi- tion, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43 -value \\< 5 \u00d7 10^ \u2212 8^. {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the em- pirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"}","title":"Height PRS using GW-significant SNPs only"},{"location":"images/Day2.docxfolder/Day2.docx/#height-prs-across-multiple-p-value-thresholds","text":"A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43 -value threshold provides the \\\"best\\\" predic- tion for our particular data, then we can calculate the PRS under several \ud835\udc43 -value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. []{#_bookmark13 .anchor}Figure 1.1: BARPLOT generated by PRSice {width=\"2.7440616797900264in\" height=\"2.761874453193351in\"} See dudbridge_power_2013 for theory on factors affecting the best-fit PRS). This process is implemented in PRSice and can be performed automatically as fol- lows: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --fastscore 15 --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT (fig. 1.1) generated by PRSice {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Height PRS across multiple P-value thresholds"},{"location":"images/Day2.docxfolder/Day2.docx/#high-resolution-scoring","text":"If we limit ourselves to a small number of \ud835\udc43 -value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a large number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: 1 Rscript ./Software/PRSice.R 2 --dir . 3 --prsice Software/PRSice_linux 4 --base Base_Data/GIANT_Height.txt 5 --target Target_Data/TAR 6 --snp MarkerName 7 --A1 Allele1 8 --A2 Allele2 9 --stat b 10 --beta 11 --pvalue p 12 --pheno Target_Data/TAR.height 13 --binary-target F 14 --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot (fig. 1.2) presenting the model fit of PRS calculated at all P-value thresholds. []{#_bookmark15 .anchor}Figure 1.2: High Resolution Plot generated by PRSice {width=\"2.463332239720035in\" height=\"2.455in\"} {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"} Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user in- puts, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --out Results/Height.sex When covariates were include in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43 \ud835\udc45\ud835\udc46.\ud835\udc45^2^ is calculated by minusing the \ud835\udc45^2^ of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45^2^ of the full model (e.g. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"0.3541666666666667in\" height=\"0.3593744531933508in\"}\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43 \ud835\udc45\ud835\udc46). Usually, with categorical variables, dummy variables have to be generated to rep- resent the different categories. Alternatively, residualized phenotype, generated by regresing the covariates against the phenotype, can be used for downstream anal- yses. A useful feature of PRSice is to automatically generate the dummy variable for users. This can be achieved with the following command: 1 Rscript.exe ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --cov-factor Sex 16 --out Results/Height.sex []{#_bookmark16 .anchor}Figure 1.3: Example of a quantile plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"}","title":"High Resolution Scoring"},{"location":"images/Day2.docxfolder/Day2.docx/#stratifying-samples-by-prs","text":"An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot (fig. 1.3). To generate quantile plots in PRSice, simply add --quantile 10 option. {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 10 17 --out Results/Height.sex {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} Figure 1.4: Example of a strata plot generated by PRSice {width=\"2.4641655730533683in\" height=\"2.4674989063867017in\"} A disadvantage of the quantile plot is that it only seperate samples into quantiles of equal size. However, it is somtimes interesting to investigate whether a specific strata (e.g. top 5% of samples), contain a higher PRS than the reference strata. For example, mavaddat_prediction_2015 found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break, which represents the upper bound of each strata, and --quant-ref, which represents the upper bound of the reference quantile: 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/TAR.height 12 --binary-target F 13 --cov Target_Data/TAR.covariate 14 --cov-col Sex 15 --plot 16 --quantile 100 17 --quant-break 1,5,10,20,40,60,80,90,95,99,100 18 --quant-ref 60 19 --out Results/Height.sex {width=\"0.3281244531933508in\" height=\"0.3281244531933508in\"}","title":"Stratifying Samples by PRS"},{"location":"images/Day2.docxfolder/Day2.docx/#case-control-studies","text":"In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Tar- get_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/cad.add.txt 4 --target Target_Data/TAR 5 --snp markername 6 --A1 effect_allele 7 --A2 noneffect_allele 8 --chr chr 9 --bp bp_hg19 10 --stat beta 11 --beta 12 --pvalue p_dgc 13 --pheno Target_Data/CAD.pheno 14 --binary-target T 15 --out Results/CAD.highres {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} {width=\"3.5316666666666667in\" height=\"2.066457786526684in\"} []{#_bookmark19 .anchor}Figure 1.5: Plot taken from Ruderfer et al. 2014 {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Case Control Studies"},{"location":"images/Day2.docxfolder/Day2.docx/#cross-trait-analysis","text":"A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ruderfer_polygenic_2014 (fig. 1.5), which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. {width=\"0.34781167979002625in\" height=\"0.34781167979002625in\"} 1 Rscript ./Software/PRSice.R 2 --prsice Software/PRSice_linux 3 --base Base_Data/GIANT_Height.txt 4 --target Target_Data/TAR 5 --snp MarkerName 6 --A1 Allele1 7 --A2 Allele2 8 --stat b 9 --beta 10 --pvalue p 11 --pheno Target_Data/CAD.pheno 12 --binary-target T 13 --out Results/Cross.highres {width=\"0.3541655730533683in\" height=\"0.3593744531933508in\"}","title":"Cross-Trait Analysis"},{"location":"images/day3/base/","text":"base","title":"Base"},{"location":"modules/Day1a.docx/","text":"Polygenic Risk Score Analyses Workshop 2024 Day 1a: GWAS & relevant Statistics Introduction to Bash Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment. Moving around the File System To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/ Looking at the Current Directory Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size Counting Number of Lines in File We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt Search File Content Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l Filtering and Reshu\ufb04ing Files A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if($7<5e-8)\" and \"$7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met. Introduction to R R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop. Basics If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R Working Directory When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\") Libraries Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet). Variables in R You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\") Functions You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2) Plotting While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point() Regression Models In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Day 1a"},{"location":"modules/Day1a.docx/#polygenic-risk-score-analyses-workshop-2024","text":"","title":"Polygenic Risk Score Analyses Workshop 2024"},{"location":"modules/Day1a.docx/#day-1a-gwas-relevant-statistics","text":"","title":"Day 1a: GWAS &amp; relevant Statistics"},{"location":"modules/Day1a.docx/#introduction-to-bash","text":"Most software in Bioinformatics and Statistical Genetics need to be run in a Unix environment (e.g. Linux or Mac OS) and most high-performance computer clusters run Unix systems. Therefore, although there are alternatives available on Windows (command line, Linux subsystems or Virtual Machines), it will be highly beneficial to become familiar with performing research in a Unix-only environment.","title":"Introduction to Bash"},{"location":"modules/Day1a.docx/#moving-around-the-file-system","text":"To begin our practical, please open up a \\\"terminal\\\" on your computer (on a Mac this is stored in Applications/Utilities/). We can change our directory using the following command: cd \\<Path>\\ where *\\ * is the path to the target directory. Some common usage of cd includes cd ~/ # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd XXX # will bring you to the XXX directory, so long as it is in the current directory As an example, we can move to the data directory by typing: cd data/","title":"Moving around the File System"},{"location":"modules/Day1a.docx/#looking-at-the-current-directory","text":"Next we can move into the ~/data/Day1a_Data/Day1a_Data folder (from the data/ folder type: cd Day1a_Data/Day1a_Data). We can list out the folder content by typing: ls For ls, there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size","title":"Looking at the Current Directory"},{"location":"modules/Day1a.docx/#counting-number-of-lines-in-file","text":"We can also count the number of lines in a file with the following command (where *\\ * is the file of interest): wc -l <file> Often we would like to store the output of a command, which we can do by redirecting the output of the command to a file. For example, we can redirect the count of the GIANT_Height.txt to giant_count using the following command: wc -l GIANT_Height.txt > giant_count.txt","title":"Counting Number of Lines in File"},{"location":"modules/Day1a.docx/#search-file-content","text":"Another common task is to search for specific words or characters in a file (e.g. does this file contain our gene of interest?). This can be performed using the \"grep\" command as follows: grep <string> file For example, to check if the Single Nucleotide Polymorphism (SNP) rs10786427 is present in GIANT_Height.txt , we can do: grep rs10786427 GIANT_Height.txt In addition, grep allows us to check if patterns contained in one file can be found in another file. For example, if we want to extract a subset of samples from the phenotype file (e.g. extract the list of samples in Data/Day_1a/TAR.height ), we can do: grep -f Select.sample TAR.height An extremely useful feature of the terminal is chaining multiple commands into one command, which we call piping . For example, we can use piping to count the number of samples in Select.sample that were found in TAR.height in a single command, as follows: bash grep -f Select.sample TAR.height | wc -l","title":"Search File Content"},{"location":"modules/Day1a.docx/#filtering-and-reshuffling-files","text":"A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by per- forming operations on its columns - this is extremely useful for scientific data sets because typically the columns features or variables of interest. For example, we can use awk to produce a new results file that only contains SNP rsIDs (column 1), allele frequencies (column 4) and P -values (column 7) as follows: awk '{ print $1,$4,$7}' GIANT_Height.txt > GIANT_Height_3cols.txt We can also use a \\\"conditional statement\\\" in awk to extract all significant [SNPs] from the results file, using the following command: awk '{if($7 < 5e-8) { print } }' GIANT_Height.txt > Significant_SNPs.txt Or the short form: awk '$7 < 5e-8{ print}' GIANT_Height.txt > Significant_SNPs.txt \"if($7<5e-8)\" and \"$7 < 5e-8\" tell awk to extract any rows with column 7 (the column containing P -value) with a value of smaller than 5e-8 and {print} means that we would like to print the entire row when this criterion is met.","title":"Filtering and Reshu\ufb04ing Files"},{"location":"modules/Day1a.docx/#introduction-to-r","text":"R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop.","title":"Introduction to R"},{"location":"modules/Day1a.docx/#basics","text":"If you are not using R Studio then you can type R in your terminal to run R in the terminal. ## Adding script to working dir cd ~/data/Day1a_Data/Day1a_Data wget https://raw.githubusercontent.com/WCSCourses/prs_2023/main/scripts/nagelkerke.R","title":"Basics"},{"location":"modules/Day1a.docx/#working-directory","text":"When we start R , we will be working in a specific folder called the working directory . We can check the current/working directory we are in by typing: getwd() And we can change our working directory to the Practical folder by setwd(\"~/data/Day1a_Data/Day1a_Data\")","title":"Working Directory"},{"location":"modules/Day1a.docx/#libraries","text":"Most functionality of R is organised in \\\"packages\\\" or \\\"libraries\\\". To access these functions, we will have to install and \\\"load\\\" these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Alternatively, we can import functions (e.g. that we have written) from an R script file on our computer. For example, you can load the Nagelkerke R2 function by typing source(\"nagelkerke.R\") And you are now able to use the Nagelkerke R2 function (we will use this function at the end of this worksheet).","title":"Libraries"},{"location":"modules/Day1a.docx/#variables-in-r","text":"You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\")","title":"Variables in R"},{"location":"modules/Day1a.docx/#functions","text":"You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2)","title":"Functions"},{"location":"modules/Day1a.docx/#plotting","text":"While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point()","title":"Plotting"},{"location":"modules/Day1a.docx/#regression-models","text":"In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coe\ufb03cient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial)) We will need the NagelkerkeR2 function to calculate the pseudo R2 for logistic model source(\"nagelkerke.R\") reg <- glm(y~x, family=binomial) Calculate the Nagelkerke R2 using the NagelkerkeR2 function NagelkerkeR2(reg)","title":"Regression Models"},{"location":"modules/Day1b.docx/","text":"Polygenic Risk Score Analyses Workshop 2024 Practical 1 Introduction to PLINK I: basics Key Learning Outcomes After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website 4. Select and exclude lists of samples and SNPs \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Introduction PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials. Command line basics In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click ) Let's begin Open up a terminal Navigate to the Day 1b working directory cd ~/data/Data_Day1b/ List all files in this directory by typing ls Test PLINK with no input by typing plink Note that you can see lots of PLINK options by using the built-in help function: plink --help \ud83d\udcdd Calling PLINK with no output will test if PLINK is installed and available in the directory, because you should see some output showing the PLINK license and some commands. If you do not see this, then please ask for help now! Exploring Data Sets Open an Explorer window ('Finder' on a Mac) and navigate to your PLINK working directory. \ud83d\udcdd An explorer window should show the same files as the ls command Open the file called 'D1D.map' with a Text Editor e.g. by typing right-click > Open . Open the file 'D1D.ped'. Note this is a large file - if it will not open or is very slow, skip this step. Go to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side) What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file? Create 'binary' format PLINK files using the recode command: plink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Open and examine files ending .bim and .fam. Do not open the .bed file. Open and skim the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary) Recoding alleles as counts Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be? PLINK website Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists. Write SNP list and extract SNPs You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!! Practical 2 Introduction to PLINK II: Performing QC & GWAS Key Learning Outcomes After completing this practical, you should be able to: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS Generate summaries to perform QC There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC. Individual missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"? SNP Missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful? Hardy-Weinberg Equilibrium Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why? Allele frequencies Generate allele frequencies using the command -- freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way Apply QC filters There are di\ufb00erent strategies for performing QC on your data: (a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice. Apply individual missingness thresholds Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening? Apply SNP missingness and MAF thresholds Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness? Apply Hardy-Weinberg thresholds Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP. Perform GWAS Case/Control GWAS - no covariates Run the following code, which performs a genetic association study using logistic regression on some case/control data: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations? Case/Control GWAS - with covariates Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs? License This work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode You are free to: Share --- copy and redistribute the material in any medium or format Adapt --- remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: {#under-the-following-terms .unnumbered} Attribution --- You must give appropriate credit, providea link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial --- You may not use the material for commercial purposes. ShareAlike --- If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions --- You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Day1b.docx"},{"location":"modules/Day1b.docx/#practical-1","text":"","title":"Practical 1"},{"location":"modules/Day1b.docx/#introduction-to-plink-i-basics","text":"","title":"Introduction to PLINK I: basics"},{"location":"modules/Day1b.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website","title":"Key Learning Outcomes"},{"location":"modules/Day1b.docx/#4-select-and-exclude-lists-of-samples-and-snps","text":"\u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only.","title":"4.  Select and exclude lists of samples and SNPs"},{"location":"modules/Day1b.docx/#introduction","text":"PLINK is the most popular software program for performing genome-wide association analyses it is extremely extensive, allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials.","title":"Introduction"},{"location":"modules/Day1b.docx/#command-line-basics","text":"In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click )","title":"Command line basics"},{"location":"modules/Day1b.docx/#lets-begin","text":"Open up a terminal Navigate to the Day 1b working directory cd ~/data/Data_Day1b/ List all files in this directory by typing ls Test PLINK with no input by typing plink Note that you can see lots of PLINK options by using the built-in help function: plink --help \ud83d\udcdd Calling PLINK with no output will test if PLINK is installed and available in the directory, because you should see some output showing the PLINK license and some commands. If you do not see this, then please ask for help now!","title":"Let's begin"},{"location":"modules/Day1b.docx/#exploring-data-sets","text":"Open an Explorer window ('Finder' on a Mac) and navigate to your PLINK working directory. \ud83d\udcdd An explorer window should show the same files as the ls command Open the file called 'D1D.map' with a Text Editor e.g. by typing right-click > Open . Open the file 'D1D.ped'. Note this is a large file - if it will not open or is very slow, skip this step. Go to the PLINK website http://zzz.bwh.harvard.edu/plink/download.shtml and investigate the format of the MAP/PED files (Look in the blue column on the left side) What do you observe? - What are the 4 columns in the map file? - What are the first 6 columns in a ped file? - What information is in the remaining columns of the ped file? Create 'binary' format PLINK files using the recode command: plink --file D1D --make-bed --out D1D List files ( ls ) and check which new files have appeared Open and examine files ending .bim and .fam. Do not open the .bed file. Open and skim the '.log' file. What do you observe? How is the fam file similar to the ped file? How is it different? How is the bim file similar to the map file? How is it different? (Use the PLINK website if necessary)","title":"Exploring Data Sets"},{"location":"modules/Day1b.docx/#recoding-alleles-as-counts","text":"Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be?","title":"Recoding alleles as counts"},{"location":"modules/Day1b.docx/#plink-website","text":"Go to http://zzz.bwh.harvard.edu/plink/download.shtml and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Data Management', click 'Write SNP list' and read the instructions there to write SNP lists.","title":"PLINK website"},{"location":"modules/Day1b.docx/#write-snp-list-and-extract-snps","text":"You will now use the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!!","title":"Write SNP list and extract SNPs"},{"location":"modules/Day1b.docx/#practical-2","text":"","title":"Practical 2"},{"location":"modules/Day1b.docx/#introduction-to-plink-ii-performing-qc-gwas","text":"","title":"Introduction to PLINK II: Performing QC &amp; GWAS"},{"location":"modules/Day1b.docx/#key-learning-outcomes_1","text":"After completing this practical, you should be able to: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS","title":"Key Learning Outcomes"},{"location":"modules/Day1b.docx/#generate-summaries-to-perform-qc","text":"There are many kinds of summaries of the data that can generated in PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC.","title":"Generate summaries to perform QC"},{"location":"modules/Day1b.docx/#individual-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"?","title":"Individual missingness"},{"location":"modules/Day1b.docx/#snp-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful?","title":"SNP Missingness"},{"location":"modules/Day1b.docx/#hardy-weinberg-equilibrium","text":"Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why?","title":"Hardy-Weinberg Equilibrium"},{"location":"modules/Day1b.docx/#allele-frequencies","text":"Generate allele frequencies using the command -- freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way","title":"Allele frequencies"},{"location":"modules/Day1b.docx/#apply-qc-filters","text":"","title":"Apply QC filters"},{"location":"modules/Day1b.docx/#there-are-different-strategies-for-performing-qc-on-your-data","text":"(a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice.","title":"There are di\ufb00erent strategies for performing QC on your data:"},{"location":"modules/Day1b.docx/#apply-individual-missingness-thresholds","text":"Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening?","title":"Apply individual missingness thresholds"},{"location":"modules/Day1b.docx/#apply-snp-missingness-and-maf-thresholds","text":"Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness?","title":"Apply SNP missingness and MAF thresholds"},{"location":"modules/Day1b.docx/#apply-hardy-weinberg-thresholds","text":"Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP.","title":"Apply Hardy-Weinberg thresholds"},{"location":"modules/Day1b.docx/#perform-gwas","text":"","title":"Perform GWAS"},{"location":"modules/Day1b.docx/#casecontrol-gwas-no-covariates","text":"Run the following code, which performs a genetic association study using logistic regression on some case/control data: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations?","title":"Case/Control GWAS - no covariates"},{"location":"modules/Day1b.docx/#casecontrol-gwas-with-covariates","text":"Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs.1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Case/Control GWAS - with covariates"},{"location":"modules/Day1b.docx/#license","text":"This work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode","title":"License"},{"location":"modules/Day1b.docx/#you-are-free-to","text":"Share --- copy and redistribute the material in any medium or format Adapt --- remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to:"},{"location":"modules/Day1b.docx/#under-the-following-terms-under-the-following-terms-unnumbered","text":"Attribution --- You must give appropriate credit, providea link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial --- You may not use the material for commercial purposes. ShareAlike --- If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions --- You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms: {#under-the-following-terms .unnumbered}"},{"location":"modules/Day1b.docx/#notices","text":"You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Notices:"},{"location":"modules/Day2.docx/","text":"Introduction to Polygenic Risk Scores Table of Contents Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents Key Learning Outcomes After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results Resources you will be using To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link Data Structure You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents Introduction A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS. Understanding GWAS Summary Statistics When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents Matching the Base and Target Data sets The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents Linkage Disequilibrium in PRS Analyses GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents Performing Clumping Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents P-Value Thresholding Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds. Height PRS using GW-significant SNPs only Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5 \u00d7 10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5 \u00d7 10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents Height PRS across multiple P-value thresholds A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents High Resolution Scoring If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5 \u00d7 10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents Stratifying Samples by PRS An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents Case Control Studies In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres \u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status. Cross-Trait Analysis A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Introduction to Polygenic Risk Scores"},{"location":"modules/Day2.docx/#introduction-to-polygenic-risk-scores","text":"","title":"Introduction to Polygenic Risk Scores"},{"location":"modules/Day2.docx/#table-of-contents","text":"Key Learning Outcomes Resources you will be using Data Structure Introduction Understanding GWAS Summary Statistics Matching the Base and Target Data sets Linkage Disequilibrium in PRS Analyses Performing Clumping P-Value Thresholding Height PRS using GW-significant SNPs only Height PRS across multiple P-value thresholds High Resolution Scoring Stratifying Samples by PRS Case Control Studies Cross-Trait Analysis Back to Table of Contents","title":"Table of Contents"},{"location":"modules/Day2.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results","title":"Key Learning Outcomes"},{"location":"modules/Day2.docx/#resources-you-will-be-using","text":"To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link","title":"Resources you will be using"},{"location":"modules/Day2.docx/#data-structure","text":"You will find all practical materials in the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory. Relevant materials that you should see there at the start of the practical are as follows: First, download the Base datasets using this command - cd ~/Downloads wget https://wcs_data_transfer.cog.sanger.ac.uk/Day2_Base_Data.zip You may also download it via dropbox if the above fails with this link . The data will be downloaded into your \"Downloads\" folder. You will need to move it to right directory, using the following command. cd ~/data/Day2_Target_Data/Day2_Target_Data mv ~/Downloads/Day2_Base_Data.zip ~/data/Day2_Target_Data/Day2_Target_Data Now, your Day2_Base_dat.zip has been moved to your Day2_Target_Data directory. However, the file is zipped so you will need to unzip it: unzip Day2_Base_Data.zip For clarity purposes, rename your Day2_Base_data to Base_data and make a directory for your Target data called \"Target_data\" in which you will move all your target datasets. mv Day2_Base_Data Base_Data mkdir Target_Data mv TAR.* Target_Data You also want to create a \"Results\" directory where all your results will be saved mkdir Results So now in your current working directory (for Day2), you should have 3 directories (in blue) called Base_data, Target_data, and Results \ud83d\udcc2: Base_Data GIANT_Height.txt, cad.add.txt, cad.add.readme. \ud83d\udcc2: Target_Data - TAR.fam - TAR.bim - TAR.bed - TAR.height - TAR.cad - TAR.covariate \ud83d\udee0\ufe0f: Software - plink_mac - plink_linux - plink.exe - PRSice.R - PRSice_mac - PRSice_linux - PRSice_win64.exe \u203c\ufe0f All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Table of Contents","title":"Data Structure"},{"location":"modules/Day2.docx/#introduction","text":"A PRS is a (usually weak) estimate of an individual's genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS.","title":"Introduction"},{"location":"modules/Day2.docx/#understanding-gwas-summary-statistics","text":"When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient (\ud835\udefd) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \ud835\udefd coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \ud835\udefd coefficient reflects how much the phenotype changes for each G allele present (NB. The \ud835\udefd can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) \ud83d\udcdc The relationship between the \ud835\udefd coefficient from the logistic regression and the OR is: OR = e \ud835\udefd and log e (OR) = \ud835\udefd. While GWAS usually convert from the \ud835\udefd to the OR when reporting results, most PRS software convert OR back to \ud835\udefd's(\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc52 (\ud835\udc42\ud835\udc45)) to allow simple addition. \ud83d\udcdc Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. \ud83d\udd0e Let us open the Height GWAS file ( GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). Answer In the GIANT_Height.txt, SNPs effect sizes are reported as \ud835\udefd coefficient and measures the effect of the 'effect allele'. So, first check identify which allele is the effect allele for the SNPs of interest grep -E 'rs4747841|rs878177' ./Base_data/GIANT_Height.txt rs4747841 A G 0.551 -0.0011 0.0029 0.7 253213 rs878177 T C 0.3 0.14 0.0031 8.2e-06 251271 Second, multiply the weight of each risk allele by their dosage Individual_1: PRS=?, Individual_2: PRS=? \u2753What do these PRS values mean in terms of the height of those individuals? Back to Table of Contents","title":"Understanding GWAS Summary Statistics"},{"location":"modules/Day2.docx/#matching-the-base-and-target-data-sets","text":"The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. \u203c\ufe0fFor SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. Back to Table of Contents","title":"Matching the Base and Target Data sets"},{"location":"modules/Day2.docx/#linkage-disequilibrium-in-prs-analyses","text":"GWAS are typically performed one-SNP-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent genetic effects (or their best proxies if these are not genotyped/imputed) challenging. There are two main options for approximating the PRS that would have been generated from full conditional GWAS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date ( Mak, T et al., 2017 ). In this workshop we will consider option 1, implemented in PRSice, but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred ( Vilhjalmsson, B et al., 2015 )and lassosum ( Mak, T et al., 2017 ) papers. Back to Table of Contents","title":"Linkage Disequilibrium in PRS Analyses"},{"location":"modules/Day2.docx/#performing-clumping","text":"Clumping is the procedure where a SNP data set is 'thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \ud835\udc43 -value. SNPs are first sorted (i.e. ranked) by their \ud835\udc43 -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \ud835\udc5f 2 >0.1, with \ud835\udc5f 2 typically calculated from phased haplotype data) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are \ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc51. This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK( Chang, C et al., 2015 ). First, you will have to navigate to the right folder where the data are stored using the terminal. Open the terminal and type the command below at the terminal prompt: plink \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height \u203c\ufe0fYou can copy & paste code from this document directly to the terminal, but this can cause problems (e.g. when opened by Preview in Mac) and distort the code. Try using Adobe Reader or first copy & pasting to a text editor (e.g. notepad) or use the script file provided that contains all the commands. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \ud835\udc5f 2 >0.1 within a 250 kb window of the index SNP are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. \u2753How many SNPs were in the GIANT_Height.txt file before clumping? Answer 2,183,049 variants \u2753How many SNPs remain after clumbing? Answer 109,496 variants \u2753If we change the r 2 threshold to 0.2, how many SNPs remain? Why are there, now, more SNPs remaining? Answer 162,275 variants \u2753Why is clumping performed for calculation of PRS? (in the standard approach). Back to Table of Contents","title":"Performing Clumping"},{"location":"modules/Day2.docx/#p-value-thresholding","text":"Deciding which SNPs to include in the calculation of PRS is one of the major challenges in the field. A simple and popular approach is to include SNPs according to their GWAS association \ud835\udc43-value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \ud835\udc43-value thresholds.","title":"P-Value Thresholding"},{"location":"modules/Day2.docx/#height-prs-using-gw-significant-snps-only","text":"Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype as target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the /home/manager/data/Day2_Target_Data/Day2_Target_Data directory, run the following command in the terminal: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --bar-levels 5e-8 --no-full --fastscore --out Results/Height.gws This command takes the Height GWAS summary statistic file (--base), informs PRSice of the column name for the column containing the SNP ID(--snp), the effect allele (--A1), the non-effect allele (--A2), the effect size (--stat) and the \ud835\udc43-value (--pvalue). We also inform PRSice that the effect size is a \ud835\udefd coefficient (--beta) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \ud835\udc43-value \\< 5 \u00d7 10 \u22128 . \ud83d\udcdcThe default of PRSice is to perform clumping with r 2 threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice, type: ~/PRSice_linux/PRSice Take some time to have a look through some of these user options. By looking at the user options, work out which user option or options were used to ensure that the command above only calculated 1 PRS at genome-wide significance of \\< 5 \u00d7 10 \u22128 . PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype. Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \ud835\udefd coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \ud835\udefd Standard.Error - The standard error of the best-fit PRS \ud835\udefd coefficient (see above) P - The \ud835\udc43 -value relating to testing the null hypothesis that the best-fit PRS \ud835\udefd coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \ud835\udc43 -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. \u2753What is the R 2 for the PRS constructed using only genome-wide significant SNPs? Answer 0.071 \u2753What is the P-value for the association between the PRS and the outcome? Is this significant? (explain your answer) Answer 1.36e-09 Back to Table of Contents","title":"Height PRS using GW-significant SNPs only"},{"location":"modules/Day2.docx/#height-prs-across-multiple-p-value-thresholds","text":"A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among the SNPs that did not reach genome-wide significance. However, since we do not know what \ud835\udc43-value threshold provides the \\\"best\\\" prediction for our particular data, then we can calculate the PRS under several \ud835\udc43-value thresholds and test their prediction accuracy to identify the \\\"best\\\" threshold (NB. See Dudbridge, F 2013 for theory on factors affecting the best-fit PRS) This process is implemented in PRSice and can be performed automatically as follows: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --fastscore --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT ( Figure 1.1 ) generated by PRSice Figure 1.1: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.05 \u2753What is the R 2 of the most predictive threshold and how does it compare to PRS generated using genome-wide significant SNPs? Answer 0.26523 Back to Table of Contents","title":"Height PRS across multiple P-value thresholds"},{"location":"modules/Day2.docx/#high-resolution-scoring","text":"If we limit ourselves to a small number of \ud835\udc43-value thresholds, we might \\\"miss\\\" the most predictive threshold. In order to identify this \\\"best\\\" threshold, we will need \\\"high-resolution scoring\\\", that is, to test the predictive power of PRS generated under a larger number of p-value thresholds. We can achieve that by simply removing the --fastscore command from the PRSice script: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot ( Figure 1.2.1 ) presenting the model fit of PRS calculated at all P-value thresholds. Figure 1.2.1: High resolution Plot generated by PRSice Figure 1.2.2: BARPLOT generated by PRSice \u2753Which is the most predictive threshold? Answer 0.058 \u2753How much better is the threshold identified using high-resolution scoring, in terms of model R 2 ? Answer The R 2 using high-resolution scoring is 0.268237. In this case, there is not significant difference between the two results. \u2753How does running fastscore vs high-resolution change the most predictive threshold identified? \u2753The default of PRSice is to iterate the P-value threshold from \\< 5 \u00d7 10 \u22128 to 0.5 with a step size of 0.00005, and to inlcude the P-value threshold of 1. Can you identify the commands controlling these parameters? Hint ./Software/PRSice_linux -h Back to Table of Contents Accounting for Covariates When performing PRS, one might want to account for covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This \ud835\udc43\ud835\udc45\ud835\udc46.\ud835\udc45 2 is calculated by substracting the \ud835\udc45 2 of the null model (e.g. \ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65) from the \ud835\udc45 2 of the full model (e.g.\ud835\udc3b\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 \u223c \ud835\udc46\ud835\udc52\ud835\udc65 + \ud835\udc43\ud835\udc45\ud835\udc46) \u2139\ufe0f Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure and it can be tedious to type all 20 or 40 PCs (e.g. PC1 , PC2 ,..., PC20 ). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substitution of numbers. If @ is found in front of the --cov-col string, any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1 , PC2 , PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1 , cov3 , cov4 , cov5 . You can also mix it up, e.g. @PC[1-3]. Sex will be interpreted as PC1 , PC2 , PC3 , Sex by PRSice. \u2753How does the inclusion of sex as covariate change the results? Answer The inclusion of sex as a covariate increased the phenotypic variance explained by both PRS and sex (FULL.R2 = 0.47) Back to Table of Contents","title":"High Resolution Scoring"},{"location":"modules/Day2.docx/#stratifying-samples-by-prs","text":"An interesting application of PRS is to test whether samples with higher PRS have higher phenotypic values. This can be nicely visualized using the quantile plot ( Figure 1.3 ) To generate quantile plots in PRSice, simply add --quantile 10 option. \ud83d\udcdc We can skip the PRS calculation using the --plot option, which will use previoulsy calculated PRS to generate the plots Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 10 --out Results/Height.sex Figure 1.3: Example of a quantile plot generated by PRSice \u2139\ufe0f The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is handy when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles, and again with 50 quantiles (checking the quantile plot each time . A disadvantage of the quantile plot is that it only separate samples into quantiles of equal size. However, it is sometimes interesting to investigate whether a specific strata (e.g. top 5% of samples),contain a higher PRS than the reference strata. For example, ( Mavaddat, N et al., 2015 ) found that samples in the highest 1% of PRS distribution have a 2.81 increased OR of breast cancer when comparing to samples at the middle quantiles (40th to 60th percentile). We can mimic their table by using --quant-break , which represents the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile: Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/TAR.height --binary-target F --cov Target_Data/TAR.covariate --cov-col Sex --plot --quantile 100 --quant-break 1,5,10,20,40,60,80,90,95,99,100 --quant-ref 60 --out Results/Height.sex Figure 1.4: Example of a strata plot generated by PRSice \ud83d\udcdc See the quantile results in Table from in the QUANTILES file, and the plots in the QUANTILES_PLOT file. Due to the small sample size of the target data the results here are underwhelming, but with high power we may observe strong deviation in the extreme quantiles. Back to Table of Contents","title":"Stratifying Samples by PRS"},{"location":"modules/Day2.docx/#case-control-studies","text":"In the previous exercises, we have performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and need to be converted to \ud835\udefd's when constructing PRS Here, we will use CAD as an example. You will find the summary statistic under Base_Data ( cad.add.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. \u2139\ufe0f GWAS summary statistics for binary traits tend to report the OR instead of the \ud835\udefd coefficient, in which case the --or should be used. However, CARDIoGRAM plus C 4 D consortium provided the \ud835\udefd coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/cad.add.txt --target Target_Data/TAR --snp markername --A1 effect_allele --A2 noneffect_allele --chr chr --bp bp_hg19 --stat beta --beta --pvalue p_dgc --pheno Target_Data/CAD.pheno --binary-target T --out Results/CAD.highres \u2139\ufe0f --chr and --bp inform PRSice the columns containing the chromosomal coordinates. This enables PRSice to check whether the SNPs in the Base and Target data have the same chromosomal coordinate. Figure 1.5: BARPLOT generated from PRSice \u2753What is the R 2 and P-value of the best-fit PRS? Answer 0.39 and 0.001, respectively \u2753Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? Answer The p-value does not suggest a significant association between the CAD PRS and CAD status.","title":"Case Control Studies"},{"location":"modules/Day2.docx/#cross-trait-analysis","text":"A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by ( Ruderfer, D et al., 2014 ) which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. \u2139\ufe0f We will only focus on the simplest form of cross-trait analysis in this practical. To perform the multi-phenotype cross-trait analysis similar to that of ( Ruderfer, D et al., 2014 ), you can use the --pheno-col to include multiple target phenotype into the analysis. Rscript ~/PRSice_linux/PRSice.R --prsice ~/PRSice_linux/PRSice_linux --base Base_Data/GIANT_Height.txt --target Target_Data/TAR --snp MarkerName --A1 Allele1 --A2 Allele2 --stat b --beta --pvalue p --pheno Target_Data/CAD.pheno --binary-target T --out Results/Cross.highres Figure 1.6: BARPLOT generated from PRSice \u2753What is the R 2 for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Answer 0.032 \u2753Now try using CAD as the base to predict height as the target trait? What is the PRS R 2 for that? Answer Back to top","title":"Cross-Trait Analysis"},{"location":"modules/Day3a.docx/","text":"Advanced Polygenic Risk Score Analyses Day 3 - Polygenic Risk Score Analyses Workshop 2024 Table of Contents Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2 Day 3a practical Key Learning Outcomes After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds. Base and Target datasets In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650) Downloading Datasets All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam Method for calculating PRS For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach. Exercise 1 Estimating R 2 Code Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2 Scenario 1: Predicting from EUR training to EUR target data: Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur Key code parameters The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389. Scenario 2: Predicting from AFR training to AFR target data: Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%). Scenario 3: Predicting from EUR training to AFR target data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%). Exercise 2 Visualising and comparing R 2 In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library(ggplot2) library(RColorBrewer) # Create a function to read the files and add ancestry information read_and_label <- function(file, ancestry) { data <- read.table(file, header = TRUE, sep = \"\\t\") data$Ancestry <- ancestry return(data) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label(\"SBP.eur.eur.summary\", \"EUR_EUR\") AFR_AFR <- read_and_label(\"SBP.afr.afr.summary\", \"AFR_AFR\") EUR_AFR <- read_and_label(\"SBP.eur.afr.summary\", \"EUR_AFR\") # Combine all data into one dataframe all_data <- rbind(EUR_EUR, AFR_AFR, EUR_AFR) # Create a bar graph with different colors for each ancestry png('/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) ancestry <- ggplot(all_data, aes(x = Ancestry, y = PRS.R2, fill = Ancestry)) + geom_bar(stat = \"identity\", position = \"dodge\") + labs(title = \"R2 Values by Ancestry\", x = \"Ancestry\", y = \"R2 Value\") + theme_minimal() + scale_fill_brewer(palette = \"Set3\") + theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"), axis.title.x = element_text(size = 14, face = \"bold\"), axis.title.y = element_text(size = 14, face = \"bold\"), axis.text.x = element_text(size = 12, angle = 45, hjust = 1), axis.text.y = element_text(size = 12), legend.position = \"none\") print(ancestry) dev.off() Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Advanced Polygenic Risk Score Analyses"},{"location":"modules/Day3a.docx/#advanced-polygenic-risk-score-analyses","text":"","title":"Advanced Polygenic Risk Score Analyses"},{"location":"modules/Day3a.docx/#day-3-polygenic-risk-score-analyses-workshop-2024","text":"","title":"Day 3 - Polygenic Risk Score Analyses Workshop 2024"},{"location":"modules/Day3a.docx/#table-of-contents","text":"Key Learning Outcomes Base and Target datasets Downloading Datasets Method for Calculating PRS Exercise 1 Estimating R 2 Exercise 2 Visualising and comparing R 2","title":"Table of Contents"},{"location":"modules/Day3a.docx/#day-3a-practical","text":"","title":"Day 3a practical"},{"location":"modules/Day3a.docx/#key-learning-outcomes","text":"After completing this practical, you should be able to: 1. Compute and analyse ancestry-matched and unmatched PRS using PRSice-2. 2. Understand and interpret the results from PRSise-2 derived scores. 3. Understand and identify the impact of ancestry on the predictive utility of PRS. 4. Understand and identify the impact of sample size on the predictive utility of PRS. 5. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds.","title":"Key Learning Outcomes"},{"location":"modules/Day3a.docx/#base-and-target-datasets","text":"In this practical, we will compute a PRS for systolic blood pressure (SBP) and assess it performance across European and African ancestry datasets to clearly illustrate the portability problem. We will assess the predictive utility of 3 scores : ANCESTRY-MATCHED 1. EUR base - EUR target: Utilise European summary statistics as the training data and individual-level genotyped data from Europeans as the target dataset. 2. AFR base - AFR target: Utilise African summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. ANCESTRY-UNMATCHED EUR base - AFR target: Utilise European summary statistics as the training data and individual-level genotyped data from Africans as the target dataset. Please note that the sample sizes of the individual-level target data are as follows: Europeans (n = ~500) and Africans (n = ~650). Note: This is simulated data with no real-life biological meaning or implication. Dataset Source Description Base dataset (EUR, AFR) simulated GWAS summary stats of SBP Target dataset (EUR, AFR) simulated EUR (n = ~500), AFR (n = ~650)","title":"Base and Target datasets"},{"location":"modules/Day3a.docx/#downloading-datasets","text":"All required software for this practical is found in the /home/manager/data/Data_Day4/software directory. \ud83d\udee0\ufe0f: Software - PRSice.R - PRSice_linux - plink_linux All required data for this practical is found in the /home/manager/data/Data_Day4/data directory. The relevant data that you should see there at the start of the practical are as follows: \ud83d\udcc2: Base_Data (summary statistics) - AFR-SBP-simulated.sumstats.prscsx - EUR-SBP-simulated.sumstats.prscsx \ud83d\udcc2: Target_Data - AFR_1kg.hm3.only.csx (.bed, .bim, .fam) - EUR_1kg.hm3.only.csx (.bed, .bim, .fam) - sbp.afr.1kg.sim_pheno - sbp.eur.1kg.sim_pheno \ud83d\udcc1: Reference files - 1kg.afr.dbSNP153.hg38.bed - 1kg.afr.dbSNP153.hg38.bim - 1kg.afr.dbSNP153.hg38.fam - 1kg.eur.dbSNP153.hg38.bed - 1kg.eur.dbSNP153.hg38.bim - 1kg.eur.dbSNP153.hg38.fam","title":"Downloading Datasets"},{"location":"modules/Day3a.docx/#method-for-calculating-prs","text":"For this practical we will use PRSice-2. PRSice-2 is one of the dedicated PRS calculation and analysis programs that makes use of a sequence of PLINK functions. The tools utilises the standard clumping and thresholding (C+T) approach.","title":"Method for calculating PRS"},{"location":"modules/Day3a.docx/#exercise-1-estimating-r2","text":"","title":"Exercise 1 Estimating R2"},{"location":"modules/Day3a.docx/#code","text":"Open the terminal and move to the data directory for this practical: cd /home/manager/data/Data_Day4/data Create the output directory - \"out\": mkdir out Look at the data files within the directory: ls -l Now, calculate PRS using PRSice 2","title":"Code"},{"location":"modules/Day3a.docx/#scenario-1-predicting-from-eur-training-to-eur-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_eur_1kg.sim_pheno \\ --pheno-col pheno100 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.eur","title":"Scenario 1: Predicting from EUR training to EUR target data:"},{"location":"modules/Day3a.docx/#key-code-parameters","text":"The parameters listed in this table remain consistent across various scenarios, but the specific values may change based on the dataset and analysis scenario. For illustrative purposes, this table uses the first scenario, EUR base and EUR target population: Click to view the parameters table Parameter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary, xxx is the operating system (mac or linux) base EUR-SBP-simulated.sumstats.prscsx Specifies the GWAS summary statistics file for input A1 A1 Column name for the effect allele in the GWAS summary statistics p value P Column name for the p-values of SNPs in the GWAS summary statistics no clump - Instructs PRSice to skip the clumping process, which is used to remove SNPs in linkage disequilibrium beta - Indicates that the effect sizes are given in beta coefficients (linear regression coefficients) snp SNP Column name for SNP identifiers in the GWAS summary statistics score sum Specifies that the score calculation should sum the product of SNP effect sizes and their genotype counts target EUR_1kg.hm3.only.csx Specifies the genotype data file for the target sample binary-target F Indicates that the phenotype of interest is not binary (e.g., quantitative trait), F for no pheno sbp_eur_1kg.sim_pheno Specifies the file containing phenotype data pheno-col pheno100 Column name in the phenotype file that contains the phenotype data to be used for PRS calculation cov* EUR.covariate Specifies the file containing covariate data for the analysis base-maf* MAF:0.01 Filter out SNPs with MAF < 0.01 in the GWAS summary statistics, using information in the MAF column base-info* INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat* OR Column name for the odds ratio (effect size) in the GWAS summary statistics or* - Inform PRSice that the effect size is an Odd Ratio thread 8 Specifies the number of computing threads to use for the analysis out SBP_trial.eur.eur Specifies the name for the output files generated by PRSice *Note: These parameters are not used within this exercise but will likely be included when conducting your own analyses. QUESTIONS Move to the out directory you created earlier: cd out Look at the files produced following your first analyis within the directory: ls -l How many files have been generated from this code and what does each file show you? This code generates six files. Each files serves a different purpose in the analysis and interpretation of derived PRS. These are outlined below: 1. **.summary**: Provides a high-level summary of the best-performing PRS analysis result, allowing for a quick assessment of the PRS model's performance. 2. **.prsice**: Provides the PRS analysis results across all p-value thresholds. This file will be the input for the bar plot that assesses the R2 at each p-value threshold. 3. **.log**: Useful for debugging and detailed tracking of the computational steps undertaken during the PRS calculation. 4. **.best**: Provides details of which individuals (IID) are included in the PRS regression analysis that assesses the association between the genotype and phenotype, and provides their individual PRS score 5. **.png (Bar plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold, with the most predictive bar being the highest R 2 and thus the tallest bar). The y axis shows the phenotypic variance explained (R 2 ), the x-axis the various p-value thresholds, and the text above each bar is the p-value showing the significance of the association between the PRS and phenotype. The colours of the bars (from red to blue) indicate the strength of the association with red indicating lower p-values (greater significance). 6. **.png (High resolution plot)**: Assist in visually assessing the performance of PRS calculated at each p-value threshold. However, this high-resolution plot uses a negative logarithmic scale on the Y-axis to show the performance of different combinations of SNPS in predicting the trait as measured by their p-values. Lower p-values indicate better performance and appear higher on the Y-axis. Examine the plot indicating the R 2 at each p-value threshold: xdg-open SBP.eur.eur_BARPLOT_2024-06-12.png Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00205005. What does \"best-fit\" mean? \"Best-fit\" refers to the p-value threshold at which the PRS accounts for the highest proportion of variance in the phenotype (R 2 ) compared to other thresholds tested. Why does this matter? Choosing the optimal p-value threshold is crucial because it affects the sensitivity and specificity of the PRS. The optimal threshold balances including informative SNPs and excluding noise from less relevant variants. A threshold that is too lenient (high p-value from GWAS association test) might include too many SNPs, adding noise and possibly diluting the predictive power of the score. Conversely, a threshold that is too stringent (low p-value) might exclude potentially informative SNPs, reducing the ability of the PRS to capture the genetic architecture of the trait. Which file provides a summary of the \"best-fit\" PRS? SBP.eur.eur.summary View this summary output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.eur.summary How much phenotypic variation does the \"best-fit\" PRS explain? What does this mean in very simple terms? R 2 = 0.0829 (8.2%). This R 2 value means that out of the total variability observed in the trait across the population (under study), 8.2% of the variation can be attributed to the genetic variants included in this PRS. What is the significance of the association (p-value) between the \"best-fit\" PRS and trait? The p-value is 1.72126e-11. A p-value below 0.05 indicates statistically significant evidence that the PRS at this threshold explains phenotypic variance and captures genuine genetic associations with the phenotype (i.e. not by chance). How many SNPs are included in the \"best-fit\" PRS? Number of SNPs = 389.","title":"Key code parameters"},{"location":"modules/Day3a.docx/#scenario-2-predicting-from-afr-training-to-afr-target-data","text":"Return to the data directory: cd /home/manager/data/Data_Day4/data Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/AFR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.afr.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.afr.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 5e-08. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 96. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0082124 (0.8%).","title":"Scenario 2: Predicting from AFR training to AFR target data:"},{"location":"modules/Day3a.docx/#scenario-3-predicting-from-eur-training-to-afr-target-data","text":"Rscript /home/manager/PRSice_linux/PRSice.R \\ --prsice /home/manager/PRSice_linux/PRSice \\ --base /home/manager/data/Data_Day4/data/EUR-SBP-simulated.sumstats.prscsx \\ --A1 A1 \\ --pvalue P \\ --no-clump \\ --beta \\ --snp SNP \\ --score sum \\ --target /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --binary-target F \\ --pheno /home/manager/data/Data_Day4/data/sbp_afr_1kg.sim_pheno \\ --pheno-col pheno50 \\ --thread 8 \\ --out /home/manager/data/Data_Day4/data/out/SBP.eur.afr View the output file: cat /home/manager/data/Data_Day4/data/out/SBP.eur.afr.summary QUESTIONS Which p-value threshold generates the \"best-fit\" PRS? P-value threshold of 0.00815005. How many SNPs are included in the \"best-fit\" PRS explain? Number of SNPs = 1192. How much phenotypic variation does the \"best-fit\" PRS explain? R 2 = 0.0160098 (1.6%).","title":"Scenario 3: Predicting from EUR training to AFR target data"},{"location":"modules/Day3a.docx/#exercise-2-visualising-and-comparing-r2","text":"In this exercise, we will analyse and compare the phenotypic variance explained (R 2 ) by PRS across different combinations of base and target ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the 'out' directory: cd /home/manager/data/Data_Day4/data/out/ Now open R: R Once in R, combine the summary files and visualise the performance of each PRS: # Load necessary libraries library(ggplot2) library(RColorBrewer) # Create a function to read the files and add ancestry information read_and_label <- function(file, ancestry) { data <- read.table(file, header = TRUE, sep = \"\\t\") data$Ancestry <- ancestry return(data) } # Read each file with the corresponding ancestry information EUR_EUR <- read_and_label(\"SBP.eur.eur.summary\", \"EUR_EUR\") AFR_AFR <- read_and_label(\"SBP.afr.afr.summary\", \"AFR_AFR\") EUR_AFR <- read_and_label(\"SBP.eur.afr.summary\", \"EUR_AFR\") # Combine all data into one dataframe all_data <- rbind(EUR_EUR, AFR_AFR, EUR_AFR) # Create a bar graph with different colors for each ancestry png('/home/manager/data/Data_Day4/data/out/PRS_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) ancestry <- ggplot(all_data, aes(x = Ancestry, y = PRS.R2, fill = Ancestry)) + geom_bar(stat = \"identity\", position = \"dodge\") + labs(title = \"R2 Values by Ancestry\", x = \"Ancestry\", y = \"R2 Value\") + theme_minimal() + scale_fill_brewer(palette = \"Set3\") + theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"), axis.title.x = element_text(size = 14, face = \"bold\"), axis.title.y = element_text(size = 14, face = \"bold\"), axis.text.x = element_text(size = 12, angle = 45, hjust = 1), axis.text.y = element_text(size = 12), legend.position = \"none\") print(ancestry) dev.off() Examine the bar plot indicating the R 2 for each base:target ancestry pair: xdg-open PRS_ancestry_analysis.png QUESTIONS Which base:target pair has the highest phenotypic variance explained? EUR:EUR. Which base:target pair has the lowest phenotypic variance explained? AFR:AFR. Explain the results? Are they as expected? THINK - PAIR - SHARE \u203c\ufe0f Note that all target phenotype data in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Back to Top","title":"Exercise 2 Visualising and comparing R2"},{"location":"modules/Day3b.docx/","text":"Day 3b practical We need to move into the directory you will be working in; cd ~/data/Data_Day4/data Introduction to Cross-Ancestry PRS computation Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python=3.7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion. 1. The 1000 Genomes datasets The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week. 2. Cross-population allele frequency Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l Questions (i) Which population contains the most SNPs? (ii) What is the significance of the observed population order? 3. Distribution of allele frequencies In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages(\"dplyr\") install.packages(\"ggplot2\") # Load necessary libraries library(dplyr) library(ggplot2) # Create a function to read the files and add ancestry information freq <-read.table(\"~/data/Data_Day4/plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) # Create a bar graph png('/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) maf_ancestry <- ggplot(na.omit(plotDat), aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") print(maf_ancestry) dev.off() Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png Questions (i) Which population has the most SNPs? (ii) What is the significance of the observed population ordering? (iii) What is the reason behind these two features? Introduction to PRS-CSx 5. Background to PRS-CSX PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and external LD reference panels from multiple populations to improve cross-population polygenic prediction. We will be using simulated trait data pertaininng to systolic blood pressure (SBP) to explore PRS performance using 2 target populations that consist of 650 individuals of African ancestry and 500 individuals of European ancestry. Please note when running PRSice that the object of the flag \"--prsice\" will change according to whether plink is being called within the linux-like environment of the virtual machine (PRSice_linux) or a mac (PRSice_mac). Both executables can be found in the /home/manager/data/Data_Day4 directory. 7. Running PRS-CSx To model the coupling of ect sizes at individual SNPs across ancestries PRS-CSx uses an MCMC (Bayesian) sampling algorithm to determine values of the global shrinkage parameter (\"phi\") by Maximum likelihood. For samples of mixed or admixed genetic ancestry (which ours are not) the optimal value of the shrinkage parameter is estimated autonomously from the data. Here we use the value of phi (1e-4), which is suggested by the software authors, given that our trait is simulated to have a relatively small number (N=110) causal variants, distributed genome-wide. Step 1: Set up environment First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE=\"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21..22}; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr${chr},/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr${chr} \\ --n_gwas=25732,4855 \\ --chrom=${chr} \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr${chr}.csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3) Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE) Task: Replace 'ancestry <- \"EUR\" ' with 'ancestry <- \"AFR\" ' and repeat the subsequent steps shown above Step 4: Merge genotype-phenotype data Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen) Step 5: Split data into validation and test sets Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices] Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights # Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur) Step 7: Prepare the variant weights matrices as vectors # Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat)) Step 8: Predict phenotype on validation and test dataset Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z Step 9: Plot phenotype distributions of validation and test data: Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal() Step 10: Plot true values against predicted values The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off() Step 11: Calculate deviance-based R 2 # Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2)) Results graph: pdf true against pred","title":"Day3b.docx"},{"location":"modules/Day3b.docx/#day-3b-practical","text":"We need to move into the directory you will be working in; cd ~/data/Data_Day4/data","title":"Day 3b practical"},{"location":"modules/Day3b.docx/#introduction-to-cross-ancestry-prs-computation","text":"Before starting the practical, the following commands will need to be run from within your virtual machine. These commands set up an 'environment' that allows you to work outside the virtual machine, which has memory restrictions. Set up the environment using conda: conda create -n \"PRScsx\" python=3.7 conda activate PRScsx pip install scipy pip install h5py The aim of this practical is to provide you with a basic understanding and some experience of running PRS-CSx software. After completing this practical, you should: Be able to perform cross-population analyses. Be familiar with running cross-ancestry PRS analyses using PRS-CSx. Understand how to evaluate linear models using Akaike\u2019s Information Criterion.","title":"Introduction to Cross-Ancestry PRS computation"},{"location":"modules/Day3b.docx/#1-the-1000-genomes-datasets","text":"The data we will be working with comes from the 1000 Genomes Project reference panel. The data relates to individuals from 26 different source populations around the world. For simplicity, the populations have been collapsed into 5 broader continental super-populations: East Asian, European, South Asian, Amerindian, African ((EAS, EUR, SAS, EUR and AFR)). The scripts used to download and process the 1000Genomes data for the purposes of this course will be provided in the course appendix at the end of this week.","title":"1. The 1000 Genomes datasets"},{"location":"modules/Day3b.docx/#2-cross-population-allele-frequency","text":"Genetic variation is conveyed using allelic frequencies. Allele frequency is shaped by evolutionary forces and drift. Here we compare profiles of allele frequency across the five ancestral populations. Global differences in allelic frequency has important implications for the portability of PRS across populations. Using plink it is possible to generate allele frequency statistics for each SNP, across populations, using the annotations provided in the file pop_info.pheno . In /home/manager/data/Data_Day4 : cd ../ ./software/plink_linux --bfile ./data/chr1-22 --freq --within ./data/pop_info.pheno Population-stratified allele frequencies are reported in the output file plink.frq.strat. For each population, print the numbers of total SNPs to screen, as follows: AFR grep -F 'AFR' plink.frq.strat | wc -l From there we can print the number of SNPs with minor allele frequencies greater than 0 (and are hence potentially available for genomic analyes). grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l EUR grep -F 'EUR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EUR. grep -F 'EUR' plink.frq.strat | awk '$6 >0' | wc -l EAS grep -F 'EAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in EAS. grep -F 'EAS' plink.frq.strat | awk '$6 >0' | wc -l SAS grep -F 'SAS' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in SAS. grep -F 'SAS' plink.frq.strat | awk '$6 >0' | wc -l AFR grep -F 'AFR' plink.frq.strat | wc -l Number of SNPs with MAF > 0 in AFR. grep -F 'AFR' plink.frq.strat | awk '$6 >0' | wc -l","title":"2. Cross-population allele frequency"},{"location":"modules/Day3b.docx/#questions","text":"","title":"Questions"},{"location":"modules/Day3b.docx/#i-which-population-contains-the-most-snps","text":"","title":"(i) Which population contains the most SNPs?"},{"location":"modules/Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-order","text":"","title":"(ii) What  is the significance of the observed population order?"},{"location":"modules/Day3b.docx/#3-distribution-of-allele-frequencies","text":"In this exercise, we will analyse and compare the distribution of allele frequencies across different ancestries. We will use R for visualisation. Open a new terminal and open R Open a new tab in the terminal ( plus icon in the top left corner ) In this new terminal window, make sure you are in the correct directory: cd /home/manager/data/Data_Day4/out/ Now open R: R Generate the plot: # Install the necessary libraries install.packages(\"dplyr\") install.packages(\"ggplot2\") # Load necessary libraries library(dplyr) library(ggplot2) # Create a function to read the files and add ancestry information freq <-read.table(\"~/data/Data_Day4/plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) # Create a bar graph png('/home/manager/data/Data_Day4/out/MAF_ancestry_analysis.png', unit='px', res=300, width=3500, height=4500) maf_ancestry <- ggplot(na.omit(plotDat), aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") print(maf_ancestry) dev.off() Examine the plot the MAF across each ancestry: # This does not work when you are in R, ensure you out of R before running: xdg-open MAF_ancestry_analysis.png","title":"3. Distribution of allele frequencies"},{"location":"modules/Day3b.docx/#questions_1","text":"","title":"Questions"},{"location":"modules/Day3b.docx/#i-which-population-has-the-most-snps","text":"","title":"(i) Which population has the most SNPs?"},{"location":"modules/Day3b.docx/#ii-what-is-the-significance-of-the-observed-population-ordering","text":"","title":"(ii) What  is the significance of the observed population ordering?"},{"location":"modules/Day3b.docx/#iii-what-is-the-reason-behind-these-two-features","text":"","title":"(iii) What is the reason behind these two features?"},{"location":"modules/Day3b.docx/#introduction-to-prs-csx","text":"","title":"Introduction to PRS-CSx"},{"location":"modules/Day3b.docx/#5-background-to-prs-csx","text":"PRS-CSx is a Python based command line tool that integrates GWAS summary statistics and external LD reference panels from multiple populations to improve cross-population polygenic prediction. We will be using simulated trait data pertaininng to systolic blood pressure (SBP) to explore PRS performance using 2 target populations that consist of 650 individuals of African ancestry and 500 individuals of European ancestry. Please note when running PRSice that the object of the flag \"--prsice\" will change according to whether plink is being called within the linux-like environment of the virtual machine (PRSice_linux) or a mac (PRSice_mac). Both executables can be found in the /home/manager/data/Data_Day4 directory.","title":"5. Background to PRS-CSX"},{"location":"modules/Day3b.docx/#7-running-prs-csx","text":"To model the coupling of ect sizes at individual SNPs across ancestries PRS-CSx uses an MCMC (Bayesian) sampling algorithm to determine values of the global shrinkage parameter (\"phi\") by Maximum likelihood. For samples of mixed or admixed genetic ancestry (which ours are not) the optimal value of the shrinkage parameter is estimated autonomously from the data. Here we use the value of phi (1e-4), which is suggested by the software authors, given that our trait is simulated to have a relatively small number (N=110) causal variants, distributed genome-wide.","title":"7. Running PRS-CSx"},{"location":"modules/Day3b.docx/#step-1-set-up-environment","text":"First change to the working directory with the data for this practical cd /home/manager/data/Data_Day4/data Make a directory called hm3_by_ancestry within the data folder, and move a folder back out of the data folder mkdir hm3_by_ancestry cd .. AFR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/AFR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx; done EUR for chr in {21..22}; do \\ /home/manager/data/Data_Day4/software/plink_linux \\ --bfile /home/manager/data/Data_Day4/data/EUR_1kg.hm3.only.csx \\ --chr $chr \\ --make-bed \\ --out /home/manager/data/Data_Day4/data/hm3_by_ancestry/EUR_1kg.hm3.chr${chr}_only.csx; done Set up the necessary environment variables for threading and verify they are set correctly. export N_THREADS=2 export MKL_NUM_THREADS=$N_THREADS export NUMEXPR_NUM_THREADS=$N_THREADS export OMP_NUM_THREADS=$N_THREADS Verify the variables are set echo $N_THREADS echo $MKL_NUM_THREADS echo $NUMEXPR_NUM_THREADS echo $OMP_NUM_THREADS","title":"Step 1: Set up environment"},{"location":"modules/Day3b.docx/#step-2-run-csx-derive-new-snps-weights-trained-on-european-and-african-summary-stats","text":"Generate job file containing the threaded PRScsx commands. First, to minimize computational resources and time, we should create a script to run the tasks in parallel. Make a script called create_multithread.sh nano create_multithread.sh Then copy and paste the code below into that script. After save, then close the script ctrl + x #!/bin/bash # Create the script file SCRIPT_FILE=\"multithread.sh\" # Write the header of the script file echo \"#!/bin/bash\" > $SCRIPT_FILE for chr in {21..22}; do echo \"python3 /home/manager/data/Data_Day4/software/PRScsx.py \\ --ref_dir=/home/manager/data/Data_Day4/reference/csx \\ --bim_prefix=/home/manager/data/Data_Day4/data/hm3_by_ancestry/AFR_1kg.hm3.chr${chr}_only.csx \\ --sst_file=/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/EUR-SBP-simulated.sumstats.chr${chr},/home/manager/data/Data_Day4/data/3b/data/sumstats_by_chr/AFR-SBP-simulated.sumstats.chr${chr} \\ --n_gwas=25732,4855 \\ --chrom=${chr} \\ --n_iter=1000 \\ --n_burnin=500 \\ --thin=5 \\ --pop=EUR,AFR \\ --phi=1e-4 \\ --out_dir=/home/manager/data/Data_Day4/out/csx \\ --out_name=afr.target_chr${chr}.csx\" >> $SCRIPT_FILE done Run: Ctrl + O, followed by Enter ' to save ' Run: Ctrl + X, to Exit You will need to change permission for the script to be able to execute chmod +x create_multithread.sh Run the builder ./create_multithread.sh To be able to run the next command you first install 'parallel' sudo apt install parallel Run the Job File with GNU Parallel: (May take a while) parallel --verbose --jobs $N_THREADS < multithread.sh","title":"Step 2: Run CSX. Derive new SNPs weights trained on European and African summary stats"},{"location":"modules/Day3b.docx/#step-3-combine-csx-derived-snp-weights-across-chromosomes-currently-excludes-chromosome-3","text":"Load R and the necessary library R Call in the package library(dplyr) Define the path to the directory containing the PRS-CSX output files path <- \"/home/manager/data/Data_Day4/out/csx\" Define the ancestry you want to combine (\"EUR\" or \"AFR\") ancestry <- \"EUR\" Initialize an empty data frame to store the combined data combined_data <- data.frame() Loop through chromosomes 21 to 22, (currently excluding chromosome 3) for (chr in setdiff(21:22, 3)) { # Construct the file name file_name <- paste0(\"afr.target_chr\", chr, \".csx_\", ancestry, \"_pst__a1_b0.5_phi1e-04_chr\", chr, \".txt\") file_path <- file.path(path, file_name) # Check if file exists before reading if (file.exists(file_path)) { # Read the data from the file data <- read.table(file_path, header = FALSE, sep = \"\\t\", col.names = c(\"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\")) # Combine the data combined_data <- rbind(combined_data, data) } else { warning(paste(\"File not found:\", file_path)) } } Write the combined data to a new file output_file <- file.path(path, paste0(\"combined_\", ancestry, \"_pst_.txt\")) write.table(combined_data, output_file, sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE)","title":"Step 3: Combine CSX-derived SNP weights across chromosomes (Currently Excludes Chromosome 3)"},{"location":"modules/Day3b.docx/#task-replace-ancestry-eur-with-ancestry-afr-and-repeat-the-subsequent-steps-shown-above","text":"","title":"Task: Replace 'ancestry &lt;- \"EUR\" ' with 'ancestry &lt;- \"AFR\" ' and repeat the subsequent steps shown above"},{"location":"modules/Day3b.docx/#step-4-merge-genotype-phenotype-data","text":"Prepare data The data is slow to merge unless you split the input bim file into just chr21 and chr22 (Q- why are those faster?) cd /home/manager/data/Data_Day4/data/3b/data/ plink --bfile AFR_1kg.hm3.only.csx --chr 21 22 --make-bed --out AFR_1kg.hm3.only.csx_21_22 Start R with sudo rights to allow you to install sudo R # Load libraries. For any unavailable package, install it with **install.packages(\"_package_name\")** install.packages(\"BiocManager\") BiocManager::install(\"snpStats\") library(data.table) library(ggplot2) library(snpStats) # Define the path to the directory containing the PLINK files and phenotypic data plink_path <- \"/home/manager/data/Data_Day4/data/3b/data/\" # Read PLINK files and phenotype data into R bim_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bim\") fam_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.fam\") bed_file <- file.path(plink_path, \"AFR_1kg.hm3.only.csx_21_22.bed\") pheno_file <- file.path(plink_path, \"sbp_afr_1kg.sim_pheno\") # Read the genotype data using snpStats geno_data <- read.plink(bed_file, bim_file, fam_file) # Extract SNP IDs from geno_data$map$snp.name snp_ids <- geno_data$map$snp.name if (is.null(snp_ids) || !is.character(snp_ids)) { stop(\"SNP IDs are missing or not in the correct format.\") } # Convert the genotype data to a matrix and then to a data.table geno_matrix <- as(geno_data$genotypes, \"matrix\") geno_df <- data.table(geno_matrix) setnames(geno_df, snp_ids) # Add IID column from the fam file geno_df[, IID := geno_data$fam$member] # Read the phenotypic data** pheno_data <- fread(pheno_file, sep = \" \", header = TRUE) Merge phenotype and genotype data # Do merge combined_data <- merge(pheno_data, geno_df, by = \"IID\") # Keep only genotype columns geno <- combined_data[, !names(combined_data) %in% c(\"FID\", \"IID\", \"pheno100\", \"pheno20\", \"pheno33\", \"pheno10\"), with = FALSE] phen <- combined_data$pheno100 # Convert geno to numeric matrix** geno <- as.matrix(geno) mode(geno) <- \"numeric\" # Convert phen to vector phen <- as.vector(phen)","title":"Step 4: Merge genotype-phenotype data"},{"location":"modules/Day3b.docx/#step-5-split-data-into-validation-and-test-sets","text":"Specify Proportion # Here we specify that 40% of all IDs will be used to construct the validation group set.seed(154) vali_proportion <- 0.4 vali_size <- round(nrow(geno) * vali_proportion) vali_indices <- sample(1:nrow(geno), vali_size, replace = FALSE) test_indices <- setdiff(1:nrow(geno), vali_indices) Subsetting of individuals X_vali <- geno[vali_indices, , drop=FALSE] y_vali <- phen[vali_indices] X_test <- geno[test_indices, , drop=FALSE] y_test <- phen[test_indices]","title":"Step 5: Split data into validation and test sets"},{"location":"modules/Day3b.docx/#step-6-prepare-the-regression-model-input-using-the-csx-derived-afr-and-eur-weights","text":"# Read the merged CSX output files AFR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_AFR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) EUR_betas <- fread(file.path(\"/home/manager/data/Data_Day4/out/csx/combined_EUR_pst_eff.txt\"), sep = \"\\t\", header = TRUE) # Assuming the beta files have columns: \"CHR\", \"rsid\", \"pos\", \"ref\", \"alt\", \"beta\" overlap_prs <- merge(AFR_betas, EUR_betas, by = \"rsid\", suffixes = c(\"_afr\", \"_eur\")) # Filter overlap_prs to include only SNPs present in X_vali overlap_prs <- overlap_prs[rsid %in% colnames(X_vali)] # Ensure that X_vali and X_test only contain SNPs present in W_afr and W_eur common_snps <- intersect(colnames(X_vali), overlap_prs$rsid) X_vali <- X_vali[, common_snps, drop=FALSE] X_test <- X_test[, common_snps, drop=FALSE] # Reorder the columns of X_vali and X_test to match the order of SNPs in overlap_prs X_vali <- X_vali[, match(overlap_prs$rsid, colnames(X_vali)), drop=FALSE] X_test <- X_test[, match(overlap_prs$rsid, colnames(X_test)), drop=FALSE] # Extract the overlapping rsid and their corresponding betas W_afr <- overlap_prs$beta_afr W_eur <- overlap_prs$beta_eur # Convert W_afr and W_eur to numeric vectors W_afr <- as.numeric(W_afr) W_eur <- as.numeric(W_eur)","title":"Step 6: Prepare the regression model input using the CSX-derived AFR and EUR weights"},{"location":"modules/Day3b.docx/#step-7-prepare-the-variant-weights-matrices-as-vectors","text":"# Pre-check the alignment between the different objects if (ncol(X_vali) != length(W_afr) || ncol(X_vali) != length(W_eur)) { stop(\"Dimensions of X_vali and W_afr/W_eur do not match.\") } # In the validation sample: # (i) Compute XWafr_vali XWafr_vali <- X_vali %*% W_afr # (ii) Convert XWafr_vali to have zero mean and unit variance XWafr_vali_z <- scale(XWafr_vali) # (iii) Compute XWeur_vali XWeur_vali <- X_vali %*% W_eur # (iv) Convert XWeur_vali to have zero mean and unit variance XWeur_vali_z <- scale(XWeur_vali) # (v) Combine the normalized matrices XW_vali <- cbind(XWafr_vali_z, XWeur_vali_z) # Fit the model model <- lm(scale(y_vali) ~ XWafr_vali_z + XWeur_vali_z - 1) # '- 1' removes the intercept # Obtain the regression parameters a_hat <- coef(model)[1] b_hat <- coef(model)[2] print(paste(\"a_hat =\", a_hat)) print(paste(\"b_hat =\", b_hat))","title":"Step 7: Prepare the variant weights matrices as vectors"},{"location":"modules/Day3b.docx/#step-8-predict-phenotype-on-validation-and-test-dataset","text":"Generate a linear combination of AFR and EUR PRSs for each individual # Each ancestry component is weighted by the regression coicient of that ancestry, in the preceding step y_hat_vali <- a_hat * XWafr_vali_z + b_hat * XWeur_vali_z # In the test sample: # Compute XWafr_test and XWeur_test XWafr_test <- X_test %*% W_afr XWafr_test_z <- scale(XWafr_test) XWeur_test <- X_test %*% W_eur XWeur_test_z <- scale(XWeur_test) # y_hat in the test sample y_hat <- a_hat * XWafr_test_z + b_hat * XWeur_test_z","title":"Step 8: Predict phenotype on validation and test dataset"},{"location":"modules/Day3b.docx/#step-9-plot-phenotype-distributions-of-validation-and-test-data","text":"Check that both distributions are approximately normal library(ggplot2) # Create data frames for validation and test sets vali_data <- data.frame(trait = y_vali, dataset = \"Validation\") test_data <- data.frame(trait = y_test, dataset = \"Test\") # Combine both data frames combined_data <- rbind(vali_data, test_data) # Plot the distributions ggplot(combined_data, aes(x = trait, fill = dataset)) + geom_density(alpha = 0.5) + labs(title = \"Trait Distributions for Validation and Test Sets\", x = \"Trait Value\", y = \"Density\") + theme_minimal()","title":"Step 9: Plot phenotype distributions of validation and test data:"},{"location":"modules/Day3b.docx/#step-10-plot-true-values-against-predicted-values","text":"The next steps use standard normal phenotype data to reduce scale differences between PRS and trait values min_true <- min(min(y_vali), min(y_test)) max_true <- max(max(y_vali), max(y_test)) min_pred <- min(min(y_hat_vali), min(y_hat)) max_pred <- max(max(y_hat_vali), max(y_hat)) pdf(\"true_against_pred.pdf\", width = 10, height = 5) par(mfrow = c(1, 2)) plot(scale(y_vali), y_hat_vali, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Validation Dataset') abline(0, 1, col = 'red', lty = 2) plot(scale(y_test), y_hat, pch = 19, col = rgb(0, 0, 0, 0.5), xlab = 'True Values', ylab = 'Predicted Values', main = 'Test Dataset') abline(0, 1, col = 'red', lty = 2) dev.off()","title":"Step 10: Plot true values against predicted values"},{"location":"modules/Day3b.docx/#step-11-calculate-deviance-based-r2","text":"# Calculate the deviance (SS_res) deviance <- sum((scale(y_test) - y_hat) ^ 2) print(paste(\"deviance =\", deviance)) # Calculate the mean of the scaled y_test y_test_mean <- mean(scale(y_test)) # Calculate the null deviance (SS_tot) deviance_null <- sum((scale(y_test) - y_test_mean)^ 2) print(paste(\"deviance_null =\", deviance_null)) # Calculate R2 R2 <- 1 - (deviance / deviance_null) print(paste(\"R2 =\", R2))","title":"Step 11: Calculate deviance-based R2"},{"location":"modules/Day3b.docx/#results","text":"graph: pdf true against pred","title":"Results"},{"location":"modules/Day4a.docx/","text":"BridgePRS Learning Objectives The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method. Introduction to BridgePRS BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values. BridgePRS Scenario 1: Application of African GWAS weights to an African target group Create configuration file for the target-only analysis In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes). Questions From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ? Note In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step. Single ancestry BridgePRS analysis: Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y Task Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders Questions What evidence can you see that the analysis was successfully executed? BridgePRS Scenario 2: Prediction into African target data using European and African summary statistics BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations. Create configuration file for base and target populations bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat Question Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located? Tasks Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below. Multi-ancestry BRIDGEPRS analysis: Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y Tasks After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory. Questions In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model? Short Quiz I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis? File types ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat Answer Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Day4a.docx"},{"location":"modules/Day4a.docx/#bridgeprs","text":"","title":"BridgePRS"},{"location":"modules/Day4a.docx/#learning-objectives","text":"The goal of this practical is to implement the basic steps needed to implement multi-ancestry PRS prediction models successfully, using the BridgePRS software. By the end of this session participants will understand how to: - Set up the configuration files used as input by the software. - Edit and interact with BridgePRS via the command line. - Implement the 3 PRS models integral to the functionality of the software, using the integrative \"easy run\" function. - Perform a single-ancestry version of the BridgePRS method.","title":"Learning Objectives"},{"location":"modules/Day4a.docx/#introduction-to-bridgeprs","text":"BridgePRS is a Bayesian PRS method that integrates trans-ancestry GWAS summary statistics. Unlike the fine-mapping approach of PRS-CSx, BridgePRS retains all variants within loci to best tag causal variants shared across ancestries. The focus is on correctly estimating causal effect sizes, which is key when the goal is prediction, rather than on estimating their location. BridgePRS is most applicable for combining information of a well-powered GWAS performed in a (discovery) population or populations unmatched to the ancestry of the target sample, with a second GWAS of relatively limited power in a (target) population that is matched to the ancestry of the target sample. BridgePRS is first applied to a large discovery population GWAS, by running Bayesian ridge regression, at putative loci. The BridgePRS algorithm is trained according to 3 PRS models: 1. PRS run using only the target (Non-European) dataset (prs-single) 2. PRS run using SNP-weights calculated from the European Model (prs-port) 3. PRS run using a prior effect-size distribution from the European Model (prs-prior) The 3 models are subsequently combined to produce a weighted PRS solution As well as applying each of the steps in sequence, models can also be run separately according to the needs of individual users. Here, we use the single-ancestry BridgePRS approach to train polygenic scores for an African target sample applying the Bayesian ridge regression approach of BridgePRS to shrink the effect size of SNPs derived from an African ancestry GWAS towards their true underlying values.","title":"Introduction to BridgePRS"},{"location":"modules/Day4a.docx/#bridgeprs-scenario-1-application-of-african-gwas-weights-to-an-african-target-group","text":"","title":"BridgePRS Scenario 1: Application of African GWAS weights to an African target group"},{"location":"modules/Day4a.docx/#create-configuration-file-for-the-target-only-analysis","text":"In this example we will run BridgePRS across chromosomes 1 - 22. The first required step is to generate the configuration file needed to run the desired single ancestry BridgePRS analysis. The following command should be run from the main directory: bridgePRS check pop -o out_config-AFR-single --pop AFR --sumstats_prefix data/pop_europe/sumstats/eur.chr --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat (BridgePRS produces on-screen information which tells you some of the tasks the software is doing behind the scenes).","title":"Create configuration file for the target-only analysis"},{"location":"modules/Day4a.docx/#questions","text":"From the on-screen output: 1. Do you notice anything interesting in the way BridgePRS handled the phenotype file? 2. How many phenotypes was BridgePRS able to identify in the phenotype file? in what way do these phenotypes differ? 3. Which resulting files have been generated in ./out/save/ ?","title":"Questions"},{"location":"modules/Day4a.docx/#note","text":"In the next code snippet the file path of the newly created .config (configuration) file has been incorporated into the run command. This information was provided as part of the on-screen output in the previous step.","title":"Note"},{"location":"modules/Day4a.docx/#single-ancestry-bridgeprs-analysis","text":"Now that we have our African configuration file prepared we are ready to perform the single-ancestry BridgePRS analysis. In this step we opt to select the continuous version of the trait for analysis. bridgePRS prs-single run -o out_config-AFR-single --pop africa --config_files out_config-AFR-single/save/primary.AFR.config --phenotype y","title":"Single ancestry BridgePRS analysis:"},{"location":"modules/Day4a.docx/#task","text":"Review the contents of the output directory out_bridge-AFR-single/prs-single_AFRICA and subfolders","title":"Task"},{"location":"modules/Day4a.docx/#questions_1","text":"What evidence can you see that the analysis was successfully executed?","title":"Questions"},{"location":"modules/Day4a.docx/#bridgeprs-scenario-2-prediction-into-african-target-data-using-european-and-african-summary-statistics","text":"BridgePRS is most commonly used to combine the power of a smaller ancestry-matched GWAS with a much larger but genetically-distant GWAS population, for the purpose of maximising PRS prediction quality for under-served target populations.","title":"BridgePRS Scenario 2:  Prediction into African target data using European and African summary statistics"},{"location":"modules/Day4a.docx/#create-configuration-file-for-base-and-target-populations","text":"bridgePRS check pops -o out_config-EUR-AFR-easyrun --pop AFR EUR --sumstats_prefix data/pop_africa/sumstats/afr.chr data/pop_europe/sumstats/eur.chr --sumstats_suffix .glm.linear.gz .glm.linear.gz --genotype_prefix data/pop_africa/genotypes/afr_genotypes --phenotype_file data/pop_africa/phenotypes/afr_pheno.dat","title":"Create configuration file for base and target populations"},{"location":"modules/Day4a.docx/#question","text":"Carefully check the information given in the on-screen output: (i) How many different .config files have been produced and (ii) where are they located?","title":"Question"},{"location":"modules/Day4a.docx/#tasks","text":"Incorporate the config path information into the code template provided below (for both European and African .config files). Based on your-recent understanding of genetic distances between continental populations, choose a sensible value of the --fst parameter to reflect the genetic distance between Africans and Europeans. This extra information will inform the prior distribution from which posterior effect weights for the target population will be calculated. This needs to be done before attempting to run the code given below.","title":"Tasks"},{"location":"modules/Day4a.docx/#multi-ancestry-bridgeprs-analysis","text":"Add the missing peices of information to the code below, as you enter it into your terminal. bridgePRS easyrun go -o out_easyrun-EUR-AFR --config_files target.AFR.config base.EUR.config --fst --phenotype y","title":"Multi-ancestry BRIDGEPRS analysis:"},{"location":"modules/Day4a.docx/#tasks_1","text":"After running the above code, navigate to the output directory: ./out_config-EUR-AFR-easyrun to inspect the results. Open either of the 2 plots that you see in the directory.","title":"Tasks"},{"location":"modules/Day4a.docx/#questions_2","text":"In the summary plot, which set of values expresses the correlation between the weights calculated by BridgePRS and the beta weights from the initial GWASs? In which output directory will you find precise values of variance explained by the prs-combined-AFR model? What is the variance explained by the prs-combined-AFR model?","title":"Questions"},{"location":"modules/Day4a.docx/#short-quiz","text":"I have GWAS data and genotype/phenotype data for a cohort consisting of >2000 samples from a small East European population. The population LD structure is unique and so I would like this information to be incorporated into my PRS prediction model. I additionally have GWAS and genotype/phenotype data from the UKB biobank that I want to include. How should I formulate the relevant Config files for the BridgePRS analysis?","title":"Short Quiz"},{"location":"modules/Day4a.docx/#file-types","text":"ukr/sumstats/ukr.sumstats.out ukr/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam, ukr/phenotypes/ukr_test.dat, ukr/phenotypes/ukr_validation.dat ukb/sumstats/ukb.sumstats.gz ukb/genotypes/chr1.bed,bim,fam...chr22.bed,bin,fam ukb/phenotypes/ukb_test.dat, ukb/phenotypes/ukb_validation.dat","title":"File types"},{"location":"modules/Day4a.docx/#answer","text":"Target config: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE= Model config file: POP= LDPOP= SUMSTATS_PREFIX= GENOTYPE_PREFIX= PHENOTYPE_FILE= VALIDATION_FILE=","title":"Answer"},{"location":"modules/Day4b.docx/","text":"Day 4 - Practical 2: Introduction to Admixture analysis Module Goals The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores Part1: Plot Decay of Ancestry LD over time Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" ) Questions (i) Describe what happens to admixture LD over time? (ii) Why does recombination also have an impact? Part 2: Global Ancestry Inference We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ``` ./admixture samples_n28_qc_thin.bed 2 --supervised -j4 #### **Questions** ##### (i) What do you think the number specified after the inout file represents? The next step is to create a plot the results Plot Global Ancestry Results R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300) #### Questions ##### (i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary) ### Part 3: Local Ancestry Inference Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm. Run the following UNIX command from the home directory module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done #### Questions ##### (i) What information is provided in the simulation results table displayed on-screen ? ### Part 4: Plot local admixture on chromosome 22 In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr) Function to read the .msp.tsv file read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) } Function to read the .Q file read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) } Specification of file paths msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q' Read in files msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file) Check for NA or empty column names print(colnames(msp_df)) Extract ancestry columns based on pattern recognition of column names ancestry_cols <- colnames(msp_df)[grep(\"\\.0$|\\.1$\", colnames(msp_df))] Function to determine ancestry based on RFMix codes determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) } Rename columns uniquely colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62]) Prepare data for plotting plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) ) Create plot of chromosome 22 across the sample plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10)) Save the plot to a file ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8) #### Questions ##### (i) How many different continental ancestries do you see represented across the 58 strands? ##### (ii) Why is the number of different ancestry backgrounds higher than it was in the previous step? ### Part 5: Formatting of admixture files for analysis using PRSice #### Step 1 - Convert phased genotypes to GenomicRange format Run from the home directory library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() } Read in phased VCF file vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22) Convert haplotypes to data frame haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\") Convert SNP info to data frame snp_info_df <- data.frame(extracted_snp_info22) Check for numerical and ordering consistency between haplotypes and SNP info if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") } Merge haplotypes and SNP info merge_chr22 <- cbind(snp_info_df, haps_dt) Convert to long format using panelr chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) Insert 'end' column after 'POS' and create 'wave' column chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\")) Remove row names and drop redundant columns using base R rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops] Rename columns names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header Create GRanges object gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) Save the GRanges object saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\") Clean up memory clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\")) ### Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix Read in MSP file msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\") Reformat genome-wide local ancestry output as GRanges object colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE) Convert the elements of the GRange object into a dataframe chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL Clean up column headers header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header Convert local ancestry calls from wide to long format chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\")) Drop redundant columns drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL Convert the reconfigured dataframe file into a GRanges object chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE) Ensure chr22_haplo_long is a GRanges object before finding overlaps chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE) Find overlaps and store matching features Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)])) Local ancestry calls are now aligned with genotypic data and positional coordinates Convert to dataframe without adding 'X' to numeric column names genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df)) Output the dataframe write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F) Clean up memory rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc() #### Part 5: Step 3 - Create separate Plink files Partition genotype and local ancestry data chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))] Reintegrate in interleaved format d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)] Prepare headers indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx] Convert LAnc data from long to wide format LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE))) Check length of LA_wide length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers Convert genotype calls to horizontal orientation geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes Clean up and finalize geno_final geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal) Name columns colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") Write final tables write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE) #### Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22 Perform general QC ahead of running PRSice on ancestry-deconvolved individuals (i) Remove SNPs with low minor allele count (MAC) Plink - Remove monomorphic SNPs (minor allele count 0-4). AFR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done EUR for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done PRSice - Generate ancestry-specific weights AFR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base EUR Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base #### Part 5: Step 5 - Evaluate the Admixture-informed PRS library(dplyr) Read the PRS files into dataframes file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F) Add the fourth column of both files file1$PRS_SUM <- file1$PRS + file2$PRS Load the phenotype data pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\") Convert phenotype column to numeric pheno$phenotype <- as.numeric(pheno$phenotype) Merge PRS data with phenotype data merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\")) Rename columns names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\" Perform linear regression for each PRS and the combined PRS model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data) Extract R-squared values r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared Print the R-squared values cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\") ``` Questions (i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?","title":"Day4b.docx"},{"location":"modules/Day4b.docx/#day-4-practical-2-introduction-to-admixture-analysis","text":"","title":"Day 4 - Practical 2: Introduction to Admixture analysis"},{"location":"modules/Day4b.docx/#module-goals","text":"The goal of this practical is to provide you with basic understanding of Admixture and the basic elements behind Admixture PRS scores. Upon completion of this practical, you should: * Gain familiarity with a variety of tools used in the context of admixed population research * Go through the basic steps of formulating admixture-informed polygenic risk scores","title":"Module Goals"},{"location":"modules/Day4b.docx/#part1-plot-decay-of-ancestry-ld-over-time","text":"Here we explore how Admixture LD varies over time and as a function of the genetic distance between loci. In R: library(ggplot2) library(reshape2) library(viridis) #range of values of r (recombination fraction) r = seq(0, 0.5, 0.1) dtmat = matrix(NA, nrow = 6, ncol = 10) # matrix to store values of dt for(i in 1:6){ dt = 0.25 for(j in 1:10){ dtmat[i,j] = dt dt = dt*(1 -r[i])^j } } dtmat = reshape2::melt(dtmat) colnames(dtmat) = c(\"r\",\"g\",\"Dt\") dtmat$r = (dtmat$r - 1)/10 ggplot(dtmat, aes(x = g, y = Dt, group = r, color = as.factor(r))) + geom_line(linewidth = 1.2) + theme_minimal(base_size = 14) + theme( axis.text = element_text(size = 14), axis.title = element_text(size = 16), legend.title = element_text(size = 14), legend.text = element_text(size = 12), plot.title = element_text(size = 18, face = \"bold\"), plot.subtitle = element_text(size = 14), panel.grid.major = element_line(color = \"gray80\") ) + scale_color_viridis_d() + scale_x_continuous(breaks = c(1:10)) + labs( title = \"Decay of Linkage Disequilibrium Over Generations\", subtitle = \"Effect of Recombination Fraction (r) on Admixture LD\", x = \"Generations since admixture (t)\", y = \"Admixture LD\", color = \"Recombination Fraction (r)\" )","title":"Part1: Plot Decay of Ancestry LD over time"},{"location":"modules/Day4b.docx/#questions","text":"","title":"Questions"},{"location":"modules/Day4b.docx/#i-describe-what-happens-to-admixture-ld-over-time","text":"","title":"(i) Describe what happens to admixture LD over time?"},{"location":"modules/Day4b.docx/#ii-why-does-recombination-also-have-an-impact","text":"","title":"(ii) Why does recombination also have an impact?"},{"location":"modules/Day4b.docx/#part-2-global-ancestry-inference","text":"We will now run an analysis using the software ADMIXTURE to calculate global ancestry proportions across a sample of 28 individuals. Here we perform a supervised analysis. Please execute the following code from location ~/RFMIX_WCS_2024/data/plink/ ``` ./admixture samples_n28_qc_thin.bed 2 --supervised -j4 #### **Questions** ##### (i) What do you think the number specified after the inout file represents? The next step is to create a plot the results","title":"Part 2: Global Ancestry Inference"},{"location":"modules/Day4b.docx/#plot-global-ancestry-results","text":"R library(ggplot2) library(reshape2) # Read the data from files fam <- read.table(\"samples_n28_qc_thin.fam\", header = FALSE) pop <- read.table(\"samples_n28_qc_thin.pop\", header = FALSE) Q <- read.table(\"samples_n28_qc_thin.2.Q\", header = FALSE) # Merge the data into one data frame merged_data <- cbind(fam, pop, Q) # Extract necessary columns (assuming the structure of the files as in the image) colnames(merged_data) <- c(\"FID\", \"IID\", \"PaternalID\", \"MaternalID\", \"Sex\", \"Phenotype\", \"Population\", \"AFR\", \"EUR\") # Order data by decreasing AFR merged_data <- merged_data[order(-merged_data$AFR), ] # Prepare data for plotting plot_data <- merged_data[, c(\"IID\", \"AFR\", \"EUR\")] plot_data$IID <- factor(plot_data$IID, levels = plot_data$IID) # Ensure order is maintained in plot plot_data_melted <- melt(plot_data, id.vars = \"IID\") # Create the plot p <- ggplot(plot_data_melted, aes(x = IID, y = value, fill = variable)) + geom_bar(stat = \"identity\", position = \"stack\") + scale_fill_manual(values = c(\"AFR\" = \"red\", \"EUR\" = \"darkgreen\")) + labs(x = \"Individual ID\", y = \"Ancestry Proportion\", fill = \"Ancestry\", title = \"Global Ancestry Proportions\") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"), axis.text.y = element_text(face = \"bold\"), # Bold y-axis numbers axis.title.x = element_text(face = \"bold\"), axis.title.y = element_text(face = \"bold\"), legend.title = element_text(face = \"bold\"), plot.title = element_text(face = \"bold\", hjust = 0.5)) # Save the plot with specified dimensions ggsave(\"ancestry_plot.png\", plot = p, width = 10, height = 6, units = \"in\", dpi = 300) #### Questions ##### (i) What are the ancestry assignments of the 28 individuals? (Provide estimated proportions where necessary) ### Part 3: Local Ancestry Inference Next we will use the RFMix software to calculate local ancestry on chromosome 22 for the same 28 individuals. The RFMix algorithm uses an unsupervised learning algorithm.","title":"Plot Global Ancestry Results"},{"location":"modules/Day4b.docx/#run-the-following-unix-command-from-the-home-directory","text":"module load cmake module load bcftools/1.10.2 for i in {22..22}; do ./software/rfmix -f ./data/rfmix/chr1-22_phased.bcf.gz -r ./reference/chr1-22.b.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf.gz --analyze-range=26.86-31.80 -m ./data/rfmix/1KG_superpop_vs_ID.txt --chromosome=${i} -g ./reference/1kg_chr1-22.gmap --n-threads=4 -o ./out/rfmix/chr${i}.local_ancestry; done #### Questions ##### (i) What information is provided in the simulation results table displayed on-screen ? ### Part 4: Plot local admixture on chromosome 22 In this step we will plot the output from the previous RFMix run. Execute the following code from the home directory R library(ggplot2) library(dplyr) library(tidyr)","title":"Run the following UNIX command from the home directory"},{"location":"modules/Day4b.docx/#function-to-read-the-msptsv-file","text":"read_msp_file <- function(msp_file) { # Read the entire file as text lines <- readLines(msp_file) # Determine the number of header lines (lines starting with '#') header_lines <- lines[grepl(\"^#\", lines)] num_header_lines <- length(header_lines) # Extract column names from the last header line col_names <- strsplit(header_lines[num_header_lines], \"\\t\")[[1]] # Read the data skipping the header lines msp_df <- read.table(msp_file, header = FALSE, skip = num_header_lines, sep = \"\\t\") # Assign column names colnames(msp_df) <- col_names return(msp_df) }","title":"Function to read the .msp.tsv file"},{"location":"modules/Day4b.docx/#function-to-read-the-q-file","text":"read_Q_file <- function(Q_file) { Q_df <- read.table(Q_file, header = TRUE, sep = \"\\t\", comment.char = \"#\") colnames(Q_df) <- c(\"sample\", \"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(Q_df) }","title":"Function to read the .Q file"},{"location":"modules/Day4b.docx/#specification-of-file-paths","text":"msp_file <- './out/rfmix/chr22.local_ancestry.msp.tsv' Q_file <- './out/rfmix/chr22.local_ancestry.rfmix.Q'","title":"Specification of file paths"},{"location":"modules/Day4b.docx/#read-in-files","text":"msp_df <- read_msp_file(msp_file) Q_df <- read_Q_file(Q_file)","title":"Read in files"},{"location":"modules/Day4b.docx/#check-for-na-or-empty-column-names","text":"print(colnames(msp_df))","title":"Check for NA or empty column names"},{"location":"modules/Day4b.docx/#extract-ancestry-columns-based-on-pattern-recognition-of-column-names","text":"ancestry_cols <- colnames(msp_df)[grep(\"\\.0$|\\.1$\", colnames(msp_df))]","title":"Extract ancestry columns based on pattern recognition of column names"},{"location":"modules/Day4b.docx/#function-to-determine-ancestry-based-on-rfmix-codes","text":"determine_ancestry <- function(value) { ancestry_map <- c(\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\") return(ancestry_map[value + 1]) }","title":"Function to determine ancestry based on RFMix codes"},{"location":"modules/Day4b.docx/#rename-columns-uniquely","text":"colnames(msp_df)[7:62] <- paste0(\"haplo_\", colnames(msp_df)[7:62])","title":"Rename columns uniquely"},{"location":"modules/Day4b.docx/#prepare-data-for-plotting","text":"plot_data <- msp_df %>% pivot_longer(cols = starts_with(\"haplo_\"), names_to = \"haplo_col\", values_to = \"ancestry_code\", names_repair = \"unique\") %>% mutate( individual = gsub(\"\\..*\", \"\", haplo_col), strand = ifelse(grepl(\"\\.0$\", haplo_col), \"Strand 1\", \"Strand 2\"), ancestry = determine_ancestry(ancestry_code) )","title":"Prepare data for plotting"},{"location":"modules/Day4b.docx/#create-plot-of-chromosome-22-across-the-sample","text":"plot <- ggplot(plot_data, aes(x = sgpos, xend = egpos, y = interaction(individual, strand, lex.order = TRUE), yend = interaction(individual, strand, lex.order = TRUE), color = ancestry)) + geom_segment(linewidth = 4) + scale_color_manual(values = c(\"AFR\" = \"blue\", \"AMR\" = \"orange\", \"EAS\" = \"green\", \"EUR\" = \"red\", \"SAS\" = \"purple\")) + labs(x = \"Genetic position (cM)\", y = \"Individual ID and Haplotype\", title = \"Local Ancestry Across Chromosome 22\", color = \"Ancestry\") + theme_minimal() + theme(axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10))","title":"Create plot of chromosome 22 across the sample"},{"location":"modules/Day4b.docx/#save-the-plot-to-a-file","text":"ggsave(\"./out/rfmix/local_ancestry_chromosome22_5Mb_subregion.png\", plot = plot, width = 10, height = 8) #### Questions ##### (i) How many different continental ancestries do you see represented across the 58 strands? ##### (ii) Why is the number of different ancestry backgrounds higher than it was in the previous step? ### Part 5: Formatting of admixture files for analysis using PRSice #### Step 1 - Convert phased genotypes to GenomicRange format","title":"Save the plot to a file"},{"location":"modules/Day4b.docx/#run-from-the-home-directory","text":"library(vcfR) library(memuse) library(panelr) library(GenomicRanges) library(data.table) clean_memory <- function(vars_to_remove) { rm(list = vars_to_remove) gc() }","title":"Run from the home directory"},{"location":"modules/Day4b.docx/#read-in-phased-vcf-file","text":"vcfr_inputfile_chr22 <- read.vcfR(\"./data/rfmix/chr22_phased.vcf.gz\", verbose = FALSE) extracted_haps22 <- extract.haps(vcfr_inputfile_chr22, mask = FALSE, unphased_as_NA = TRUE, verbose = TRUE) extracted_snp_info22 <- getFIX(vcfr_inputfile_chr22)","title":"Read in phased VCF file"},{"location":"modules/Day4b.docx/#convert-haplotypes-to-data-frame","text":"haps_df <- data.frame(extracted_haps22, check.names = FALSE) haps_dt <- setDT(haps_df, keep.rownames = \"snps\")","title":"Convert haplotypes to data frame"},{"location":"modules/Day4b.docx/#convert-snp-info-to-data-frame","text":"snp_info_df <- data.frame(extracted_snp_info22)","title":"Convert SNP info to data frame"},{"location":"modules/Day4b.docx/#check-for-numerical-and-ordering-consistency-between-haplotypes-and-snp-info","text":"if (!identical(haps_dt[['snps']], snp_info_df[['ID']])) { stop(\"Numerical and ordering inconsistency between haplotypes and SNP info\") }","title":"Check for numerical and ordering consistency between haplotypes and SNP info"},{"location":"modules/Day4b.docx/#merge-haplotypes-and-snp-info","text":"merge_chr22 <- cbind(snp_info_df, haps_dt)","title":"Merge haplotypes and SNP info"},{"location":"modules/Day4b.docx/#convert-to-long-format-using-panelr","text":"chr22_haplo_long <- long_panel(merge_chr22, prefix = \"_\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE)","title":"Convert to long format using panelr"},{"location":"modules/Day4b.docx/#insert-end-column-after-pos-and-create-wave-column","text":"chr22_haplo_long$end <- chr22_haplo_long$POS chr22_haplo_long$wave <- ifelse(chr22_haplo_long$wave == \"0\", \"+\", ifelse(chr22_haplo_long$wave == \"1\", \"-\", \"Z\"))","title":"Insert 'end' column after 'POS' and create 'wave' column"},{"location":"modules/Day4b.docx/#remove-row-names-and-drop-redundant-columns-using-base-r","text":"rownames(chr22_haplo_long) <- NULL drops <- c(\"id\", \"snps\", \"QUAL\", \"FILTER\") chr22_haplo_long <- chr22_haplo_long[, !names(chr22_haplo_long) %in% drops]","title":"Remove row names and drop redundant columns using base R"},{"location":"modules/Day4b.docx/#rename-columns","text":"names(chr22_haplo_long)[3] <- \"start\" names(chr22_haplo_long)[1] <- \"strand\" header <- gsub(\".*_\", \"\", colnames(chr22_haplo_long)[7:ncol(chr22_haplo_long)]) names(chr22_haplo_long)[7:ncol(chr22_haplo_long)] <- header","title":"Rename columns"},{"location":"modules/Day4b.docx/#create-granges-object","text":"gr_obj_chr22 <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE)","title":"Create GRanges object"},{"location":"modules/Day4b.docx/#save-the-granges-object","text":"saveRDS(gr_obj_chr22, file = \"./out/rfmix/chr22_phased_gr.rds\")","title":"Save the GRanges object"},{"location":"modules/Day4b.docx/#clean-up-memory","text":"clean_memory(c(\"vcfr_inputfile_chr22\", \"extracted_haps22\", \"merge_chr22\", \"haps_df\", \"haps_dt\", \"snp_info_df\", \"chr22_haplo_long\", \"gr_obj_chr22\")) ### Part 5: Step 2 - Merge genotypes from Step 1 with local ancestry calls by RFMix","title":"Clean up memory"},{"location":"modules/Day4b.docx/#read-in-msp-file","text":"msp <- fread(\"./out/rfmix/chr22.local_ancestry.msp.tsv\")","title":"Read in MSP file"},{"location":"modules/Day4b.docx/#reformat-genome-wide-local-ancestry-output-as-granges-object","text":"colnames(msp)[1] <- \"chm\" msp_gr <- makeGRangesFromDataFrame(msp, seqnames.field = \"chm\", start.field = \"spos\", end.field = \"epos\", keep.extra.columns = TRUE)","title":"Reformat genome-wide local ancestry output as GRanges object"},{"location":"modules/Day4b.docx/#convert-the-elements-of-the-grange-object-into-a-dataframe","text":"chr22_rf <- data.frame(seqnames = seqnames(msp_gr), ranges = ranges(msp_gr), strand = strand(msp_gr), mcols(msp_gr), check.names = FALSE) names(chr22_rf)[1:5] <- c(\"chr\", \"start\", \"end\", \"width\", \"strand\") rownames(chr22_rf) <- NULL","title":"Convert the elements of the GRange object into a dataframe"},{"location":"modules/Day4b.docx/#clean-up-column-headers","text":"header <- gsub(\".*_\", \"\", colnames(chr22_rf)[9:ncol(chr22_rf)]) names(chr22_rf)[9:ncol(chr22_rf)] <- header","title":"Clean up column headers"},{"location":"modules/Day4b.docx/#convert-local-ancestry-calls-from-wide-to-long-format","text":"chr22_rf_long <- long_panel(chr22_rf, prefix = \".\", begin = 0, end = 1, label_location = \"end\", as_panel_data = FALSE) chr22_rf_long <- as.data.frame(chr22_rf_long, check.names = FALSE) chr22_rf_long$strand <- ifelse(chr22_rf_long$wave == \"0\", \"+\", ifelse(chr22_rf_long$wave == \"1\", \"-\", \"Z\"))","title":"Convert local ancestry calls from wide to long format"},{"location":"modules/Day4b.docx/#drop-redundant-columns","text":"drops <- c(\"wave\", \"id\") chr22_rf_long <- chr22_rf_long[, !(names(chr22_rf_long) %in% drops)] rownames(chr22_rf_long) <- NULL","title":"Drop redundant columns"},{"location":"modules/Day4b.docx/#convert-the-reconfigured-dataframe-file-into-a-granges-object","text":"chr22_msp_gr <- makeGRangesFromDataFrame(chr22_rf_long, ignore.strand = FALSE, keep.extra.columns = TRUE)","title":"Convert the reconfigured dataframe file into a GRanges object"},{"location":"modules/Day4b.docx/#ensure-chr22_haplo_long-is-a-granges-object-before-finding-overlaps","text":"chr22_haplo_gr <- makeGRangesFromDataFrame(chr22_haplo_long, ignore.strand = FALSE, keep.extra.columns = TRUE)","title":"Ensure chr22_haplo_long is a GRanges object before finding overlaps"},{"location":"modules/Day4b.docx/#find-overlaps-and-store-matching-features","text":"","title":"Find overlaps and store matching features"},{"location":"modules/Day4b.docx/#matching-is-coordinates-based-base-position-startendis-used-to-match-the-two-files","text":"matched_regions <- findOverlaps(chr22_haplo_gr, chr22_msp_gr) chr22_haps_lanc_gr <- chr22_haplo_gr[queryHits(matched_regions)] #Store matching features in a new dataframe, add metadata from RFmix output. mcols(chr22_haps_lanc_gr) <- cbind.data.frame(mcols(chr22_haps_lanc_gr), mcols(chr22_msp_gr[subjectHits(matched_regions)]))","title":"Matching is coordinates based. Base position (\"start\"/\"end\")is used to match the two files"},{"location":"modules/Day4b.docx/#local-ancestry-calls-are-now-aligned-with-genotypic-data-and-positional-coordinates","text":"","title":"Local ancestry calls are now aligned with genotypic data and positional coordinates"},{"location":"modules/Day4b.docx/#convert-to-dataframe-without-adding-x-to-numeric-column-names","text":"genes_df <- as.data.frame(chr22_haps_lanc_gr) names(genes_df) <- sub('^X', '', names(genes_df))","title":"Convert to dataframe without adding 'X' to numeric column names"},{"location":"modules/Day4b.docx/#output-the-dataframe","text":"write.table(genes_df, file = \"./out/rfmix/chr22_phased_geno_lanc.txt\", quote = FALSE, row.names=F)","title":"Output the dataframe"},{"location":"modules/Day4b.docx/#clean-up-memory_1","text":"rm(list = c(\"chr22_haplo_gr\", \"chr22_haplo_long\", \"msp\", \"msp_gr\", \"chr22_rf\", \"header\", \"chr22_rf_long\", \"chr22_msp_gr\", \"matched_regions\", \"chr22_haps_lanc_gr\")) gc() #### Part 5: Step 3 - Create separate Plink files","title":"Clean up memory"},{"location":"modules/Day4b.docx/#partition-genotype-and-local-ancestry-data","text":"chr22_genos <- genes_df[, c(1, 2, 6, 9:36)] chr22_LA <- genes_df[, c(1, 2, 6, 40:ncol(genes_df))]","title":"Partition genotype and local ancestry data"},{"location":"modules/Day4b.docx/#reintegrate-in-interleaved-format","text":"d <- chr22_genos[, -c(1:3)] # Retain ID columns only d2 <- chr22_LA[, -c(1:3)]","title":"Reintegrate in interleaved format"},{"location":"modules/Day4b.docx/#prepare-headers","text":"indx <- rbind(names(d), names(d2)) dmerge <- cbind(d, d2) dfinal <- dmerge[, indx]","title":"Prepare headers"},{"location":"modules/Day4b.docx/#convert-lanc-data-from-long-to-wide-format","text":"LA_wide <- lapply(1:ncol(chr22_LA), function(i) as.data.frame(matrix(chr22_LA[, i], ncol = 2, byrow = TRUE)))","title":"Convert LAnc data from long to wide format"},{"location":"modules/Day4b.docx/#check-length-of-la_wide","text":"length(LA_wide) # Expected number. Each item contains a set of matched strand pairs per individual LA_final <- as.data.frame(do.call(cbind, LA_wide)) LA_final <- LA_final[, -c(2, 4, 6)] # Remove duplicates of first 3 cols colnames(LA_final)[4:ncol(LA_final)] <- colnames(dfinal) # Apply headers","title":"Check length of LA_wide"},{"location":"modules/Day4b.docx/#convert-genotype-calls-to-horizontal-orientation","text":"geno_wide <- lapply(1:ncol(chr22_genos), function(i) as.data.frame(matrix(chr22_genos[, i], ncol = 2, byrow = TRUE))) # Genotypic part: Get horizontal genotypes geno_final <- as.data.frame(do.call(cbind, geno_wide)) # Merge horizontal genotypes","title":"Convert genotype calls to horizontal orientation"},{"location":"modules/Day4b.docx/#clean-up-and-finalize-geno_final","text":"geno_final <- geno_final[, -c(2, 4, 6)] # Redundant columns colnames(geno_final)[4:ncol(geno_final)] <- colnames(dfinal)","title":"Clean up and finalize geno_final"},{"location":"modules/Day4b.docx/#name-columns","text":"colnames(geno_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\") colnames(LA_final)[1:3] <- c(\"CHROM\", \"BP\", \"ID\")","title":"Name columns"},{"location":"modules/Day4b.docx/#write-final-tables","text":"write.table(geno_final, \"./out/rfmix/chr22_geno.txt\", row.names = FALSE, quote = FALSE) write.table(LA_final, \"./out/rfmix/chr22_LA.txt\", row.names = FALSE, quote = FALSE) #### Part 5: Step 4 - Use custom software (RFTransform) to create Plink files for input into PRSice module load cmake ./software/RFTransform/build/RFTransformer ./out/rfmix/chr22_geno.txt ./out/rfmix/chr22_LA.txt ./out/plink/chr22","title":"Write final tables"},{"location":"modules/Day4b.docx/#perform-general-qc-ahead-of-running-prsice-on-ancestry-deconvolved-individuals","text":"","title":"Perform general QC ahead of running PRSice on ancestry-deconvolved individuals"},{"location":"modules/Day4b.docx/#i-remove-snps-with-low-minor-allele-count-mac","text":"","title":"(i) Remove SNPs with low minor allele count (MAC)"},{"location":"modules/Day4b.docx/#plink-remove-monomorphic-snps-minor-allele-count-0-4","text":"","title":"Plink - Remove monomorphic SNPs (minor allele count 0-4)."},{"location":"modules/Day4b.docx/#afr","text":"for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-AFR \\ --mac 5 \\ --make-bed \\ --out ./out/plink/chr${i}-AFR.mac done","title":"AFR"},{"location":"modules/Day4b.docx/#eur","text":"for i in {22..22}; do ./software/plink2 \\ --bfile ./out/plink/chr${i}-EUR \\ --mac 5 \\ --make-bed \\ --out ./out/plink//chr${i}-EUR.mac done","title":"EUR"},{"location":"modules/Day4b.docx/#prsice-generate-ancestry-specific-weights","text":"","title":"PRSice - Generate ancestry-specific weights"},{"location":"modules/Day4b.docx/#afr_1","text":"Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/AFR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-AFR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_AFR-base","title":"AFR"},{"location":"modules/Day4b.docx/#eur_1","text":"Rscript ./software/PRSice.R \\ --prsice ./software/PRSice_linux \\ --base ./data/plink/EUR-BMI.Phenotype.glm.linear \\ --extract ./data/plink/snp.valid \\ --A1 A1 \\ --pvalue P \\ --stat BETA \\ --pheno ./data/plink/pheno.plink \\ --beta \\ --snp ID \\ --score sum \\ --target ./out/plink/chr22-EUR.mac \\ --binary-target F \\ --out ./out/prsice/BMI_EUR-base #### Part 5: Step 5 - Evaluate the Admixture-informed PRS library(dplyr)","title":"EUR"},{"location":"modules/Day4b.docx/#read-the-prs-files-into-dataframes","text":"file1 <- read.table(\"./out/prsice/BMI_EUR-base.best\", header = TRUE, check.names=F) file2 <- read.table(\"./out/prsice/BMI_AFR-base.best\", header = TRUE, check.names=F)","title":"Read the PRS files into dataframes"},{"location":"modules/Day4b.docx/#add-the-fourth-column-of-both-files","text":"file1$PRS_SUM <- file1$PRS + file2$PRS","title":"Add the fourth column of both files"},{"location":"modules/Day4b.docx/#load-the-phenotype-data","text":"pheno <- read.table(\"./data/plink/pheno.plink\", header = F) colnames(pheno) <- c(\"FID\", \"IID\", \"phenotype\")","title":"Load the phenotype data"},{"location":"modules/Day4b.docx/#convert-phenotype-column-to-numeric","text":"pheno$phenotype <- as.numeric(pheno$phenotype)","title":"Convert phenotype column to numeric"},{"location":"modules/Day4b.docx/#merge-prs-data-with-phenotype-data","text":"merged_data <- merge(file1[, c(\"FID\", \"IID\", \"PRS\", \"PRS_SUM\")], pheno, by = c(\"FID\", \"IID\")) merged_data <- merge(merged_data, file2[, c(\"FID\", \"IID\", \"PRS\")], by = c(\"FID\", \"IID\"))","title":"Merge PRS data with phenotype data"},{"location":"modules/Day4b.docx/#rename-columns_1","text":"names(merged_data)[names(merged_data) == \"PRS.x\"] <- \"PRS1\" names(merged_data)[names(merged_data) == \"PRS.y\"] <- \"PRS2\"","title":"Rename columns"},{"location":"modules/Day4b.docx/#perform-linear-regression-for-each-prs-and-the-combined-prs","text":"model1 <- lm(phenotype ~ PRS1, data = merged_data) model2 <- lm(phenotype ~ PRS2, data = merged_data) model_sum <- lm(phenotype ~ PRS_SUM, data = merged_data)","title":"Perform linear regression for each PRS and the combined PRS"},{"location":"modules/Day4b.docx/#extract-r-squared-values","text":"r_squared1 <- summary(model1)$r.squared r_squared2 <- summary(model2)$r.squared r_squared_sum <- summary(model_sum)$r.squared","title":"Extract R-squared values"},{"location":"modules/Day4b.docx/#print-the-r-squared-values","text":"cat(\"R-squared for PRS1: \", r_squared1, \"\\n\") cat(\"R-squared for PRS2: \", r_squared2, \"\\n\") cat(\"R-squared for PRS_SUM: \", r_squared_sum, \"\\n\") ```","title":"Print the R-squared values"},{"location":"modules/Day4b.docx/#questions_1","text":"","title":"Questions"},{"location":"modules/Day4b.docx/#i-how-does-the-r-squared-of-the-combined-ancestry-prs-perform-relative-to-the-2-partial-genome-prss","text":"","title":"(i) How does the R-squared of the combined-ancestry PRS perform relative to the 2 partial-genome PRSs?"},{"location":"modules/Day5_Projects/","text":"Integration of Polygenic Risk Scores Task \u2013 PRSmix calculation [3.5 hrs] You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024) Group project data files Group 1 https://github.com/tinashedoc/cvx/blob/main/monodta.txt Group 2 https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt Group 3 https://github.com/tinashedoc/cvx/blob/main/pltdta.txt Group 4 https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Integration of Polygenic Risk Scores"},{"location":"modules/Day5_Projects/#integration-of-polygenic-risk-scores","text":"","title":"Integration of Polygenic Risk Scores"},{"location":"modules/Day5_Projects/#task-prsmix-calculation-35-hrs","text":"You have just completed the Wellcome Connecting Science PRS course and are now acquainted with tools such as PRSCSx and BRIDGEPRS. You also got to hear a talk by Prof Nilanjan Chatterjee, who you later realized has been involved in developing three latest PRS software programs, PROSPER, MUSSEL, and CT-SLEB, that claim to improve PRS prediction in Africans. You are now getting overwhelmed by the sheer number of tools to test out and do not know which tool to use for your PRS project, which involves continental Africans. You decide to read more, and you then stumble across a novel approach called PRSmix, which integrates PRSs developed using diverse methods, forming an integrated PRS that has been noted to outperform previously reported PRS, as shown below. This seems as a much more pragmatic approach, and you are eager to apply it to your PRS projects. Qn1. May you outline the steps (analysis plan) that you will follow to apply the PRSmix approach to the trait you have been assigned. Indicate the metrics you will use to evaluate the predictivity of the PRS for the trait you have been assigned? Qn2. You have been given 15 to 23 PRSs, phenotype, age, sex and PCs depending on your group data files. May you implement your analysis plan and illustrate the codes and outputs that you will share in the form of an Rmarkdown file . May plot the predictivity of the PRSmix vs other PGS in your data file? Qn3. State the pros and cons of using the PRSmix approach in continental Africans? Figure 1 Predictivity of various PRSs compared to PRSmix (Adopted Buu et al. 2024)","title":"Task \u2013 PRSmix calculation [3.5 hrs]"},{"location":"modules/Day5_Projects/#group-project-data-files","text":"","title":"Group project data files"},{"location":"modules/Day5_Projects/#group-1","text":"https://github.com/tinashedoc/cvx/blob/main/monodta.txt","title":"Group 1"},{"location":"modules/Day5_Projects/#group-2","text":"https://github.com/tinashedoc/cvx/blob/main/wbcdta.txt","title":"Group 2"},{"location":"modules/Day5_Projects/#group-3","text":"https://github.com/tinashedoc/cvx/blob/main/pltdta.txt","title":"Group 3"},{"location":"modules/Day5_Projects/#group-4","text":"https://github.com/tinashedoc/cvx/blob/main/eosdta.txt","title":"Group 4"},{"location":"modules/READme/","text":"Module directory Directory for placing the course module files - these should be markdown or PDF documents They include the presentations and practical manuals for the module. Converting between markdown to PDF can be performed using pandoc. Here is a tutorial and system for that: Converting with Pandoc There is an example markdown file - module_base.md","title":"Module directory"},{"location":"modules/READme/#module-directory","text":"Directory for placing the course module files - these should be markdown or PDF documents They include the presentations and practical manuals for the module. Converting between markdown to PDF can be performed using pandoc. Here is a tutorial and system for that: Converting with Pandoc There is an example markdown file - module_base.md","title":"Module directory"},{"location":"modules/module_base/","text":"This is a base file to modify with your module content Please rename this md file to suit your module Markdown guide is available https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax Example git https://github.com/WCSCourses/WWPG_2022/blob/main/manuals/module_linux_scripting/module_linux_scripting.md","title":"This is a base file to modify with your module content"},{"location":"modules/module_base/#this-is-a-base-file-to-modify-with-your-module-content","text":"","title":"This is a base file to modify with your module content"},{"location":"modules/module_base/#please-rename-this-md-file-to-suit-your-module","text":"","title":"Please rename this md file to suit your module"},{"location":"modules/module_base/#markdown-guide-is-available","text":"https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax","title":"Markdown guide is available"},{"location":"modules/module_base/#example-git","text":"https://github.com/WCSCourses/WWPG_2022/blob/main/manuals/module_linux_scripting/module_linux_scripting.md","title":"Example git"},{"location":"scripts/base/","text":"scripts base","title":"Base"},{"location":"scripts/day4update/","text":"wget https://raw.githubusercontent.com/WCSCourses/PRS2024/main/scripts/prs24_day4_builds.sh chmod +x prs24_day4_builds.sh ./prs24_day4_builds.sh","title":"Day4update"}]}